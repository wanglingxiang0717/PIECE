import torch
from utils.utils import print_rank_0, to_device, save_hf_format, save_hf_format_task, set_random_seed, get_all_reduce_mean, get_optimizer_grouped_parameters, save_zero_three_model, load_hf_tokenizer
from utils.data.data_utils import create_prompt_dataset
from utils.data.data_collator import DataCollator
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from tqdm import tqdm
import torch
import torch.distributed as dist
import torch.nn.functional as F
import json
import os
import time
from evaluations import eval_ScienceQA, eval_MeetingBank, eval_PapyrusF, eval_CStance, eval_Py150, eval_FOMC, eval_NumGLUE_cm, eval_NumGLUE_ds # to be continued
from transformers import GenerationConfig
from utils.test_model import generation_file
# from utils.get_sample import process_get_result_file
# from utils.test_model_safe import generation_safe_file
# from human_eval.data import write_jsonl, read_problems
generation_config = GenerationConfig(
    temperature=0.1,
    do_sample=True,
    num_return_sequences=1
)

def merge_generation_results(all_results):
    final = {}
    for res in all_results:
        for task, items in res.items():
            final.setdefault(task, []).extend(items)
    return final

def write_json_or_jsonl(file_path, data, encoding='utf-8'):
    txt = file_path.split('/')[-1].split('.')[-1]
    if txt == 'json':
        with open(file_path, 'w', encoding=encoding) as f_w:
            json.dump(data, f_w, ensure_ascii=False, indent=2)
    elif txt == 'jsonl':
        with open(file_path, 'w', encoding=encoding) as f_w:
            for line in data:
                json_line = json.dumps(line)
                f_w.write(json_line + '\n')
    else:
        print("file_path not exisits")

class CL_Base_Model:
    def __init__(self,
                 model,
                 tokenizer,
                 processor,
                 optimizer,
                 train_task_list,
                 eval_task_list,
                 test_task_list,
                 eval_dataloader_dict,
                 args):
        self.model = model
        self.tokenizer = tokenizer
        self.processor = processor
        self.optimizer = optimizer
        self.train_task_list = train_task_list
        self.eval_task_list = eval_task_list
        self.test_task_list = test_task_list
        self.eval_dataloader_dict = eval_dataloader_dict
        self.args = args
        
        
    def perplexity_evaluation(self, eval_dataloader, device):
        # 验证集上测困惑度
        self.model.eval()
        losses = 0
        with torch.no_grad():
            for step, batch in enumerate(eval_dataloader):
                # implementation, batch = {k: v.to(device) for k, v in batch.items()}
                if 'sources' in batch:
                    del batch['sources']
                batch = to_device(batch, device)
                with torch.no_grad():
                    outputs = self.model(**batch, use_cache=False)
                loss = outputs.loss
                losses += loss.float()
            losses = losses / (step + 1)
            try:
                perplexity = torch.exp(losses)
            except OverflowError:
                perplexity = float("inf")
            try:
                perplexity = get_all_reduce_mean(perplexity).item()
                loss = get_all_reduce_mean(loss).item()
            except:
                pass
        self.model.train()
        return perplexity, losses


    def train_one_task(self, task, i_task, epochs):
        # 在单独某个任务上训练
        print_rank_0(
            f"***** Task: {task} *****",
            self.args.global_rank)
        
        if self.args.local_rank == -1:
            device = torch.device("cuda")
        else:
            torch.cuda.set_device(self.args.local_rank)
            device = torch.device("cuda", self.args.local_rank)
        
        #### TRAIN ####
        train_dataloader = self.train_task_list[task]
        eval_dataloader = self.eval_task_list[task]
        total_steps = epochs * len(train_dataloader)
        progress_bar = tqdm(total=total_steps, leave=True, disable=(self.args.global_rank != 0))
        for epoch in range(epochs):
            print_rank_0(
                f"Beginning of Epoch {epoch+1}/{epochs}, Total Micro Batches {len(train_dataloader)}",
                self.args.global_rank)
            self.model.train()

            for step, batch in enumerate(train_dataloader):
                if 'sources' in batch:
                    del batch['sources']
                batch = to_device(batch, device)
                outputs = self.model(**batch, use_cache=False)
                loss = outputs.loss
                # Update the description to include current step and loss, if needed
                if self.args.global_rank == 0:
                    # Update the progress bar
                    progress_bar.update(1)
                    description = f"Epoch {epoch+1}, Step {step}, Loss: {loss.item():.4f}"
                    progress_bar.set_description(description, refresh=False)

                self.model.backward(loss)
                # Correct gradient accumulation steps are handled withing the deepspeed engine's backward call.
                self.model.step()
                if (step + 2) % self.args.gradient_accumulation_steps == 0:
                    self.eval_on_dataset(epoch, epochs, step, task, device, total_steps)
            self.eval_model_generation(task, epoch, epochs) 
            # self.save_task_model(task, epoch+1)

    def eval_model_generation(self, task, epoch, epochs):
        # if epoch + 1 == epochs:
        local_results = generation_file(task, self.args.dataset_name, self.model, self.processor, 
                        self.args.test_file_dir, os.path.join(self.args.output_dir, task) + "/" + str(epoch + 1))
        dist.barrier()
        if dist.get_rank() == 0:
            all_results = [None for _ in range(dist.get_world_size())]
        else:
            all_results = None

        dist.gather_object(local_results, all_results, dst=0)

        if dist.get_rank() == 0:
            merged = merge_generation_results(all_results)
            save_dir = os.path.join(self.args.output_dir, task) + "/" + str(epoch + 1)
            if not os.path.exists(save_dir): 
                os.makedirs(save_dir)
            for task in merged:
                file_save_path = f"{save_dir}/{task}_multibatch_result.json"
                write_json_or_jsonl(file_save_path, merged[task])
            
        # if epoch + 1 == epochs and self.args.global_rank == 0:
        # if epoch + 1 == epochs:
        #     local_samples = process_get_result_file(self.model, self.tokenizer)
        #     dist.barrier()
        #     all_samples = [None for _ in range(dist.get_world_size())] if dist.get_rank() == 0 else None
        #     dist.gather_object(local_samples, all_samples, dst=0)
        #     merged = []
        #     if dist.get_rank() == 0:
        #         for part in all_samples:
        #             merged.extend(part)
        #         humaneval_save_dir = os.path.join(self.args.output_dir, task) + "/" + str(epoch + 1)
        #         write_jsonl(f"{humaneval_save_dir}/humaneval_dosample_test.jsonl", merged)
                
        #     local_result_safe = generation_safe_file(self.model, self.tokenizer, self.args.test_file_dir)
        #     dist.barrier()
        #     all_samples_safe = [None for _ in range(dist.get_world_size())] if dist.get_rank() == 0 else None
        #     dist.gather_object(local_result_safe, all_samples_safe, dst=0)
        #     merged_safe = []
        #     if dist.get_rank() == 0:
        #         for part in all_samples_safe:
        #             merged_safe.extend(part)
        #         safe_save_dir = os.path.join(self.args.output_dir, task) + "/" + str(epoch + 1)
        #         # write_jsonl(f"{humaneval_save_dir}/humaneval_dosample_test.jsonl", merged)
        #         write_json_or_jsonl(f"{safe_save_dir}/MaliciousInstruct_multibatch_result.json", merged_safe) 
        self.model.train()  
    
    def train_continual(self):
        for i_task, task in enumerate(self.train_task_list):
            self.train_one_task(task, i_task, int(self.args.num_train_epochs[i_task]))
            # self.save_model(i_task)

    def eval_on_dataset(self, epoch, epochs, step, task, device, total_steps):
        # if (step + 2) % self.args.gradient_accumulation_steps == 0:
        eval_dataloader = self.eval_task_list[task]
        g_s = (step + 2) / self.args.gradient_accumulation_steps - 1
        print_rank_0(
            f"***** Evaluating perplexity, Epoch {epoch+1}/{epochs}, step {g_s} *****",
            self.args.global_rank)
        ppl_eval, losses_eval = self.perplexity_evaluation(eval_dataloader, device)
        print_rank_0(f"[eval loss, ppl] step:{total_steps / self.args.gradient_accumulation_steps + g_s}, \tloss: {losses_eval}, \tppl: {ppl_eval}", 
                        self.args.global_rank)
        if self.eval_dataloader_dict is not None:
            for item in self.eval_dataloader_dict:
                ppl, losses = self.perplexity_evaluation(self.eval_dataloader_dict[item], device)
                # print_rank_0(f"[{item} loss, ppl]", "step:", (epoch * len(train_dataloader)) / self.args.gradient_accumulation_steps + g_s, "\tloss:", losses, "\tppl:", ppl)
                print_rank_0(f"[{item} loss, ppl] step:{total_steps / self.args.gradient_accumulation_steps + g_s}, \tloss: {losses}, \tppl: {ppl}", 
                                                self.args.global_rank) 
    
    def save_model(self, round):
        if self.args.output_dir is not None:
            print_rank_0('saving model to ' + self.args.output_dir + "/" + str(round) + '...', self.args.global_rank)

        if self.args.global_rank == 0:
            save_hf_format(self.model, self.tokenizer, self.processor, self.args, sub_folder=str(round))

        if self.args.zero_stage == 3:
            # For zero stage 3, each gpu only has a part of the model, so we need a special save function
            save_zero_three_model(self.model,
                                  self.args.global_rank,
                                  self.args.output_dir,
                                  zero_stage=self.args.zero_stage,
                                  sub_folder=str(round))
        print_rank_0('Sucessful saving model after round {}'.format(round), self.args.global_rank)
        
    def save_task_model(self, task, round):
        self.args.task_output_dir = os.path.join(self.args.output_dir, task)
        if self.args.task_output_dir is not None:
            print_rank_0('saving model to ' + self.args.task_output_dir + "/" + str(round) + '...', self.args.global_rank)

        # if self.args.global_rank == 0:
        #     save_hf_format_task(self.model, self.tokenizer, self.args, sub_folder=str(round))
        if self.args.global_rank == 0:
            save_hf_format_task(self.model, self.tokenizer, self.processor, self.args, sub_folder=str(round))

        if self.args.zero_stage == 3:
            # For zero stage 3, each gpu only has a part of the model, so we need a special save function
            save_zero_three_model(self.model,
                                  self.args.global_rank,
                                  self.args.task_output_dir,
                                  zero_stage=self.args.zero_stage,
                                  sub_folder=str(round))
        print_rank_0('Sucessful saving model after round {}'.format(round), self.args.global_rank)
        
