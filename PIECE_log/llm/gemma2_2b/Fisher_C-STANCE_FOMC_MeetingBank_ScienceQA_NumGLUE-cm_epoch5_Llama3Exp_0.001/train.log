[2025-10-21 16:54:44,094] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 16:54:46,159] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 16:54:46,367] [WARNING] [runner.py:220:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-10-21 16:54:46,367] [INFO] [runner.py:610:main] cmd = /home/TAP/anaconda3/envs/llama2/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=26253 --enable_each_rank_log=None training/main.py --data_path /data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/NumGLUE-cm --model_name_or_path /data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/5 --per_device_train_batch_size 2 --per_device_eval_batch_size 1 --max_prompt_len 1024 --max_ans_len 512 --learning_rate 1e-5 --weight_decay 0. --num_train_epochs 5 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --num_warmup_steps 0 --seed 42 --zero_stage 2 --deepspeed --print_loss --CL_method base --enable_tensorboard --tensorboard_path /data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/log/ --offload --ood_eval_dir /data2/TAP/data/continue_eval_loss_data_small --mask_method Fisher --top_ratio 0.001 --target_name NumGLUE-cm --output_dir /data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001 --test_file_dir /data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000
[2025-10-21 16:54:48,567] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 16:54:50,645] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 16:54:50,850] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-10-21 16:54:50,850] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-10-21 16:54:50,850] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-10-21 16:54:50,850] [INFO] [launch.py:164:main] dist_world_size=4
[2025-10-21 16:54:50,850] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-10-21 16:54:50,850] [INFO] [launch.py:256:main] process 1246906 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=0', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/NumGLUE-cm', '--model_name_or_path', '/data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/5', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'Fisher', '--top_ratio', '0.001', '--target_name', 'NumGLUE-cm', '--output_dir', '/data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001', '--test_file_dir', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000']
[2025-10-21 16:54:50,851] [INFO] [launch.py:256:main] process 1246907 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=1', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/NumGLUE-cm', '--model_name_or_path', '/data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/5', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'Fisher', '--top_ratio', '0.001', '--target_name', 'NumGLUE-cm', '--output_dir', '/data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001', '--test_file_dir', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000']
[2025-10-21 16:54:50,852] [INFO] [launch.py:256:main] process 1246908 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=2', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/NumGLUE-cm', '--model_name_or_path', '/data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/5', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'Fisher', '--top_ratio', '0.001', '--target_name', 'NumGLUE-cm', '--output_dir', '/data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001', '--test_file_dir', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000']
[2025-10-21 16:54:50,853] [INFO] [launch.py:256:main] process 1246909 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=3', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/NumGLUE-cm', '--model_name_or_path', '/data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/5', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'Fisher', '--top_ratio', '0.001', '--target_name', 'NumGLUE-cm', '--output_dir', '/data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001', '--test_file_dir', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000']
[2025-10-21 16:54:54,693] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 16:54:54,693] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 16:54:54,696] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 16:54:54,708] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 16:54:56,570] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 16:54:56,636] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 16:54:56,649] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 16:54:56,757] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Using integrations.HfDeepSpeedConfig (new API)
Using integrations.HfDeepSpeedConfig (new API)
Using integrations.HfDeepSpeedConfig (new API)
Using integrations.HfDeepSpeedConfig (new API)
/data1/TAP/model_exp_2b/1020_NumGLUE-cm_Fisher_parameters_test_epoch1_random_1000/parameters_grad_2/top0.001
[2025-10-21 16:54:57,652] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-10-21 16:54:57,652] [INFO] [comm.py:706:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/data1/TAP/model_exp_2b/1020_NumGLUE-cm_Fisher_parameters_test_epoch1_random_1000/parameters_grad_2/top0.001
/data1/TAP/model_exp_2b/1020_NumGLUE-cm_Fisher_parameters_test_epoch1_random_1000/parameters_grad_2/top0.001
/data1/TAP/model_exp_2b/1020_NumGLUE-cm_Fisher_parameters_test_epoch1_random_1000/parameters_grad_2/top0.001
[2025-10-21 16:54:58,072] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-10-21 16:54:58,074] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-10-21 16:54:58,105] [INFO] [comm.py:675:init_distributed] cdb=None
ninja: no work to do.
Time to load cpu_adam op: 2.3386666774749756 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-10-21 16:57:46,570] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.1, git-hash=unknown, git-branch=unknown
[2025-10-21 16:57:46,570] [INFO] [comm.py:700:init_distributed] Distributed backend already initialized
[2025-10-21 16:57:46,570] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
ninja: no work to do.
Time to load cpu_adam op: 2.3725879192352295 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-10-21 16:57:46,687] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
ninja: no work to do.
Time to load cpu_adam op: 2.393164873123169 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-10-21 16:57:46,715] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
ninja: no work to do.
Time to load cpu_adam op: 2.4176013469696045 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-10-21 16:57:46,741] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
[2025-10-21 16:57:48,882] [INFO] [engine.py:1325:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=4
	 self.mp_world_size=1
	 self.seq_dp_world_size=4
	 self.sequence_parallel_size=1
***********************************************
[2025-10-21 16:57:52,732] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-10-21 16:57:52,734] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-10-21 16:57:52,735] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-10-21 16:57:52,754] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-10-21 16:57:52,754] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-10-21 16:57:52,754] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2025-10-21 16:57:52,754] [INFO] [stage_1_and_2.py:151:__init__] Reduce bucket size 500000000
[2025-10-21 16:57:52,754] [INFO] [stage_1_and_2.py:152:__init__] Allgather bucket size 500000000
[2025-10-21 16:57:52,754] [INFO] [stage_1_and_2.py:153:__init__] CPU Offload: True
[2025-10-21 16:57:52,754] [INFO] [stage_1_and_2.py:154:__init__] Round robin gradient partitioning: False
[2025-10-21 16:58:02,940] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-10-21 16:58:02,941] [INFO] [utils.py:782:see_memory_usage] MA 4.91 GB         Max_MA 4.91 GB         CA 4.91 GB         Max_CA 5 GB 
[2025-10-21 16:58:02,941] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 61.44 GB, percent = 6.1%
[2025-10-21 16:58:03,224] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-10-21 16:58:03,225] [INFO] [utils.py:782:see_memory_usage] MA 4.91 GB         Max_MA 4.91 GB         CA 4.91 GB         Max_CA 5 GB 
[2025-10-21 16:58:03,225] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 63.29 GB, percent = 6.3%
[2025-10-21 16:58:03,225] [INFO] [stage_1_and_2.py:573:__init__] optimizer state initialized
[2025-10-21 16:58:03,403] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-10-21 16:58:03,404] [INFO] [utils.py:782:see_memory_usage] MA 4.91 GB         Max_MA 4.91 GB         CA 4.91 GB         Max_CA 5 GB 
[2025-10-21 16:58:03,404] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 63.29 GB, percent = 6.3%
[2025-10-21 16:58:03,406] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-10-21 16:58:03,406] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-10-21 16:58:03,406] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x753a0c485f30>
[2025-10-21 16:58:03,407] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 16:58:03,407] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-10-21 16:58:03,408] [INFO] [config.py:921:print] DeepSpeedEngine configuration:
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   amp_enabled .................. False
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   amp_params ................... False
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   bfloat16_config .............. enabled=False immediate_grad_update=False check_grad_overflow=False
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   checkpoint_parallel_write_pipeline  False
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   checkpoint_tag_validation_enabled  True
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   checkpoint_tag_validation_fail  False
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x753a0c485660>
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   communication_data_type ...... None
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   curriculum_enabled_legacy .... False
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   curriculum_params_legacy ..... False
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   data_efficiency_enabled ...... False
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   dataloader_drop_last ......... False
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   disable_allgather ............ False
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   dump_state ................... False
[2025-10-21 16:58:03,408] [INFO] [config.py:925:print]   eigenvalue_enabled ........... False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   eigenvalue_gas_boundary_resolution  1
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   eigenvalue_layer_num ......... 0
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   eigenvalue_max_iter .......... 100
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   eigenvalue_stability ......... 1e-06
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   eigenvalue_tol ............... 0.01
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   eigenvalue_verbose ........... False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   elasticity_enabled ........... False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   global_rank .................. 0
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   grad_accum_dtype ............. None
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   gradient_accumulation_steps .. 8
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   gradient_clipping ............ 1.0
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   gradient_predivide_factor .... 1.0
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   graph_harvesting ............. False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   load_universal_checkpoint .... False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   memory_breakdown ............. False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   mics_hierarchial_params_gather  False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   mics_shard_size .............. -1
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=True, output_path='/data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/log//ds_tensorboard_logs/', job_name='v2_sft_tensorboard') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   optimizer_legacy_fusion ...... False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   optimizer_name ............... None
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   optimizer_params ............. None
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   pld_enabled .................. False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   pld_params ................... False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   prescale_gradients ........... False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   scheduler_name ............... None
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   scheduler_params ............. None
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   seq_parallel_communication_data_type  torch.float32
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   sparse_attention ............. None
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   sparse_gradients_enabled ..... False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   steps_per_print .............. 10
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   timers_config ................ enabled=True synchronized=True
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   train_batch_size ............. 64
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   train_micro_batch_size_per_gpu  2
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   use_data_before_expert_parallel_  False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   use_node_local_storage ....... False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   wall_clock_breakdown ......... False
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   weight_quantization_config ... None
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   world_size ................... 4
[2025-10-21 16:58:03,409] [INFO] [config.py:925:print]   zero_allow_untested_optimizer  False
[2025-10-21 16:58:03,410] [INFO] [config.py:925:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-10-21 16:58:03,410] [INFO] [config.py:925:print]   zero_enabled ................. True
[2025-10-21 16:58:03,410] [INFO] [config.py:925:print]   zero_force_ds_cpu_optimizer .. True
[2025-10-21 16:58:03,410] [INFO] [config.py:925:print]   zero_optimization_stage ...... 2
[2025-10-21 16:58:03,410] [INFO] [config.py:911:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 2, 
    "steps_per_print": 10, 
    "zero_optimization": {
        "stage": 2, 
        "offload_param": {
            "device": "cpu"
        }, 
        "offload_optimizer": {
            "device": "cpu"
        }, 
        "stage3_param_persistence_threshold": 1.000000e+04, 
        "stage3_max_live_parameters": 3.000000e+07, 
        "stage3_prefetch_bucket_size": 3.000000e+07, 
        "memory_efficient_linear": false
    }, 
    "bfloat16": {
        "enabled": "auto"
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "hybrid_engine": {
        "enabled": false, 
        "max_out_tokens": 512, 
        "inference_tp_size": 1, 
        "release_inference_cache": false, 
        "pin_parameters": true, 
        "tp_gather_partition_size": 8
    }, 
    "tensorboard": {
        "enabled": true, 
        "output_path": "/data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/log//ds_tensorboard_logs/", 
        "job_name": "v2_sft_tensorboard"
    }
}
***** Running training *****
Beginning of Epoch 1/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 1/5, step 0.0 *****
[eval loss, ppl] step:0.0, 	loss: 3.8583433628082275, 	ppl: 109.03621673583984
[eval_CSTANCE loss, ppl] step:0.0, 	loss: 0.43296298384666443, 	ppl: 1.6879239082336426
[eval_20Minuten loss, ppl] step:0.0, 	loss: 2.0902514457702637, 	ppl: 8.562322616577148
[eval_FOMC loss, ppl] step:0.0, 	loss: 0.5365403890609741, 	ppl: 1.6158171892166138
[eval_NumGLUEcm loss, ppl] step:0.0, 	loss: 4.358842372894287, 	ppl: 130.29507446289062
[eval_ScienceQA loss, ppl] step:0.0, 	loss: 0.9817591905593872, 	ppl: 2.637650728225708
[eval_NumGLUEds loss, ppl] step:0.0, 	loss: 5.33801794052124, 	ppl: 174.10865783691406
[eval_Py150 loss, ppl] step:0.0, 	loss: 3.36698055267334, 	ppl: 25.886045455932617
[eval_MeetingBank loss, ppl] step:0.0, 	loss: 1.6364799737930298, 	ppl: 4.999930381774902
***** Evaluating perplexity, Epoch 1/5, step 1.0 *****
[eval loss, ppl] step:1.0, 	loss: 1.7116925716400146, 	ppl: 9.036148071289062
[eval_CSTANCE loss, ppl] step:1.0, 	loss: 0.42497923970222473, 	ppl: 1.6541318893432617
[eval_20Minuten loss, ppl] step:1.0, 	loss: 2.060784101486206, 	ppl: 8.328309059143066
[eval_FOMC loss, ppl] step:1.0, 	loss: 0.5001353621482849, 	ppl: 1.5170549154281616
[eval_NumGLUEcm loss, ppl] step:1.0, 	loss: 1.89716374874115, 	ppl: 11.043145179748535
[eval_ScienceQA loss, ppl] step:1.0, 	loss: 0.982587456703186, 	ppl: 2.6403255462646484
[eval_NumGLUEds loss, ppl] step:1.0, 	loss: 3.078751564025879, 	ppl: 24.370664596557617
[eval_Py150 loss, ppl] step:1.0, 	loss: 3.257411241531372, 	ppl: 23.29206085205078
[eval_MeetingBank loss, ppl] step:1.0, 	loss: 1.6315877437591553, 	ppl: 4.944046974182129
***** Evaluating perplexity, Epoch 1/5, step 2.0 *****
[eval loss, ppl] step:2.0, 	loss: 1.1477949619293213, 	ppl: 3.611664056777954
[eval_CSTANCE loss, ppl] step:2.0, 	loss: 0.42574450373649597, 	ppl: 1.6319448947906494
[eval_20Minuten loss, ppl] step:2.0, 	loss: 2.033449172973633, 	ppl: 8.105096817016602
[eval_FOMC loss, ppl] step:2.0, 	loss: 0.48123809695243835, 	ppl: 1.4854445457458496
[eval_NumGLUEcm loss, ppl] step:2.0, 	loss: 1.169667363166809, 	ppl: 3.9338083267211914
[eval_ScienceQA loss, ppl] step:2.0, 	loss: 0.9876684546470642, 	ppl: 2.657693862915039
[eval_NumGLUEds loss, ppl] step:2.0, 	loss: 1.6600223779678345, 	ppl: 7.11430025100708
[eval_Py150 loss, ppl] step:2.0, 	loss: 3.1311848163604736, 	ppl: 20.652002334594727
[eval_MeetingBank loss, ppl] step:2.0, 	loss: 1.6282007694244385, 	ppl: 4.899405002593994
***** Evaluating perplexity, Epoch 1/5, step 3.0 *****
[eval loss, ppl] step:3.0, 	loss: 1.1313228607177734, 	ppl: 3.431786060333252
[eval_CSTANCE loss, ppl] step:3.0, 	loss: 0.4244641363620758, 	ppl: 1.6296552419662476
[eval_20Minuten loss, ppl] step:3.0, 	loss: 2.0192389488220215, 	ppl: 8.007555961608887
[eval_FOMC loss, ppl] step:3.0, 	loss: 0.4817730188369751, 	ppl: 1.4934080839157104
[eval_NumGLUEcm loss, ppl] step:3.0, 	loss: 1.1235477924346924, 	ppl: 3.702009677886963
[eval_ScienceQA loss, ppl] step:3.0, 	loss: 0.9947926998138428, 	ppl: 2.6753976345062256
[eval_NumGLUEds loss, ppl] step:3.0, 	loss: 1.50199294090271, 	ppl: 5.804332256317139
[eval_Py150 loss, ppl] step:3.0, 	loss: 3.0475845336914062, 	ppl: 19.11278533935547
[eval_MeetingBank loss, ppl] step:3.0, 	loss: 1.6270251274108887, 	ppl: 4.8926591873168945
***** Evaluating perplexity, Epoch 1/5, step 4.0 *****
[eval loss, ppl] step:4.0, 	loss: 1.1408350467681885, 	ppl: 3.4247019290924072
[eval_CSTANCE loss, ppl] step:4.0, 	loss: 0.4173266887664795, 	ppl: 1.602726936340332
[eval_20Minuten loss, ppl] step:4.0, 	loss: 2.003814935684204, 	ppl: 7.888482093811035
[eval_FOMC loss, ppl] step:4.0, 	loss: 0.4663655459880829, 	ppl: 1.4804478883743286
[eval_NumGLUEcm loss, ppl] step:4.0, 	loss: 1.104600191116333, 	ppl: 3.666203737258911
[eval_ScienceQA loss, ppl] step:4.0, 	loss: 1.0024821758270264, 	ppl: 2.7001094818115234
[eval_NumGLUEds loss, ppl] step:4.0, 	loss: 1.4216221570968628, 	ppl: 5.16438627243042
[eval_Py150 loss, ppl] step:4.0, 	loss: 2.9502499103546143, 	ppl: 17.533660888671875
[eval_MeetingBank loss, ppl] step:4.0, 	loss: 1.627730369567871, 	ppl: 4.889054298400879
***** Evaluating perplexity, Epoch 1/5, step 5.0 *****
[eval loss, ppl] step:5.0, 	loss: 1.1574972867965698, 	ppl: 3.393787145614624
[eval_CSTANCE loss, ppl] step:5.0, 	loss: 0.41638562083244324, 	ppl: 1.5979803800582886
[eval_20Minuten loss, ppl] step:5.0, 	loss: 1.9985181093215942, 	ppl: 7.838662147521973
[eval_FOMC loss, ppl] step:5.0, 	loss: 0.46263688802719116, 	ppl: 1.4847337007522583
[eval_NumGLUEcm loss, ppl] step:5.0, 	loss: 1.0859289169311523, 	ppl: 3.6517515182495117
[eval_ScienceQA loss, ppl] step:5.0, 	loss: 1.0105705261230469, 	ppl: 2.722283363342285
[eval_NumGLUEds loss, ppl] step:5.0, 	loss: 1.3841660022735596, 	ppl: 4.91574764251709
[eval_Py150 loss, ppl] step:5.0, 	loss: 2.9048516750335693, 	ppl: 16.706371307373047
[eval_MeetingBank loss, ppl] step:5.0, 	loss: 1.6298432350158691, 	ppl: 4.894615650177002
***** Evaluating perplexity, Epoch 1/5, step 6.0 *****
[eval loss, ppl] step:6.0, 	loss: 1.1500351428985596, 	ppl: 3.324209690093994
[eval_CSTANCE loss, ppl] step:6.0, 	loss: 0.41871562600135803, 	ppl: 1.5977184772491455
[eval_20Minuten loss, ppl] step:6.0, 	loss: 1.9919005632400513, 	ppl: 7.807328701019287
[eval_FOMC loss, ppl] step:6.0, 	loss: 0.46316224336624146, 	ppl: 1.4909205436706543
[eval_NumGLUEcm loss, ppl] step:6.0, 	loss: 1.0417286157608032, 	ppl: 3.5787038803100586
[eval_ScienceQA loss, ppl] step:6.0, 	loss: 1.0174150466918945, 	ppl: 2.7453348636627197
[eval_NumGLUEds loss, ppl] step:6.0, 	loss: 1.3333522081375122, 	ppl: 4.634854316711426
[eval_Py150 loss, ppl] step:6.0, 	loss: 2.8611233234405518, 	ppl: 16.019296646118164
[eval_MeetingBank loss, ppl] step:6.0, 	loss: 1.6308127641677856, 	ppl: 4.899468421936035
***** Evaluating perplexity, Epoch 1/5, step 7.0 *****
[eval loss, ppl] step:7.0, 	loss: 1.1243972778320312, 	ppl: 3.208526611328125
[eval_CSTANCE loss, ppl] step:7.0, 	loss: 0.4199680685997009, 	ppl: 1.596488356590271
[eval_20Minuten loss, ppl] step:7.0, 	loss: 1.9911141395568848, 	ppl: 7.785873889923096
[eval_FOMC loss, ppl] step:7.0, 	loss: 0.4627111554145813, 	ppl: 1.490995168685913
[eval_NumGLUEcm loss, ppl] step:7.0, 	loss: 0.9929332137107849, 	ppl: 3.458800792694092
[eval_ScienceQA loss, ppl] step:7.0, 	loss: 1.0237445831298828, 	ppl: 2.7645533084869385
[eval_NumGLUEds loss, ppl] step:7.0, 	loss: 1.2879585027694702, 	ppl: 4.453481674194336
[eval_Py150 loss, ppl] step:7.0, 	loss: 2.827821731567383, 	ppl: 15.494829177856445
[eval_MeetingBank loss, ppl] step:7.0, 	loss: 1.6323715448379517, 	ppl: 4.905795574188232
***** Evaluating perplexity, Epoch 1/5, step 8.0 *****
[eval loss, ppl] step:8.0, 	loss: 1.1178135871887207, 	ppl: 3.112954616546631
[eval_CSTANCE loss, ppl] step:8.0, 	loss: 0.41736337542533875, 	ppl: 1.595707654953003
[eval_20Minuten loss, ppl] step:8.0, 	loss: 1.9860423803329468, 	ppl: 7.776519298553467
[eval_FOMC loss, ppl] step:8.0, 	loss: 0.45482754707336426, 	ppl: 1.4893912076950073
[eval_NumGLUEcm loss, ppl] step:8.0, 	loss: 0.941695511341095, 	ppl: 3.38761830329895
[eval_ScienceQA loss, ppl] step:8.0, 	loss: 1.0299513339996338, 	ppl: 2.783280849456787
[eval_NumGLUEds loss, ppl] step:8.0, 	loss: 1.2381080389022827, 	ppl: 4.329362869262695
[eval_Py150 loss, ppl] step:8.0, 	loss: 2.802687168121338, 	ppl: 15.148632049560547
[eval_MeetingBank loss, ppl] step:8.0, 	loss: 1.6326658725738525, 	ppl: 4.914490699768066
***** Evaluating perplexity, Epoch 1/5, step 9.0 *****
[eval loss, ppl] step:9.0, 	loss: 1.1030906438827515, 	ppl: 3.071780204772949
[eval_CSTANCE loss, ppl] step:9.0, 	loss: 0.4209529459476471, 	ppl: 1.5971903800964355
[eval_20Minuten loss, ppl] step:9.0, 	loss: 1.991032600402832, 	ppl: 7.79116153717041
[eval_FOMC loss, ppl] step:9.0, 	loss: 0.459323912858963, 	ppl: 1.4965351819992065
[eval_NumGLUEcm loss, ppl] step:9.0, 	loss: 0.9273763298988342, 	ppl: 3.358262538909912
[eval_ScienceQA loss, ppl] step:9.0, 	loss: 1.0353844165802002, 	ppl: 2.798877239227295
[eval_NumGLUEds loss, ppl] step:9.0, 	loss: 1.2149620056152344, 	ppl: 4.269398212432861
[eval_Py150 loss, ppl] step:9.0, 	loss: 2.779066324234009, 	ppl: 14.840538024902344
[eval_MeetingBank loss, ppl] step:9.0, 	loss: 1.6334376335144043, 	ppl: 4.917288303375244
[2025-10-21 17:01:43,080] [INFO] [logging.py:107:log_dist] [Rank 0] step=10, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 17:01:43,296] [INFO] [timer.py:264:stop] epoch=0/micro_step=80/global_step=10, RunningAvgSamplesPerSec=4.923917247918028, CurrSamplesPerSec=4.873993054695841, MemAllocated=8.84GB, MaxMemAllocated=13.82GB
***** Evaluating perplexity, Epoch 1/5, step 10.0 *****
[eval loss, ppl] step:10.0, 	loss: 1.0793449878692627, 	ppl: 2.983386993408203
[eval_CSTANCE loss, ppl] step:10.0, 	loss: 0.42412593960762024, 	ppl: 1.6055835485458374
[eval_20Minuten loss, ppl] step:10.0, 	loss: 1.9912869930267334, 	ppl: 7.794955730438232
[eval_FOMC loss, ppl] step:10.0, 	loss: 0.46223562955856323, 	ppl: 1.4967647790908813
[eval_NumGLUEcm loss, ppl] step:10.0, 	loss: 0.8939565420150757, 	ppl: 3.287771224975586
[eval_ScienceQA loss, ppl] step:10.0, 	loss: 1.0401004552841187, 	ppl: 2.8154478073120117
[eval_NumGLUEds loss, ppl] step:10.0, 	loss: 1.2054556608200073, 	ppl: 4.276208877563477
[eval_Py150 loss, ppl] step:10.0, 	loss: 2.7712953090667725, 	ppl: 14.642069816589355
[eval_MeetingBank loss, ppl] step:10.0, 	loss: 1.6362054347991943, 	ppl: 4.925247669219971
***** Evaluating perplexity, Epoch 1/5, step 11.0 *****
[eval loss, ppl] step:11.0, 	loss: 1.0679762363433838, 	ppl: 2.8824267387390137
[eval_CSTANCE loss, ppl] step:11.0, 	loss: 0.4222487807273865, 	ppl: 1.5981051921844482
[eval_20Minuten loss, ppl] step:11.0, 	loss: 1.991924524307251, 	ppl: 7.80095911026001
[eval_FOMC loss, ppl] step:11.0, 	loss: 0.4586297273635864, 	ppl: 1.4950774908065796
[eval_NumGLUEcm loss, ppl] step:11.0, 	loss: 0.8402263522148132, 	ppl: 3.1927218437194824
[eval_ScienceQA loss, ppl] step:11.0, 	loss: 1.0442311763763428, 	ppl: 2.8303728103637695
[eval_NumGLUEds loss, ppl] step:11.0, 	loss: 1.1733465194702148, 	ppl: 4.231334209442139
[eval_Py150 loss, ppl] step:11.0, 	loss: 2.7494583129882812, 	ppl: 14.395009994506836
[eval_MeetingBank loss, ppl] step:11.0, 	loss: 1.6350605487823486, 	ppl: 4.936330318450928
***** Evaluating perplexity, Epoch 1/5, step 12.0 *****
[eval loss, ppl] step:12.0, 	loss: 1.0441139936447144, 	ppl: 2.8030431270599365
[eval_CSTANCE loss, ppl] step:12.0, 	loss: 0.43063950538635254, 	ppl: 1.6061530113220215
[eval_20Minuten loss, ppl] step:12.0, 	loss: 1.9909930229187012, 	ppl: 7.805673122406006
[eval_FOMC loss, ppl] step:12.0, 	loss: 0.45833882689476013, 	ppl: 1.4923253059387207
[eval_NumGLUEcm loss, ppl] step:12.0, 	loss: 0.8091363310813904, 	ppl: 3.114734172821045
[eval_ScienceQA loss, ppl] step:12.0, 	loss: 1.0487499237060547, 	ppl: 2.8451640605926514
[eval_NumGLUEds loss, ppl] step:12.0, 	loss: 1.1627599000930786, 	ppl: 4.271682262420654
[eval_Py150 loss, ppl] step:12.0, 	loss: 2.746670961380005, 	ppl: 14.2998046875
[eval_MeetingBank loss, ppl] step:12.0, 	loss: 1.6372437477111816, 	ppl: 4.940737724304199
***** Evaluating perplexity, Epoch 1/5, step 13.0 *****
[eval loss, ppl] step:13.0, 	loss: 0.9961423873901367, 	ppl: 2.7023653984069824
[eval_CSTANCE loss, ppl] step:13.0, 	loss: 0.42459866404533386, 	ppl: 1.601341962814331
[eval_20Minuten loss, ppl] step:13.0, 	loss: 1.9923354387283325, 	ppl: 7.814231872558594
[eval_FOMC loss, ppl] step:13.0, 	loss: 0.45853152871131897, 	ppl: 1.500108242034912
[eval_NumGLUEcm loss, ppl] step:13.0, 	loss: 0.757361888885498, 	ppl: 3.019077777862549
[eval_ScienceQA loss, ppl] step:13.0, 	loss: 1.0533472299575806, 	ppl: 2.8595476150512695
[eval_NumGLUEds loss, ppl] step:13.0, 	loss: 1.1324944496154785, 	ppl: 4.278460502624512
[eval_Py150 loss, ppl] step:13.0, 	loss: 2.7314956188201904, 	ppl: 14.077634811401367
[eval_MeetingBank loss, ppl] step:13.0, 	loss: 1.6373943090438843, 	ppl: 4.947305679321289
***** Evaluating perplexity, Epoch 1/5, step 14.0 *****
[eval loss, ppl] step:14.0, 	loss: 0.9619105458259583, 	ppl: 2.625866413116455
[eval_CSTANCE loss, ppl] step:14.0, 	loss: 0.42289769649505615, 	ppl: 1.6008663177490234
[eval_20Minuten loss, ppl] step:14.0, 	loss: 1.9975956678390503, 	ppl: 7.8222455978393555
[eval_FOMC loss, ppl] step:14.0, 	loss: 0.4618993401527405, 	ppl: 1.4993903636932373
[eval_NumGLUEcm loss, ppl] step:14.0, 	loss: 0.7367358207702637, 	ppl: 2.9297032356262207
[eval_ScienceQA loss, ppl] step:14.0, 	loss: 1.0590204000473022, 	ppl: 2.8753981590270996
[eval_NumGLUEds loss, ppl] step:14.0, 	loss: 1.1301774978637695, 	ppl: 4.263237953186035
[eval_Py150 loss, ppl] step:14.0, 	loss: 2.7125062942504883, 	ppl: 13.878901481628418
[eval_MeetingBank loss, ppl] step:14.0, 	loss: 1.6371158361434937, 	ppl: 4.953710556030273
saving model to /data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/1...
Sucessful saving model after epoch 1
Beginning of Epoch 2/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 2/5, step 0.0 *****
[eval loss, ppl] step:15.625, 	loss: 0.9053918123245239, 	ppl: 2.5554006099700928
[eval_CSTANCE loss, ppl] step:15.625, 	loss: 0.4259909987449646, 	ppl: 1.6054669618606567
[eval_20Minuten loss, ppl] step:15.625, 	loss: 2.0012922286987305, 	ppl: 7.852137088775635
[eval_FOMC loss, ppl] step:15.625, 	loss: 0.4624561369419098, 	ppl: 1.4987766742706299
[eval_NumGLUEcm loss, ppl] step:15.625, 	loss: 0.7113540768623352, 	ppl: 2.840130090713501
[eval_ScienceQA loss, ppl] step:15.625, 	loss: 1.0667215585708618, 	ppl: 2.9009103775024414
[eval_NumGLUEds loss, ppl] step:15.625, 	loss: 1.0940330028533936, 	ppl: 4.296483516693115
[eval_Py150 loss, ppl] step:15.625, 	loss: 2.6902737617492676, 	ppl: 13.630516052246094
[eval_MeetingBank loss, ppl] step:15.625, 	loss: 1.6401076316833496, 	ppl: 4.968317031860352
***** Evaluating perplexity, Epoch 2/5, step 1.0 *****
[eval loss, ppl] step:16.625, 	loss: 0.8872191905975342, 	ppl: 2.5078587532043457
[eval_CSTANCE loss, ppl] step:16.625, 	loss: 0.42141440510749817, 	ppl: 1.597415566444397
[eval_20Minuten loss, ppl] step:16.625, 	loss: 2.002286434173584, 	ppl: 7.851317405700684
[eval_FOMC loss, ppl] step:16.625, 	loss: 0.46687090396881104, 	ppl: 1.5054534673690796
[eval_NumGLUEcm loss, ppl] step:16.625, 	loss: 0.6883019804954529, 	ppl: 2.794614553451538
[eval_ScienceQA loss, ppl] step:16.625, 	loss: 1.0701143741607666, 	ppl: 2.9139299392700195
[eval_NumGLUEds loss, ppl] step:16.625, 	loss: 1.090965986251831, 	ppl: 4.322549343109131
[eval_Py150 loss, ppl] step:16.625, 	loss: 2.6772868633270264, 	ppl: 13.497794151306152
[eval_MeetingBank loss, ppl] step:16.625, 	loss: 1.6403443813323975, 	ppl: 4.968205451965332
***** Evaluating perplexity, Epoch 2/5, step 2.0 *****
[eval loss, ppl] step:17.625, 	loss: 0.864721953868866, 	ppl: 2.4719161987304688
[eval_CSTANCE loss, ppl] step:17.625, 	loss: 0.4238709807395935, 	ppl: 1.6036031246185303
[eval_20Minuten loss, ppl] step:17.625, 	loss: 2.0045249462127686, 	ppl: 7.866415500640869
[eval_FOMC loss, ppl] step:17.625, 	loss: 0.45764949917793274, 	ppl: 1.5048828125
[eval_NumGLUEcm loss, ppl] step:17.625, 	loss: 0.671774685382843, 	ppl: 2.775010585784912
[eval_ScienceQA loss, ppl] step:17.625, 	loss: 1.0730375051498413, 	ppl: 2.922102928161621
[eval_NumGLUEds loss, ppl] step:17.625, 	loss: 1.0960315465927124, 	ppl: 4.306712627410889
[eval_Py150 loss, ppl] step:17.625, 	loss: 2.6722803115844727, 	ppl: 13.467528343200684
[eval_MeetingBank loss, ppl] step:17.625, 	loss: 1.6412756443023682, 	ppl: 4.969109535217285
***** Evaluating perplexity, Epoch 2/5, step 3.0 *****
[eval loss, ppl] step:18.625, 	loss: 0.8507024049758911, 	ppl: 2.4182167053222656
[eval_CSTANCE loss, ppl] step:18.625, 	loss: 0.4237816631793976, 	ppl: 1.6008273363113403
[eval_20Minuten loss, ppl] step:18.625, 	loss: 2.0018205642700195, 	ppl: 7.869592666625977
[eval_FOMC loss, ppl] step:18.625, 	loss: 0.45510613918304443, 	ppl: 1.5028051137924194
[eval_NumGLUEcm loss, ppl] step:18.625, 	loss: 0.6172360777854919, 	ppl: 2.706557273864746
[eval_ScienceQA loss, ppl] step:18.625, 	loss: 1.0754034519195557, 	ppl: 2.9311683177948
[eval_NumGLUEds loss, ppl] step:18.625, 	loss: 1.1018459796905518, 	ppl: 4.323896884918213
[eval_Py150 loss, ppl] step:18.625, 	loss: 2.665787935256958, 	ppl: 13.34826374053955
[eval_MeetingBank loss, ppl] step:18.625, 	loss: 1.6428290605545044, 	ppl: 4.975728511810303
[2025-10-21 17:04:34,929] [INFO] [logging.py:107:log_dist] [Rank 0] step=20, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 17:04:35,111] [INFO] [timer.py:264:stop] epoch=0/micro_step=160/global_step=20, RunningAvgSamplesPerSec=4.990280167210595, CurrSamplesPerSec=4.960539171102257, MemAllocated=8.8GB, MaxMemAllocated=13.83GB
***** Evaluating perplexity, Epoch 2/5, step 4.0 *****
[eval loss, ppl] step:19.625, 	loss: 0.8305208086967468, 	ppl: 2.3725268840789795
[eval_CSTANCE loss, ppl] step:19.625, 	loss: 0.42125484347343445, 	ppl: 1.605099081993103
[eval_20Minuten loss, ppl] step:19.625, 	loss: 2.0029067993164062, 	ppl: 7.869925022125244
[eval_FOMC loss, ppl] step:19.625, 	loss: 0.45737817883491516, 	ppl: 1.5059354305267334
[eval_NumGLUEcm loss, ppl] step:19.625, 	loss: 0.606995701789856, 	ppl: 2.661900281906128
[eval_ScienceQA loss, ppl] step:19.625, 	loss: 1.0784810781478882, 	ppl: 2.9375720024108887
[eval_NumGLUEds loss, ppl] step:19.625, 	loss: 1.1165229082107544, 	ppl: 4.328676223754883
[eval_Py150 loss, ppl] step:19.625, 	loss: 2.653055191040039, 	ppl: 13.2293119430542
[eval_MeetingBank loss, ppl] step:19.625, 	loss: 1.6422173976898193, 	ppl: 4.973456859588623
***** Evaluating perplexity, Epoch 2/5, step 5.0 *****
[eval loss, ppl] step:20.625, 	loss: 0.8088851571083069, 	ppl: 2.3700244426727295
[eval_CSTANCE loss, ppl] step:20.625, 	loss: 0.42155036330223083, 	ppl: 1.6034088134765625
[eval_20Minuten loss, ppl] step:20.625, 	loss: 2.005079984664917, 	ppl: 7.884337902069092
[eval_FOMC loss, ppl] step:20.625, 	loss: 0.45790964365005493, 	ppl: 1.5060800313949585
[eval_NumGLUEcm loss, ppl] step:20.625, 	loss: 0.588326096534729, 	ppl: 2.6633760929107666
[eval_ScienceQA loss, ppl] step:20.625, 	loss: 1.0792897939682007, 	ppl: 2.9438018798828125
[eval_NumGLUEds loss, ppl] step:20.625, 	loss: 1.1233844757080078, 	ppl: 4.333532810211182
[eval_Py150 loss, ppl] step:20.625, 	loss: 2.642953872680664, 	ppl: 13.189159393310547
[eval_MeetingBank loss, ppl] step:20.625, 	loss: 1.6415678262710571, 	ppl: 4.97274112701416
***** Evaluating perplexity, Epoch 2/5, step 6.0 *****
[eval loss, ppl] step:21.625, 	loss: 0.7983823418617249, 	ppl: 2.3451521396636963
[eval_CSTANCE loss, ppl] step:21.625, 	loss: 0.42585909366607666, 	ppl: 1.6138447523117065
[eval_20Minuten loss, ppl] step:21.625, 	loss: 2.005650520324707, 	ppl: 7.8851118087768555
[eval_FOMC loss, ppl] step:21.625, 	loss: 0.45734208822250366, 	ppl: 1.5109959840774536
[eval_NumGLUEcm loss, ppl] step:21.625, 	loss: 0.5595803260803223, 	ppl: 2.636676549911499
[eval_ScienceQA loss, ppl] step:21.625, 	loss: 1.0803042650222778, 	ppl: 2.9487431049346924
[eval_NumGLUEds loss, ppl] step:21.625, 	loss: 1.1357473134994507, 	ppl: 4.353296279907227
[eval_Py150 loss, ppl] step:21.625, 	loss: 2.641967296600342, 	ppl: 13.110909461975098
[eval_MeetingBank loss, ppl] step:21.625, 	loss: 1.6406272649765015, 	ppl: 4.974474906921387
***** Evaluating perplexity, Epoch 2/5, step 7.0 *****
[eval loss, ppl] step:22.625, 	loss: 0.7925126552581787, 	ppl: 2.363356113433838
[eval_CSTANCE loss, ppl] step:22.625, 	loss: 0.4257214665412903, 	ppl: 1.607332706451416
[eval_20Minuten loss, ppl] step:22.625, 	loss: 2.0061075687408447, 	ppl: 7.889214515686035
[eval_FOMC loss, ppl] step:22.625, 	loss: 0.4591011106967926, 	ppl: 1.5107117891311646
[eval_NumGLUEcm loss, ppl] step:22.625, 	loss: 0.5551925897598267, 	ppl: 2.656346321105957
[eval_ScienceQA loss, ppl] step:22.625, 	loss: 1.0818169116973877, 	ppl: 2.9521896839141846
[eval_NumGLUEds loss, ppl] step:22.625, 	loss: 1.145261526107788, 	ppl: 4.402463436126709
[eval_Py150 loss, ppl] step:22.625, 	loss: 2.6412858963012695, 	ppl: 13.030247688293457
[eval_MeetingBank loss, ppl] step:22.625, 	loss: 1.644284725189209, 	ppl: 4.976315021514893
***** Evaluating perplexity, Epoch 2/5, step 8.0 *****
[eval loss, ppl] step:23.625, 	loss: 0.7905669808387756, 	ppl: 2.346644639968872
[eval_CSTANCE loss, ppl] step:23.625, 	loss: 0.4211918115615845, 	ppl: 1.6035994291305542
[eval_20Minuten loss, ppl] step:23.625, 	loss: 2.005776882171631, 	ppl: 7.888178825378418
[eval_FOMC loss, ppl] step:23.625, 	loss: 0.45834723114967346, 	ppl: 1.513159990310669
[eval_NumGLUEcm loss, ppl] step:23.625, 	loss: 0.5447430610656738, 	ppl: 2.637307643890381
[eval_ScienceQA loss, ppl] step:23.625, 	loss: 1.0829331874847412, 	ppl: 2.957247257232666
[eval_NumGLUEds loss, ppl] step:23.625, 	loss: 1.150909185409546, 	ppl: 4.444709300994873
[eval_Py150 loss, ppl] step:23.625, 	loss: 2.638629674911499, 	ppl: 12.914669036865234
[eval_MeetingBank loss, ppl] step:23.625, 	loss: 1.644952416419983, 	ppl: 4.976597785949707
***** Evaluating perplexity, Epoch 2/5, step 9.0 *****
[eval loss, ppl] step:24.625, 	loss: 0.7858914732933044, 	ppl: 2.3362998962402344
[eval_CSTANCE loss, ppl] step:24.625, 	loss: 0.4178159832954407, 	ppl: 1.609043836593628
[eval_20Minuten loss, ppl] step:24.625, 	loss: 2.0064895153045654, 	ppl: 7.900337219238281
[eval_FOMC loss, ppl] step:24.625, 	loss: 0.45289623737335205, 	ppl: 1.5103998184204102
[eval_NumGLUEcm loss, ppl] step:24.625, 	loss: 0.530732274055481, 	ppl: 2.6132378578186035
[eval_ScienceQA loss, ppl] step:24.625, 	loss: 1.084301471710205, 	ppl: 2.962674617767334
[eval_NumGLUEds loss, ppl] step:24.625, 	loss: 1.149882197380066, 	ppl: 4.467742919921875
[eval_Py150 loss, ppl] step:24.625, 	loss: 2.6264495849609375, 	ppl: 12.915453910827637
[eval_MeetingBank loss, ppl] step:24.625, 	loss: 1.6439505815505981, 	ppl: 4.977688789367676
***** Evaluating perplexity, Epoch 2/5, step 10.0 *****
[eval loss, ppl] step:25.625, 	loss: 0.774332582950592, 	ppl: 2.3342270851135254
[eval_CSTANCE loss, ppl] step:25.625, 	loss: 0.4232625663280487, 	ppl: 1.6050269603729248
[eval_20Minuten loss, ppl] step:25.625, 	loss: 2.004939079284668, 	ppl: 7.9045090675354
[eval_FOMC loss, ppl] step:25.625, 	loss: 0.454528272151947, 	ppl: 1.5121102333068848
[eval_NumGLUEcm loss, ppl] step:25.625, 	loss: 0.5234554409980774, 	ppl: 2.617225170135498
[eval_ScienceQA loss, ppl] step:25.625, 	loss: 1.08529531955719, 	ppl: 2.9664177894592285
[eval_NumGLUEds loss, ppl] step:25.625, 	loss: 1.144189715385437, 	ppl: 4.471871852874756
[eval_Py150 loss, ppl] step:25.625, 	loss: 2.6297004222869873, 	ppl: 12.952766418457031
[eval_MeetingBank loss, ppl] step:25.625, 	loss: 1.643540859222412, 	ppl: 4.983451843261719
***** Evaluating perplexity, Epoch 2/5, step 11.0 *****
[eval loss, ppl] step:26.625, 	loss: 0.7612259984016418, 	ppl: 2.299956798553467
[eval_CSTANCE loss, ppl] step:26.625, 	loss: 0.42788898944854736, 	ppl: 1.6052048206329346
[eval_20Minuten loss, ppl] step:26.625, 	loss: 2.0105361938476562, 	ppl: 7.917515754699707
[eval_FOMC loss, ppl] step:26.625, 	loss: 0.4605396091938019, 	ppl: 1.5195890665054321
[eval_NumGLUEcm loss, ppl] step:26.625, 	loss: 0.5203335285186768, 	ppl: 2.577132225036621
[eval_ScienceQA loss, ppl] step:26.625, 	loss: 1.0876246690750122, 	ppl: 2.9735782146453857
[eval_NumGLUEds loss, ppl] step:26.625, 	loss: 1.1521024703979492, 	ppl: 4.493198871612549
[eval_Py150 loss, ppl] step:26.625, 	loss: 2.6319360733032227, 	ppl: 12.968055725097656
[eval_MeetingBank loss, ppl] step:26.625, 	loss: 1.6441576480865479, 	ppl: 4.984108924865723
***** Evaluating perplexity, Epoch 2/5, step 12.0 *****
[eval loss, ppl] step:27.625, 	loss: 0.7340572476387024, 	ppl: 2.269623041152954
[eval_CSTANCE loss, ppl] step:27.625, 	loss: 0.41848790645599365, 	ppl: 1.6070750951766968
[eval_20Minuten loss, ppl] step:27.625, 	loss: 2.0083343982696533, 	ppl: 7.921551704406738
[eval_FOMC loss, ppl] step:27.625, 	loss: 0.46130335330963135, 	ppl: 1.5122393369674683
[eval_NumGLUEcm loss, ppl] step:27.625, 	loss: 0.5016367435455322, 	ppl: 2.5412282943725586
[eval_ScienceQA loss, ppl] step:27.625, 	loss: 1.0893062353134155, 	ppl: 2.9804434776306152
[eval_NumGLUEds loss, ppl] step:27.625, 	loss: 1.1221786737442017, 	ppl: 4.475801944732666
[eval_Py150 loss, ppl] step:27.625, 	loss: 2.6417527198791504, 	ppl: 13.043598175048828
[eval_MeetingBank loss, ppl] step:27.625, 	loss: 1.6449620723724365, 	ppl: 4.995183944702148
***** Evaluating perplexity, Epoch 2/5, step 13.0 *****
[eval loss, ppl] step:28.625, 	loss: 0.7268955111503601, 	ppl: 2.2584025859832764
[eval_CSTANCE loss, ppl] step:28.625, 	loss: 0.420826256275177, 	ppl: 1.6012645959854126
[eval_20Minuten loss, ppl] step:28.625, 	loss: 2.0109705924987793, 	ppl: 7.9183807373046875
[eval_FOMC loss, ppl] step:28.625, 	loss: 0.45320990681648254, 	ppl: 1.5166484117507935
[eval_NumGLUEcm loss, ppl] step:28.625, 	loss: 0.4936240613460541, 	ppl: 2.541585922241211
[eval_ScienceQA loss, ppl] step:28.625, 	loss: 1.0913889408111572, 	ppl: 2.985989809036255
[eval_NumGLUEds loss, ppl] step:28.625, 	loss: 1.1003801822662354, 	ppl: 4.4334025382995605
[eval_Py150 loss, ppl] step:28.625, 	loss: 2.6402838230133057, 	ppl: 13.069551467895508
[eval_MeetingBank loss, ppl] step:28.625, 	loss: 1.6458874940872192, 	ppl: 4.9968132972717285
[2025-10-21 17:07:21,326] [INFO] [logging.py:107:log_dist] [Rank 0] step=30, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 17:07:21,519] [INFO] [timer.py:264:stop] epoch=0/micro_step=240/global_step=30, RunningAvgSamplesPerSec=5.029441800612827, CurrSamplesPerSec=5.123962518018468, MemAllocated=8.92GB, MaxMemAllocated=13.83GB
***** Evaluating perplexity, Epoch 2/5, step 14.0 *****
[eval loss, ppl] step:29.625, 	loss: 0.7128121256828308, 	ppl: 2.239563465118408
[eval_CSTANCE loss, ppl] step:29.625, 	loss: 0.4214235842227936, 	ppl: 1.5976265668869019
[eval_20Minuten loss, ppl] step:29.625, 	loss: 2.0120623111724854, 	ppl: 7.9334001541137695
[eval_FOMC loss, ppl] step:29.625, 	loss: 0.4625066816806793, 	ppl: 1.5149716138839722
[eval_NumGLUEcm loss, ppl] step:29.625, 	loss: 0.48753684759140015, 	ppl: 2.526677370071411
[eval_ScienceQA loss, ppl] step:29.625, 	loss: 1.0931262969970703, 	ppl: 2.9921412467956543
[eval_NumGLUEds loss, ppl] step:29.625, 	loss: 1.0905287265777588, 	ppl: 4.474442005157471
[eval_Py150 loss, ppl] step:29.625, 	loss: 2.6450231075286865, 	ppl: 13.13290786743164
[eval_MeetingBank loss, ppl] step:29.625, 	loss: 1.645234227180481, 	ppl: 5.003106117248535
saving model to /data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/2...
Sucessful saving model after epoch 2
Beginning of Epoch 3/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 3/5, step 0.0 *****
[eval loss, ppl] step:31.25, 	loss: 0.6991178393363953, 	ppl: 2.1693003177642822
[eval_CSTANCE loss, ppl] step:31.25, 	loss: 0.4206600487232208, 	ppl: 1.6050631999969482
[eval_20Minuten loss, ppl] step:31.25, 	loss: 2.013362407684326, 	ppl: 7.948097229003906
[eval_FOMC loss, ppl] step:31.25, 	loss: 0.4561402499675751, 	ppl: 1.5151073932647705
[eval_NumGLUEcm loss, ppl] step:31.25, 	loss: 0.44481542706489563, 	ppl: 2.4387426376342773
[eval_ScienceQA loss, ppl] step:31.25, 	loss: 1.0945552587509155, 	ppl: 2.998589515686035
[eval_NumGLUEds loss, ppl] step:31.25, 	loss: 1.0966993570327759, 	ppl: 4.536318302154541
[eval_Py150 loss, ppl] step:31.25, 	loss: 2.641585111618042, 	ppl: 13.123111724853516
[eval_MeetingBank loss, ppl] step:31.25, 	loss: 1.645938515663147, 	ppl: 5.008528709411621
***** Evaluating perplexity, Epoch 3/5, step 1.0 *****
[eval loss, ppl] step:32.25, 	loss: 0.7075994610786438, 	ppl: 2.169421911239624
[eval_CSTANCE loss, ppl] step:32.25, 	loss: 0.41982582211494446, 	ppl: 1.6035159826278687
[eval_20Minuten loss, ppl] step:32.25, 	loss: 2.0139451026916504, 	ppl: 7.9516401290893555
[eval_FOMC loss, ppl] step:32.25, 	loss: 0.4561835527420044, 	ppl: 1.5168232917785645
[eval_NumGLUEcm loss, ppl] step:32.25, 	loss: 0.43925580382347107, 	ppl: 2.4295225143432617
[eval_ScienceQA loss, ppl] step:32.25, 	loss: 1.0949863195419312, 	ppl: 2.99764084815979
[eval_NumGLUEds loss, ppl] step:32.25, 	loss: 1.1025387048721313, 	ppl: 4.547606468200684
[eval_Py150 loss, ppl] step:32.25, 	loss: 2.6431307792663574, 	ppl: 13.108485221862793
[eval_MeetingBank loss, ppl] step:32.25, 	loss: 1.6462022066116333, 	ppl: 5.008050441741943
***** Evaluating perplexity, Epoch 3/5, step 2.0 *****
[eval loss, ppl] step:33.25, 	loss: 0.7243941426277161, 	ppl: 2.165632486343384
[eval_CSTANCE loss, ppl] step:33.25, 	loss: 0.42331477999687195, 	ppl: 1.606628656387329
[eval_20Minuten loss, ppl] step:33.25, 	loss: 2.016981601715088, 	ppl: 7.958404064178467
[eval_FOMC loss, ppl] step:33.25, 	loss: 0.4508521556854248, 	ppl: 1.512067198753357
[eval_NumGLUEcm loss, ppl] step:33.25, 	loss: 0.4304906725883484, 	ppl: 2.411365032196045
[eval_ScienceQA loss, ppl] step:33.25, 	loss: 1.0928277969360352, 	ppl: 2.9949538707733154
[eval_NumGLUEds loss, ppl] step:33.25, 	loss: 1.121639609336853, 	ppl: 4.555856704711914
[eval_Py150 loss, ppl] step:33.25, 	loss: 2.6437416076660156, 	ppl: 13.090529441833496
[eval_MeetingBank loss, ppl] step:33.25, 	loss: 1.645532250404358, 	ppl: 5.002699851989746
***** Evaluating perplexity, Epoch 3/5, step 3.0 *****
[eval loss, ppl] step:34.25, 	loss: 0.7386893630027771, 	ppl: 2.154478073120117
[eval_CSTANCE loss, ppl] step:34.25, 	loss: 0.4202611446380615, 	ppl: 1.610010027885437
[eval_20Minuten loss, ppl] step:34.25, 	loss: 2.0133676528930664, 	ppl: 7.955458641052246
[eval_FOMC loss, ppl] step:34.25, 	loss: 0.45278510451316833, 	ppl: 1.5158674716949463
[eval_NumGLUEcm loss, ppl] step:34.25, 	loss: 0.4319155216217041, 	ppl: 2.3866236209869385
[eval_ScienceQA loss, ppl] step:34.25, 	loss: 1.091853380203247, 	ppl: 2.9938454627990723
[eval_NumGLUEds loss, ppl] step:34.25, 	loss: 1.1385343074798584, 	ppl: 4.643119812011719
[eval_Py150 loss, ppl] step:34.25, 	loss: 2.6372182369232178, 	ppl: 13.025883674621582
[eval_MeetingBank loss, ppl] step:34.25, 	loss: 1.6453644037246704, 	ppl: 4.998161315917969
***** Evaluating perplexity, Epoch 3/5, step 4.0 *****
[eval loss, ppl] step:35.25, 	loss: 0.7340065836906433, 	ppl: 2.153542995452881
[eval_CSTANCE loss, ppl] step:35.25, 	loss: 0.42582693696022034, 	ppl: 1.6094942092895508
[eval_20Minuten loss, ppl] step:35.25, 	loss: 2.0163352489471436, 	ppl: 7.9619574546813965
[eval_FOMC loss, ppl] step:35.25, 	loss: 0.45567214488983154, 	ppl: 1.5194991827011108
[eval_NumGLUEcm loss, ppl] step:35.25, 	loss: 0.42977577447891235, 	ppl: 2.3783700466156006
[eval_ScienceQA loss, ppl] step:35.25, 	loss: 1.0908985137939453, 	ppl: 2.991319417953491
[eval_NumGLUEds loss, ppl] step:35.25, 	loss: 1.1458380222320557, 	ppl: 4.71958065032959
[eval_Py150 loss, ppl] step:35.25, 	loss: 2.646375894546509, 	ppl: 13.118629455566406
[eval_MeetingBank loss, ppl] step:35.25, 	loss: 1.644719123840332, 	ppl: 5.004586219787598
***** Evaluating perplexity, Epoch 3/5, step 5.0 *****
[eval loss, ppl] step:36.25, 	loss: 0.7299110293388367, 	ppl: 2.1442408561706543
[eval_CSTANCE loss, ppl] step:36.25, 	loss: 0.42498213052749634, 	ppl: 1.6099529266357422
[eval_20Minuten loss, ppl] step:36.25, 	loss: 2.0179638862609863, 	ppl: 7.977825164794922
[eval_FOMC loss, ppl] step:36.25, 	loss: 0.4503522217273712, 	ppl: 1.51318359375
[eval_NumGLUEcm loss, ppl] step:36.25, 	loss: 0.4282664656639099, 	ppl: 2.3643271923065186
[eval_ScienceQA loss, ppl] step:36.25, 	loss: 1.0912909507751465, 	ppl: 2.989100694656372
[eval_NumGLUEds loss, ppl] step:36.25, 	loss: 1.175856590270996, 	ppl: 4.841882228851318
[eval_Py150 loss, ppl] step:36.25, 	loss: 2.6430139541625977, 	ppl: 13.136218070983887
[eval_MeetingBank loss, ppl] step:36.25, 	loss: 1.6444239616394043, 	ppl: 4.994986534118652
***** Evaluating perplexity, Epoch 3/5, step 6.0 *****
[eval loss, ppl] step:37.25, 	loss: 0.7080187797546387, 	ppl: 2.1167993545532227
[eval_CSTANCE loss, ppl] step:37.25, 	loss: 0.42374640703201294, 	ppl: 1.6092125177383423
[eval_20Minuten loss, ppl] step:37.25, 	loss: 2.020601987838745, 	ppl: 7.992397308349609
[eval_FOMC loss, ppl] step:37.25, 	loss: 0.4604170322418213, 	ppl: 1.515601396560669
[eval_NumGLUEcm loss, ppl] step:37.25, 	loss: 0.4254331588745117, 	ppl: 2.3276619911193848
[eval_ScienceQA loss, ppl] step:37.25, 	loss: 1.0900838375091553, 	ppl: 2.9868698120117188
[eval_NumGLUEds loss, ppl] step:37.25, 	loss: 1.1911814212799072, 	ppl: 4.9564924240112305
[eval_Py150 loss, ppl] step:37.25, 	loss: 2.6430976390838623, 	ppl: 13.16221809387207
[eval_MeetingBank loss, ppl] step:37.25, 	loss: 1.6423349380493164, 	ppl: 4.9929351806640625
***** Evaluating perplexity, Epoch 3/5, step 7.0 *****
[eval loss, ppl] step:38.25, 	loss: 0.6958650350570679, 	ppl: 2.091651439666748
[eval_CSTANCE loss, ppl] step:38.25, 	loss: 0.4186554253101349, 	ppl: 1.6138007640838623
[eval_20Minuten loss, ppl] step:38.25, 	loss: 2.0235397815704346, 	ppl: 8.00318431854248
[eval_FOMC loss, ppl] step:38.25, 	loss: 0.4520871639251709, 	ppl: 1.5160034894943237
[eval_NumGLUEcm loss, ppl] step:38.25, 	loss: 0.42532601952552795, 	ppl: 2.2967605590820312
[eval_ScienceQA loss, ppl] step:38.25, 	loss: 1.0902540683746338, 	ppl: 2.9853289127349854
[eval_NumGLUEds loss, ppl] step:38.25, 	loss: 1.1720852851867676, 	ppl: 5.0278520584106445
[eval_Py150 loss, ppl] step:38.25, 	loss: 2.6538312435150146, 	ppl: 13.217583656311035
[eval_MeetingBank loss, ppl] step:38.25, 	loss: 1.6434717178344727, 	ppl: 4.991844654083252
[2025-10-21 17:10:10,545] [INFO] [logging.py:107:log_dist] [Rank 0] step=40, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 17:10:10,720] [INFO] [timer.py:264:stop] epoch=0/micro_step=320/global_step=40, RunningAvgSamplesPerSec=5.061290440927177, CurrSamplesPerSec=5.118664587234474, MemAllocated=8.8GB, MaxMemAllocated=13.83GB
***** Evaluating perplexity, Epoch 3/5, step 8.0 *****
[eval loss, ppl] step:39.25, 	loss: 0.6706959009170532, 	ppl: 2.045635223388672
[eval_CSTANCE loss, ppl] step:39.25, 	loss: 0.4138948619365692, 	ppl: 1.6133720874786377
[eval_20Minuten loss, ppl] step:39.25, 	loss: 2.025181770324707, 	ppl: 8.01439094543457
[eval_FOMC loss, ppl] step:39.25, 	loss: 0.45059525966644287, 	ppl: 1.521461844444275
[eval_NumGLUEcm loss, ppl] step:39.25, 	loss: 0.4327833950519562, 	ppl: 2.2396602630615234
[eval_ScienceQA loss, ppl] step:39.25, 	loss: 1.0911787748336792, 	ppl: 2.987276792526245
[eval_NumGLUEds loss, ppl] step:39.25, 	loss: 1.1753984689712524, 	ppl: 5.095447063446045
[eval_Py150 loss, ppl] step:39.25, 	loss: 2.667412281036377, 	ppl: 13.329627990722656
[eval_MeetingBank loss, ppl] step:39.25, 	loss: 1.6428943872451782, 	ppl: 4.995616436004639
***** Evaluating perplexity, Epoch 3/5, step 9.0 *****
[eval loss, ppl] step:40.25, 	loss: 0.6470142006874084, 	ppl: 2.017329454421997
[eval_CSTANCE loss, ppl] step:40.25, 	loss: 0.4172115921974182, 	ppl: 1.6168394088745117
[eval_20Minuten loss, ppl] step:40.25, 	loss: 2.0220751762390137, 	ppl: 8.014836311340332
[eval_FOMC loss, ppl] step:40.25, 	loss: 0.44734475016593933, 	ppl: 1.517467975616455
[eval_NumGLUEcm loss, ppl] step:40.25, 	loss: 0.4337199926376343, 	ppl: 2.2006304264068604
[eval_ScienceQA loss, ppl] step:40.25, 	loss: 1.0909607410430908, 	ppl: 2.9879531860351562
[eval_NumGLUEds loss, ppl] step:40.25, 	loss: 1.1847445964813232, 	ppl: 5.168943881988525
[eval_Py150 loss, ppl] step:40.25, 	loss: 2.6751275062561035, 	ppl: 13.426700592041016
[eval_MeetingBank loss, ppl] step:40.25, 	loss: 1.6437431573867798, 	ppl: 5.001834869384766
***** Evaluating perplexity, Epoch 3/5, step 10.0 *****
[eval loss, ppl] step:41.25, 	loss: 0.6245658993721008, 	ppl: 1.978144645690918
[eval_CSTANCE loss, ppl] step:41.25, 	loss: 0.42416611313819885, 	ppl: 1.6275060176849365
[eval_20Minuten loss, ppl] step:41.25, 	loss: 2.025179862976074, 	ppl: 8.031254768371582
[eval_FOMC loss, ppl] step:41.25, 	loss: 0.4472065269947052, 	ppl: 1.5109716653823853
[eval_NumGLUEcm loss, ppl] step:41.25, 	loss: 0.4467390477657318, 	ppl: 2.15451717376709
[eval_ScienceQA loss, ppl] step:41.25, 	loss: 1.092948317527771, 	ppl: 2.9937586784362793
[eval_NumGLUEds loss, ppl] step:41.25, 	loss: 1.1770458221435547, 	ppl: 5.214074611663818
[eval_Py150 loss, ppl] step:41.25, 	loss: 2.6800918579101562, 	ppl: 13.533825874328613
[eval_MeetingBank loss, ppl] step:41.25, 	loss: 1.6428862810134888, 	ppl: 5.003465175628662
***** Evaluating perplexity, Epoch 3/5, step 11.0 *****
[eval loss, ppl] step:42.25, 	loss: 0.6033378839492798, 	ppl: 1.9569785594940186
[eval_CSTANCE loss, ppl] step:42.25, 	loss: 0.42392194271087646, 	ppl: 1.626274824142456
[eval_20Minuten loss, ppl] step:42.25, 	loss: 2.026226282119751, 	ppl: 8.046769142150879
[eval_FOMC loss, ppl] step:42.25, 	loss: 0.4462805390357971, 	ppl: 1.518386960029602
[eval_NumGLUEcm loss, ppl] step:42.25, 	loss: 0.4405623972415924, 	ppl: 2.1252024173736572
[eval_ScienceQA loss, ppl] step:42.25, 	loss: 1.0958322286605835, 	ppl: 3.001816749572754
[eval_NumGLUEds loss, ppl] step:42.25, 	loss: 1.1305679082870483, 	ppl: 5.133835792541504
[eval_Py150 loss, ppl] step:42.25, 	loss: 2.695953130722046, 	ppl: 13.726877212524414
[eval_MeetingBank loss, ppl] step:42.25, 	loss: 1.6449964046478271, 	ppl: 5.009681701660156
***** Evaluating perplexity, Epoch 3/5, step 12.0 *****
[eval loss, ppl] step:43.25, 	loss: 0.5836105346679688, 	ppl: 1.92673659324646
[eval_CSTANCE loss, ppl] step:43.25, 	loss: 0.419293075799942, 	ppl: 1.6237720251083374
[eval_20Minuten loss, ppl] step:43.25, 	loss: 2.02579927444458, 	ppl: 8.049077033996582
[eval_FOMC loss, ppl] step:43.25, 	loss: 0.4411580264568329, 	ppl: 1.51726496219635
[eval_NumGLUEcm loss, ppl] step:43.25, 	loss: 0.45346906781196594, 	ppl: 2.081700563430786
[eval_ScienceQA loss, ppl] step:43.25, 	loss: 1.0970512628555298, 	ppl: 3.0075433254241943
[eval_NumGLUEds loss, ppl] step:43.25, 	loss: 1.1331502199172974, 	ppl: 5.205781936645508
[eval_Py150 loss, ppl] step:43.25, 	loss: 2.702404737472534, 	ppl: 13.860319137573242
[eval_MeetingBank loss, ppl] step:43.25, 	loss: 1.6452302932739258, 	ppl: 5.01436710357666
***** Evaluating perplexity, Epoch 3/5, step 13.0 *****
[eval loss, ppl] step:44.25, 	loss: 0.5723528861999512, 	ppl: 1.9054200649261475
[eval_CSTANCE loss, ppl] step:44.25, 	loss: 0.42343670129776, 	ppl: 1.6334383487701416
[eval_20Minuten loss, ppl] step:44.25, 	loss: 2.0279412269592285, 	ppl: 8.06227970123291
[eval_FOMC loss, ppl] step:44.25, 	loss: 0.44547292590141296, 	ppl: 1.52312433719635
[eval_NumGLUEcm loss, ppl] step:44.25, 	loss: 0.4604426324367523, 	ppl: 2.0496578216552734
[eval_ScienceQA loss, ppl] step:44.25, 	loss: 1.1002556085586548, 	ppl: 3.0155277252197266
[eval_NumGLUEds loss, ppl] step:44.25, 	loss: 1.1371196508407593, 	ppl: 5.207611083984375
[eval_Py150 loss, ppl] step:44.25, 	loss: 2.7232015132904053, 	ppl: 13.963494300842285
[eval_MeetingBank loss, ppl] step:44.25, 	loss: 1.646794319152832, 	ppl: 5.022672176361084
***** Evaluating perplexity, Epoch 3/5, step 14.0 *****
[eval loss, ppl] step:45.25, 	loss: 0.563164234161377, 	ppl: 1.8983551263809204
[eval_CSTANCE loss, ppl] step:45.25, 	loss: 0.4260792136192322, 	ppl: 1.643394947052002
[eval_20Minuten loss, ppl] step:45.25, 	loss: 2.0296173095703125, 	ppl: 8.081486701965332
[eval_FOMC loss, ppl] step:45.25, 	loss: 0.45534107089042664, 	ppl: 1.523698329925537
[eval_NumGLUEcm loss, ppl] step:45.25, 	loss: 0.4813846945762634, 	ppl: 2.0357799530029297
[eval_ScienceQA loss, ppl] step:45.25, 	loss: 1.103041410446167, 	ppl: 3.02632999420166
[eval_NumGLUEds loss, ppl] step:45.25, 	loss: 1.115274429321289, 	ppl: 5.216246604919434
[eval_Py150 loss, ppl] step:45.25, 	loss: 2.729893684387207, 	ppl: 14.168787002563477
[eval_MeetingBank loss, ppl] step:45.25, 	loss: 1.6471220254898071, 	ppl: 5.029894828796387
saving model to /data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/3...
Sucessful saving model after epoch 3
Beginning of Epoch 4/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 4/5, step 0.0 *****
[eval loss, ppl] step:46.875, 	loss: 0.5422971248626709, 	ppl: 1.88258695602417
[eval_CSTANCE loss, ppl] step:46.875, 	loss: 0.42785903811454773, 	ppl: 1.641194224357605
[eval_20Minuten loss, ppl] step:46.875, 	loss: 2.0312397480010986, 	ppl: 8.102298736572266
[eval_FOMC loss, ppl] step:46.875, 	loss: 0.45813724398612976, 	ppl: 1.5338728427886963
[eval_NumGLUEcm loss, ppl] step:46.875, 	loss: 0.4868566393852234, 	ppl: 2.008718490600586
[eval_ScienceQA loss, ppl] step:46.875, 	loss: 1.1058731079101562, 	ppl: 3.0349979400634766
[eval_NumGLUEds loss, ppl] step:46.875, 	loss: 1.1023510694503784, 	ppl: 5.261553764343262
[eval_Py150 loss, ppl] step:46.875, 	loss: 2.7458953857421875, 	ppl: 14.30469799041748
[eval_MeetingBank loss, ppl] step:46.875, 	loss: 1.6490918397903442, 	ppl: 5.0393757820129395
***** Evaluating perplexity, Epoch 4/5, step 1.0 *****
[eval loss, ppl] step:47.875, 	loss: 0.542131245136261, 	ppl: 1.891663908958435
[eval_CSTANCE loss, ppl] step:47.875, 	loss: 0.432738333940506, 	ppl: 1.649283766746521
[eval_20Minuten loss, ppl] step:47.875, 	loss: 2.0332794189453125, 	ppl: 8.106290817260742
[eval_FOMC loss, ppl] step:47.875, 	loss: 0.4500862658023834, 	ppl: 1.5260270833969116
[eval_NumGLUEcm loss, ppl] step:47.875, 	loss: 0.5122935771942139, 	ppl: 2.014481544494629
[eval_ScienceQA loss, ppl] step:47.875, 	loss: 1.1082113981246948, 	ppl: 3.0431528091430664
[eval_NumGLUEds loss, ppl] step:47.875, 	loss: 1.0916764736175537, 	ppl: 5.279215335845947
[eval_Py150 loss, ppl] step:47.875, 	loss: 2.748087167739868, 	ppl: 14.396322250366211
[eval_MeetingBank loss, ppl] step:47.875, 	loss: 1.6475337743759155, 	ppl: 5.043964385986328
***** Evaluating perplexity, Epoch 4/5, step 2.0 *****
[eval loss, ppl] step:48.875, 	loss: 0.5324363708496094, 	ppl: 1.8826780319213867
[eval_CSTANCE loss, ppl] step:48.875, 	loss: 0.42896372079849243, 	ppl: 1.6473006010055542
[eval_20Minuten loss, ppl] step:48.875, 	loss: 2.0313971042633057, 	ppl: 8.102724075317383
[eval_FOMC loss, ppl] step:48.875, 	loss: 0.4550631046295166, 	ppl: 1.5318522453308105
[eval_NumGLUEcm loss, ppl] step:48.875, 	loss: 0.5136319994926453, 	ppl: 1.9974238872528076
[eval_ScienceQA loss, ppl] step:48.875, 	loss: 1.110801100730896, 	ppl: 3.0475564002990723
[eval_NumGLUEds loss, ppl] step:48.875, 	loss: 1.0881719589233398, 	ppl: 5.323927402496338
[eval_Py150 loss, ppl] step:48.875, 	loss: 2.7690513134002686, 	ppl: 14.499507904052734
[eval_MeetingBank loss, ppl] step:48.875, 	loss: 1.6499924659729004, 	ppl: 5.053346633911133
[2025-10-21 17:13:04,445] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 17:13:04,683] [INFO] [timer.py:264:stop] epoch=0/micro_step=400/global_step=50, RunningAvgSamplesPerSec=5.073034631278503, CurrSamplesPerSec=5.0978389937440935, MemAllocated=8.81GB, MaxMemAllocated=13.83GB
***** Evaluating perplexity, Epoch 4/5, step 3.0 *****
[eval loss, ppl] step:49.875, 	loss: 0.5444111824035645, 	ppl: 1.919021487236023
[eval_CSTANCE loss, ppl] step:49.875, 	loss: 0.4306018352508545, 	ppl: 1.6561946868896484
[eval_20Minuten loss, ppl] step:49.875, 	loss: 2.0318729877471924, 	ppl: 8.123642921447754
[eval_FOMC loss, ppl] step:49.875, 	loss: 0.45433512330055237, 	ppl: 1.532701015472412
[eval_NumGLUEcm loss, ppl] step:49.875, 	loss: 0.5489853024482727, 	ppl: 2.0363171100616455
[eval_ScienceQA loss, ppl] step:49.875, 	loss: 1.1108795404434204, 	ppl: 3.049241781234741
[eval_NumGLUEds loss, ppl] step:49.875, 	loss: 1.0828216075897217, 	ppl: 5.3525590896606445
[eval_Py150 loss, ppl] step:49.875, 	loss: 2.7712156772613525, 	ppl: 14.613064765930176
[eval_MeetingBank loss, ppl] step:49.875, 	loss: 1.6513346433639526, 	ppl: 5.059768199920654
***** Evaluating perplexity, Epoch 4/5, step 4.0 *****
[eval loss, ppl] step:50.875, 	loss: 0.5359592437744141, 	ppl: 1.897847294807434
[eval_CSTANCE loss, ppl] step:50.875, 	loss: 0.4325217008590698, 	ppl: 1.6464107036590576
[eval_20Minuten loss, ppl] step:50.875, 	loss: 2.0358691215515137, 	ppl: 8.134525299072266
[eval_FOMC loss, ppl] step:50.875, 	loss: 0.45659682154655457, 	ppl: 1.5366393327713013
[eval_NumGLUEcm loss, ppl] step:50.875, 	loss: 0.5556746125221252, 	ppl: 2.0135860443115234
[eval_ScienceQA loss, ppl] step:50.875, 	loss: 1.1117172241210938, 	ppl: 3.0506863594055176
[eval_NumGLUEds loss, ppl] step:50.875, 	loss: 1.094522476196289, 	ppl: 5.4099860191345215
[eval_Py150 loss, ppl] step:50.875, 	loss: 2.7760190963745117, 	ppl: 14.666861534118652
[eval_MeetingBank loss, ppl] step:50.875, 	loss: 1.6515175104141235, 	ppl: 5.061408519744873
***** Evaluating perplexity, Epoch 4/5, step 5.0 *****
[eval loss, ppl] step:51.875, 	loss: 0.5386499166488647, 	ppl: 1.8972020149230957
[eval_CSTANCE loss, ppl] step:51.875, 	loss: 0.43408748507499695, 	ppl: 1.6457395553588867
[eval_20Minuten loss, ppl] step:51.875, 	loss: 2.03595232963562, 	ppl: 8.134775161743164
[eval_FOMC loss, ppl] step:51.875, 	loss: 0.45732781291007996, 	ppl: 1.542801856994629
[eval_NumGLUEcm loss, ppl] step:51.875, 	loss: 0.566305935382843, 	ppl: 2.0068533420562744
[eval_ScienceQA loss, ppl] step:51.875, 	loss: 1.1120744943618774, 	ppl: 3.0526351928710938
[eval_NumGLUEds loss, ppl] step:51.875, 	loss: 1.0995140075683594, 	ppl: 5.444058418273926
[eval_Py150 loss, ppl] step:51.875, 	loss: 2.7775068283081055, 	ppl: 14.7041597366333
[eval_MeetingBank loss, ppl] step:51.875, 	loss: 1.6509307622909546, 	ppl: 5.059945583343506
***** Evaluating perplexity, Epoch 4/5, step 6.0 *****
[eval loss, ppl] step:52.875, 	loss: 0.5415301322937012, 	ppl: 1.8923842906951904
[eval_CSTANCE loss, ppl] step:52.875, 	loss: 0.430634081363678, 	ppl: 1.6445978879928589
[eval_20Minuten loss, ppl] step:52.875, 	loss: 2.034881830215454, 	ppl: 8.142066955566406
[eval_FOMC loss, ppl] step:52.875, 	loss: 0.45317530632019043, 	ppl: 1.5405374765396118
[eval_NumGLUEcm loss, ppl] step:52.875, 	loss: 0.5401721000671387, 	ppl: 2.007253408432007
[eval_ScienceQA loss, ppl] step:52.875, 	loss: 1.1127392053604126, 	ppl: 3.054208517074585
[eval_NumGLUEds loss, ppl] step:52.875, 	loss: 1.115635633468628, 	ppl: 5.419990062713623
[eval_Py150 loss, ppl] step:52.875, 	loss: 2.7841153144836426, 	ppl: 14.76510238647461
[eval_MeetingBank loss, ppl] step:52.875, 	loss: 1.6520133018493652, 	ppl: 5.06719970703125
***** Evaluating perplexity, Epoch 4/5, step 7.0 *****
[eval loss, ppl] step:53.875, 	loss: 0.5441102385520935, 	ppl: 1.8525761365890503
[eval_CSTANCE loss, ppl] step:53.875, 	loss: 0.4276583790779114, 	ppl: 1.6464297771453857
[eval_20Minuten loss, ppl] step:53.875, 	loss: 2.034872531890869, 	ppl: 8.14095401763916
[eval_FOMC loss, ppl] step:53.875, 	loss: 0.45750194787979126, 	ppl: 1.537757158279419
[eval_NumGLUEcm loss, ppl] step:53.875, 	loss: 0.5053567886352539, 	ppl: 1.9696261882781982
[eval_ScienceQA loss, ppl] step:53.875, 	loss: 1.112024188041687, 	ppl: 3.049926280975342
[eval_NumGLUEds loss, ppl] step:53.875, 	loss: 1.1278122663497925, 	ppl: 5.434350490570068
[eval_Py150 loss, ppl] step:53.875, 	loss: 2.7858572006225586, 	ppl: 14.800115585327148
[eval_MeetingBank loss, ppl] step:53.875, 	loss: 1.6509541273117065, 	ppl: 5.067354679107666
***** Evaluating perplexity, Epoch 4/5, step 8.0 *****
[eval loss, ppl] step:54.875, 	loss: 0.5356857776641846, 	ppl: 1.8207614421844482
[eval_CSTANCE loss, ppl] step:54.875, 	loss: 0.4259146451950073, 	ppl: 1.644948959350586
[eval_20Minuten loss, ppl] step:54.875, 	loss: 2.0359103679656982, 	ppl: 8.138132095336914
[eval_FOMC loss, ppl] step:54.875, 	loss: 0.4614650309085846, 	ppl: 1.545327067375183
[eval_NumGLUEcm loss, ppl] step:54.875, 	loss: 0.4667435586452484, 	ppl: 1.942427158355713
[eval_ScienceQA loss, ppl] step:54.875, 	loss: 1.1107956171035767, 	ppl: 3.0477347373962402
[eval_NumGLUEds loss, ppl] step:54.875, 	loss: 1.1343286037445068, 	ppl: 5.358940124511719
[eval_Py150 loss, ppl] step:54.875, 	loss: 2.782107353210449, 	ppl: 14.823817253112793
[eval_MeetingBank loss, ppl] step:54.875, 	loss: 1.6508313417434692, 	ppl: 5.061105728149414
***** Evaluating perplexity, Epoch 4/5, step 9.0 *****
[eval loss, ppl] step:55.875, 	loss: 0.5633233189582825, 	ppl: 1.8037137985229492
[eval_CSTANCE loss, ppl] step:55.875, 	loss: 0.43318063020706177, 	ppl: 1.6451970338821411
[eval_20Minuten loss, ppl] step:55.875, 	loss: 2.0363309383392334, 	ppl: 8.148107528686523
[eval_FOMC loss, ppl] step:55.875, 	loss: 0.4524664580821991, 	ppl: 1.5434997081756592
[eval_NumGLUEcm loss, ppl] step:55.875, 	loss: 0.39553189277648926, 	ppl: 1.9348795413970947
[eval_ScienceQA loss, ppl] step:55.875, 	loss: 1.1106525659561157, 	ppl: 3.043484687805176
[eval_NumGLUEds loss, ppl] step:55.875, 	loss: 1.154373049736023, 	ppl: 5.310042381286621
[eval_Py150 loss, ppl] step:55.875, 	loss: 2.78094220161438, 	ppl: 14.779382705688477
[eval_MeetingBank loss, ppl] step:55.875, 	loss: 1.6508418321609497, 	ppl: 5.064705848693848
***** Evaluating perplexity, Epoch 4/5, step 10.0 *****
[eval loss, ppl] step:56.875, 	loss: 0.6046063303947449, 	ppl: 1.809297800064087
[eval_CSTANCE loss, ppl] step:56.875, 	loss: 0.43521252274513245, 	ppl: 1.6504325866699219
[eval_20Minuten loss, ppl] step:56.875, 	loss: 2.0362918376922607, 	ppl: 8.14245319366455
[eval_FOMC loss, ppl] step:56.875, 	loss: 0.4491569697856903, 	ppl: 1.5376596450805664
[eval_NumGLUEcm loss, ppl] step:56.875, 	loss: 0.37015947699546814, 	ppl: 1.9501488208770752
[eval_ScienceQA loss, ppl] step:56.875, 	loss: 1.108106255531311, 	ppl: 3.0402889251708984
[eval_NumGLUEds loss, ppl] step:56.875, 	loss: 1.1582870483398438, 	ppl: 5.2802252769470215
[eval_Py150 loss, ppl] step:56.875, 	loss: 2.78708815574646, 	ppl: 14.841323852539062
[eval_MeetingBank loss, ppl] step:56.875, 	loss: 1.6509588956832886, 	ppl: 5.065492630004883
***** Evaluating perplexity, Epoch 4/5, step 11.0 *****
[eval loss, ppl] step:57.875, 	loss: 0.6319906115531921, 	ppl: 1.8134266138076782
[eval_CSTANCE loss, ppl] step:57.875, 	loss: 0.434760719537735, 	ppl: 1.6504486799240112
[eval_20Minuten loss, ppl] step:57.875, 	loss: 2.0364582538604736, 	ppl: 8.15432071685791
[eval_FOMC loss, ppl] step:57.875, 	loss: 0.45124247670173645, 	ppl: 1.541712760925293
[eval_NumGLUEcm loss, ppl] step:57.875, 	loss: 0.34109678864479065, 	ppl: 1.9592671394348145
[eval_ScienceQA loss, ppl] step:57.875, 	loss: 1.1087067127227783, 	ppl: 3.0399277210235596
[eval_NumGLUEds loss, ppl] step:57.875, 	loss: 1.180558204650879, 	ppl: 5.222238063812256
[eval_Py150 loss, ppl] step:57.875, 	loss: 2.784687042236328, 	ppl: 14.854985237121582
[eval_MeetingBank loss, ppl] step:57.875, 	loss: 1.6497302055358887, 	ppl: 5.063400745391846
***** Evaluating perplexity, Epoch 4/5, step 12.0 *****
[eval loss, ppl] step:58.875, 	loss: 0.6602285504341125, 	ppl: 1.8338379859924316
[eval_CSTANCE loss, ppl] step:58.875, 	loss: 0.44027748703956604, 	ppl: 1.648872971534729
[eval_20Minuten loss, ppl] step:58.875, 	loss: 2.0392143726348877, 	ppl: 8.157180786132812
[eval_FOMC loss, ppl] step:58.875, 	loss: 0.4449487626552582, 	ppl: 1.5443122386932373
[eval_NumGLUEcm loss, ppl] step:58.875, 	loss: 0.34625616669654846, 	ppl: 1.9861738681793213
[eval_ScienceQA loss, ppl] step:58.875, 	loss: 1.1081475019454956, 	ppl: 3.0383846759796143
[eval_NumGLUEds loss, ppl] step:58.875, 	loss: 1.1629037857055664, 	ppl: 5.222434997558594
[eval_Py150 loss, ppl] step:58.875, 	loss: 2.79886794090271, 	ppl: 14.910284042358398
[eval_MeetingBank loss, ppl] step:58.875, 	loss: 1.6484814882278442, 	ppl: 5.062943458557129
[2025-10-21 17:15:46,012] [INFO] [logging.py:107:log_dist] [Rank 0] step=60, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 17:15:46,192] [INFO] [timer.py:264:stop] epoch=0/micro_step=480/global_step=60, RunningAvgSamplesPerSec=5.110558411504661, CurrSamplesPerSec=5.306559733096209, MemAllocated=8.8GB, MaxMemAllocated=13.83GB
***** Evaluating perplexity, Epoch 4/5, step 13.0 *****
[eval loss, ppl] step:59.875, 	loss: 0.6799675822257996, 	ppl: 1.8535171747207642
[eval_CSTANCE loss, ppl] step:59.875, 	loss: 0.4390023946762085, 	ppl: 1.6465752124786377
[eval_20Minuten loss, ppl] step:59.875, 	loss: 2.0378270149230957, 	ppl: 8.156097412109375
[eval_FOMC loss, ppl] step:59.875, 	loss: 0.4582960605621338, 	ppl: 1.5447347164154053
[eval_NumGLUEcm loss, ppl] step:59.875, 	loss: 0.3446301519870758, 	ppl: 2.0141992568969727
[eval_ScienceQA loss, ppl] step:59.875, 	loss: 1.1093111038208008, 	ppl: 3.0416598320007324
[eval_NumGLUEds loss, ppl] step:59.875, 	loss: 1.1625407934188843, 	ppl: 5.225643157958984
[eval_Py150 loss, ppl] step:59.875, 	loss: 2.7942161560058594, 	ppl: 14.95598030090332
[eval_MeetingBank loss, ppl] step:59.875, 	loss: 1.6512645483016968, 	ppl: 5.071133136749268
***** Evaluating perplexity, Epoch 4/5, step 14.0 *****
[eval loss, ppl] step:60.875, 	loss: 0.6678892970085144, 	ppl: 1.8509891033172607
[eval_CSTANCE loss, ppl] step:60.875, 	loss: 0.43300747871398926, 	ppl: 1.6461704969406128
[eval_20Minuten loss, ppl] step:60.875, 	loss: 2.0392894744873047, 	ppl: 8.16153335571289
[eval_FOMC loss, ppl] step:60.875, 	loss: 0.4532104432582855, 	ppl: 1.5459167957305908
[eval_NumGLUEcm loss, ppl] step:60.875, 	loss: 0.3442208766937256, 	ppl: 2.0203864574432373
[eval_ScienceQA loss, ppl] step:60.875, 	loss: 1.1087937355041504, 	ppl: 3.041998863220215
[eval_NumGLUEds loss, ppl] step:60.875, 	loss: 1.1650516986846924, 	ppl: 5.34721040725708
[eval_Py150 loss, ppl] step:60.875, 	loss: 2.804415702819824, 	ppl: 15.028717994689941
[eval_MeetingBank loss, ppl] step:60.875, 	loss: 1.6503243446350098, 	ppl: 5.070087432861328
saving model to /data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/4...
Sucessful saving model after epoch 4
Beginning of Epoch 5/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 5/5, step 0.0 *****
[eval loss, ppl] step:62.5, 	loss: 0.6037111282348633, 	ppl: 1.8534095287322998
[eval_CSTANCE loss, ppl] step:62.5, 	loss: 0.4394535720348358, 	ppl: 1.660672664642334
[eval_20Minuten loss, ppl] step:62.5, 	loss: 2.0408995151519775, 	ppl: 8.175124168395996
[eval_FOMC loss, ppl] step:62.5, 	loss: 0.44720059633255005, 	ppl: 1.5534672737121582
[eval_NumGLUEcm loss, ppl] step:62.5, 	loss: 0.3566702902317047, 	ppl: 2.033082962036133
[eval_ScienceQA loss, ppl] step:62.5, 	loss: 1.111401915550232, 	ppl: 3.049487352371216
[eval_NumGLUEds loss, ppl] step:62.5, 	loss: 1.1476407051086426, 	ppl: 5.617817401885986
[eval_Py150 loss, ppl] step:62.5, 	loss: 2.815535545349121, 	ppl: 15.242799758911133
[eval_MeetingBank loss, ppl] step:62.5, 	loss: 1.651482105255127, 	ppl: 5.0763773918151855
***** Evaluating perplexity, Epoch 5/5, step 1.0 *****
[eval loss, ppl] step:63.5, 	loss: 0.5958753824234009, 	ppl: 1.8833954334259033
[eval_CSTANCE loss, ppl] step:63.5, 	loss: 0.435814768075943, 	ppl: 1.6578378677368164
[eval_20Minuten loss, ppl] step:63.5, 	loss: 2.040947675704956, 	ppl: 8.183563232421875
[eval_FOMC loss, ppl] step:63.5, 	loss: 0.45093366503715515, 	ppl: 1.559067964553833
[eval_NumGLUEcm loss, ppl] step:63.5, 	loss: 0.37607723474502563, 	ppl: 2.0721073150634766
[eval_ScienceQA loss, ppl] step:63.5, 	loss: 1.1124441623687744, 	ppl: 3.05273699760437
[eval_NumGLUEds loss, ppl] step:63.5, 	loss: 1.1460109949111938, 	ppl: 5.8104119300842285
[eval_Py150 loss, ppl] step:63.5, 	loss: 2.8277790546417236, 	ppl: 15.406106948852539
[eval_MeetingBank loss, ppl] step:63.5, 	loss: 1.6511512994766235, 	ppl: 5.082300186157227
***** Evaluating perplexity, Epoch 5/5, step 2.0 *****
[eval loss, ppl] step:64.5, 	loss: 0.5793541669845581, 	ppl: 1.9302384853363037
[eval_CSTANCE loss, ppl] step:64.5, 	loss: 0.4412252902984619, 	ppl: 1.655320167541504
[eval_20Minuten loss, ppl] step:64.5, 	loss: 2.0411219596862793, 	ppl: 8.191890716552734
[eval_FOMC loss, ppl] step:64.5, 	loss: 0.44798052310943604, 	ppl: 1.5548512935638428
[eval_NumGLUEcm loss, ppl] step:64.5, 	loss: 0.4161534309387207, 	ppl: 2.120069980621338
[eval_ScienceQA loss, ppl] step:64.5, 	loss: 1.114502191543579, 	ppl: 3.061593532562256
[eval_NumGLUEds loss, ppl] step:64.5, 	loss: 1.1240955591201782, 	ppl: 6.017672061920166
[eval_Py150 loss, ppl] step:64.5, 	loss: 2.8434295654296875, 	ppl: 15.542648315429688
[eval_MeetingBank loss, ppl] step:64.5, 	loss: 1.6516802310943604, 	ppl: 5.087436199188232
***** Evaluating perplexity, Epoch 5/5, step 3.0 *****
[eval loss, ppl] step:65.5, 	loss: 0.575191855430603, 	ppl: 1.9891952276229858
[eval_CSTANCE loss, ppl] step:65.5, 	loss: 0.4335865378379822, 	ppl: 1.6563770771026611
[eval_20Minuten loss, ppl] step:65.5, 	loss: 2.041456460952759, 	ppl: 8.197394371032715
[eval_FOMC loss, ppl] step:65.5, 	loss: 0.4571021795272827, 	ppl: 1.561763048171997
[eval_NumGLUEcm loss, ppl] step:65.5, 	loss: 0.45518845319747925, 	ppl: 2.1783034801483154
[eval_ScienceQA loss, ppl] step:65.5, 	loss: 1.1158818006515503, 	ppl: 3.06315016746521
[eval_NumGLUEds loss, ppl] step:65.5, 	loss: 1.1382054090499878, 	ppl: 6.253998279571533
[eval_Py150 loss, ppl] step:65.5, 	loss: 2.847754716873169, 	ppl: 15.696510314941406
[eval_MeetingBank loss, ppl] step:65.5, 	loss: 1.6527966260910034, 	ppl: 5.091488838195801
***** Evaluating perplexity, Epoch 5/5, step 4.0 *****
[eval loss, ppl] step:66.5, 	loss: 0.5843379497528076, 	ppl: 2.044016122817993
[eval_CSTANCE loss, ppl] step:66.5, 	loss: 0.43586471676826477, 	ppl: 1.6670993566513062
[eval_20Minuten loss, ppl] step:66.5, 	loss: 2.042099714279175, 	ppl: 8.205986022949219
[eval_FOMC loss, ppl] step:66.5, 	loss: 0.4584890604019165, 	ppl: 1.5649584531784058
[eval_NumGLUEcm loss, ppl] step:66.5, 	loss: 0.5044077634811401, 	ppl: 2.23185396194458
[eval_ScienceQA loss, ppl] step:66.5, 	loss: 1.1164499521255493, 	ppl: 3.0648176670074463
[eval_NumGLUEds loss, ppl] step:66.5, 	loss: 1.1177489757537842, 	ppl: 6.449916362762451
[eval_Py150 loss, ppl] step:66.5, 	loss: 2.8622167110443115, 	ppl: 15.727554321289062
[eval_MeetingBank loss, ppl] step:66.5, 	loss: 1.6537916660308838, 	ppl: 5.095834732055664
***** Evaluating perplexity, Epoch 5/5, step 5.0 *****
[eval loss, ppl] step:67.5, 	loss: 0.5954969525337219, 	ppl: 2.110675096511841
[eval_CSTANCE loss, ppl] step:67.5, 	loss: 0.4359370470046997, 	ppl: 1.6680715084075928
[eval_20Minuten loss, ppl] step:67.5, 	loss: 2.0431082248687744, 	ppl: 8.200801849365234
[eval_FOMC loss, ppl] step:67.5, 	loss: 0.4635113775730133, 	ppl: 1.5704357624053955
[eval_NumGLUEcm loss, ppl] step:67.5, 	loss: 0.5501540303230286, 	ppl: 2.2987608909606934
[eval_ScienceQA loss, ppl] step:67.5, 	loss: 1.1155744791030884, 	ppl: 3.060241460800171
[eval_NumGLUEds loss, ppl] step:67.5, 	loss: 1.146785855293274, 	ppl: 6.728973388671875
[eval_Py150 loss, ppl] step:67.5, 	loss: 2.865211009979248, 	ppl: 15.839082717895508
[eval_MeetingBank loss, ppl] step:67.5, 	loss: 1.6532272100448608, 	ppl: 5.094586372375488
***** Evaluating perplexity, Epoch 5/5, step 6.0 *****
[eval loss, ppl] step:68.5, 	loss: 0.6131282448768616, 	ppl: 2.1332733631134033
[eval_CSTANCE loss, ppl] step:68.5, 	loss: 0.4295518398284912, 	ppl: 1.662677526473999
[eval_20Minuten loss, ppl] step:68.5, 	loss: 2.044187545776367, 	ppl: 8.205060958862305
[eval_FOMC loss, ppl] step:68.5, 	loss: 0.4641001224517822, 	ppl: 1.569675087928772
[eval_NumGLUEcm loss, ppl] step:68.5, 	loss: 0.591210663318634, 	ppl: 2.3152880668640137
[eval_ScienceQA loss, ppl] step:68.5, 	loss: 1.1138010025024414, 	ppl: 3.055361032485962
[eval_NumGLUEds loss, ppl] step:68.5, 	loss: 1.155266523361206, 	ppl: 6.87401819229126
[eval_Py150 loss, ppl] step:68.5, 	loss: 2.866168737411499, 	ppl: 15.905496597290039
[eval_MeetingBank loss, ppl] step:68.5, 	loss: 1.6533594131469727, 	ppl: 5.095968246459961
[2025-10-21 17:18:32,693] [INFO] [logging.py:107:log_dist] [Rank 0] step=70, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 17:18:32,884] [INFO] [timer.py:264:stop] epoch=0/micro_step=560/global_step=70, RunningAvgSamplesPerSec=5.137119121853773, CurrSamplesPerSec=5.28026934027328, MemAllocated=8.81GB, MaxMemAllocated=13.83GB
***** Evaluating perplexity, Epoch 5/5, step 7.0 *****
[eval loss, ppl] step:69.5, 	loss: 0.6429657340049744, 	ppl: 2.169057607650757
[eval_CSTANCE loss, ppl] step:69.5, 	loss: 0.4378233253955841, 	ppl: 1.6695119142532349
[eval_20Minuten loss, ppl] step:69.5, 	loss: 2.043456554412842, 	ppl: 8.204826354980469
[eval_FOMC loss, ppl] step:69.5, 	loss: 0.4627656638622284, 	ppl: 1.5663058757781982
[eval_NumGLUEcm loss, ppl] step:69.5, 	loss: 0.609957218170166, 	ppl: 2.349250555038452
[eval_ScienceQA loss, ppl] step:69.5, 	loss: 1.1121090650558472, 	ppl: 3.0490493774414062
[eval_NumGLUEds loss, ppl] step:69.5, 	loss: 1.17471444606781, 	ppl: 7.047933578491211
[eval_Py150 loss, ppl] step:69.5, 	loss: 2.862056016921997, 	ppl: 15.879722595214844
[eval_MeetingBank loss, ppl] step:69.5, 	loss: 1.652620553970337, 	ppl: 5.087801933288574
***** Evaluating perplexity, Epoch 5/5, step 8.0 *****
[eval loss, ppl] step:70.5, 	loss: 0.6120489239692688, 	ppl: 2.1361377239227295
[eval_CSTANCE loss, ppl] step:70.5, 	loss: 0.4357995390892029, 	ppl: 1.6730557680130005
[eval_20Minuten loss, ppl] step:70.5, 	loss: 2.0411171913146973, 	ppl: 8.18532943725586
[eval_FOMC loss, ppl] step:70.5, 	loss: 0.4634590744972229, 	ppl: 1.5719789266586304
[eval_NumGLUEcm loss, ppl] step:70.5, 	loss: 0.596441388130188, 	ppl: 2.3146567344665527
[eval_ScienceQA loss, ppl] step:70.5, 	loss: 1.1110715866088867, 	ppl: 3.0454230308532715
[eval_NumGLUEds loss, ppl] step:70.5, 	loss: 1.1971899271011353, 	ppl: 7.087638854980469
[eval_Py150 loss, ppl] step:70.5, 	loss: 2.8788723945617676, 	ppl: 15.924354553222656
[eval_MeetingBank loss, ppl] step:70.5, 	loss: 1.6520529985427856, 	ppl: 5.086189270019531
***** Evaluating perplexity, Epoch 5/5, step 9.0 *****
[eval loss, ppl] step:71.5, 	loss: 0.6249945759773254, 	ppl: 2.0441014766693115
[eval_CSTANCE loss, ppl] step:71.5, 	loss: 0.43584102392196655, 	ppl: 1.6695518493652344
[eval_20Minuten loss, ppl] step:71.5, 	loss: 2.039297103881836, 	ppl: 8.185395240783691
[eval_FOMC loss, ppl] step:71.5, 	loss: 0.45370492339134216, 	ppl: 1.5629684925079346
[eval_NumGLUEcm loss, ppl] step:71.5, 	loss: 0.5663763880729675, 	ppl: 2.2278056144714355
[eval_ScienceQA loss, ppl] step:71.5, 	loss: 1.1093610525131226, 	ppl: 3.0401482582092285
[eval_NumGLUEds loss, ppl] step:71.5, 	loss: 1.2015938758850098, 	ppl: 6.97487735748291
[eval_Py150 loss, ppl] step:71.5, 	loss: 2.8695712089538574, 	ppl: 15.926090240478516
[eval_MeetingBank loss, ppl] step:71.5, 	loss: 1.6526817083358765, 	ppl: 5.088088512420654
***** Evaluating perplexity, Epoch 5/5, step 10.0 *****
[eval loss, ppl] step:72.5, 	loss: 0.6171231269836426, 	ppl: 1.9372389316558838
[eval_CSTANCE loss, ppl] step:72.5, 	loss: 0.4385956823825836, 	ppl: 1.6626484394073486
[eval_20Minuten loss, ppl] step:72.5, 	loss: 2.0391430854797363, 	ppl: 8.177339553833008
[eval_FOMC loss, ppl] step:72.5, 	loss: 0.45713162422180176, 	ppl: 1.5617551803588867
[eval_NumGLUEcm loss, ppl] step:72.5, 	loss: 0.5335947871208191, 	ppl: 2.1150407791137695
[eval_ScienceQA loss, ppl] step:72.5, 	loss: 1.1088916063308716, 	ppl: 3.0338950157165527
[eval_NumGLUEds loss, ppl] step:72.5, 	loss: 1.2183820009231567, 	ppl: 6.93872594833374
[eval_Py150 loss, ppl] step:72.5, 	loss: 2.871741533279419, 	ppl: 15.89361572265625
[eval_MeetingBank loss, ppl] step:72.5, 	loss: 1.6515766382217407, 	ppl: 5.081998348236084
***** Evaluating perplexity, Epoch 5/5, step 11.0 *****
[eval loss, ppl] step:73.5, 	loss: 0.6060101985931396, 	ppl: 1.8880401849746704
[eval_CSTANCE loss, ppl] step:73.5, 	loss: 0.43075889348983765, 	ppl: 1.6555261611938477
[eval_20Minuten loss, ppl] step:73.5, 	loss: 2.0398197174072266, 	ppl: 8.169439315795898
[eval_FOMC loss, ppl] step:73.5, 	loss: 0.4574550688266754, 	ppl: 1.556762456893921
[eval_NumGLUEcm loss, ppl] step:73.5, 	loss: 0.5300745964050293, 	ppl: 2.0482821464538574
[eval_ScienceQA loss, ppl] step:73.5, 	loss: 1.107202410697937, 	ppl: 3.0303730964660645
[eval_NumGLUEds loss, ppl] step:73.5, 	loss: 1.2282367944717407, 	ppl: 6.7900543212890625
[eval_Py150 loss, ppl] step:73.5, 	loss: 2.8706507682800293, 	ppl: 15.943807601928711
[eval_MeetingBank loss, ppl] step:73.5, 	loss: 1.650959849357605, 	ppl: 5.0780720710754395
***** Evaluating perplexity, Epoch 5/5, step 12.0 *****
[eval loss, ppl] step:74.5, 	loss: 0.6033211946487427, 	ppl: 1.8298736810684204
[eval_CSTANCE loss, ppl] step:74.5, 	loss: 0.4309041202068329, 	ppl: 1.6601901054382324
[eval_20Minuten loss, ppl] step:74.5, 	loss: 2.039245128631592, 	ppl: 8.161704063415527
[eval_FOMC loss, ppl] step:74.5, 	loss: 0.45883315801620483, 	ppl: 1.5637584924697876
[eval_NumGLUEcm loss, ppl] step:74.5, 	loss: 0.48829755187034607, 	ppl: 1.9803518056869507
[eval_ScienceQA loss, ppl] step:74.5, 	loss: 1.107475996017456, 	ppl: 3.0282607078552246
[eval_NumGLUEds loss, ppl] step:74.5, 	loss: 1.218141794204712, 	ppl: 6.5799713134765625
[eval_Py150 loss, ppl] step:74.5, 	loss: 2.8755760192871094, 	ppl: 15.965391159057617
[eval_MeetingBank loss, ppl] step:74.5, 	loss: 1.6511961221694946, 	ppl: 5.0843706130981445
***** Evaluating perplexity, Epoch 5/5, step 13.0 *****
[eval loss, ppl] step:75.5, 	loss: 0.5913759469985962, 	ppl: 1.793967843055725
[eval_CSTANCE loss, ppl] step:75.5, 	loss: 0.4294741749763489, 	ppl: 1.656356930732727
[eval_20Minuten loss, ppl] step:75.5, 	loss: 2.03700590133667, 	ppl: 8.153310775756836
[eval_FOMC loss, ppl] step:75.5, 	loss: 0.45938369631767273, 	ppl: 1.566105842590332
[eval_NumGLUEcm loss, ppl] step:75.5, 	loss: 0.45882394909858704, 	ppl: 1.9369069337844849
[eval_ScienceQA loss, ppl] step:75.5, 	loss: 1.1101486682891846, 	ppl: 3.031841516494751
[eval_NumGLUEds loss, ppl] step:75.5, 	loss: 1.1927095651626587, 	ppl: 6.370717525482178
[eval_Py150 loss, ppl] step:75.5, 	loss: 2.8750550746917725, 	ppl: 16.08943748474121
[eval_MeetingBank loss, ppl] step:75.5, 	loss: 1.6508448123931885, 	ppl: 5.090190887451172
***** Evaluating perplexity, Epoch 5/5, step 14.0 *****
[eval loss, ppl] step:76.5, 	loss: 0.5710111856460571, 	ppl: 1.785719871520996
[eval_CSTANCE loss, ppl] step:76.5, 	loss: 0.4369466006755829, 	ppl: 1.6607294082641602
[eval_20Minuten loss, ppl] step:76.5, 	loss: 2.0357635021209717, 	ppl: 8.152280807495117
[eval_FOMC loss, ppl] step:76.5, 	loss: 0.4567626118659973, 	ppl: 1.5705691576004028
[eval_NumGLUEcm loss, ppl] step:76.5, 	loss: 0.4318353533744812, 	ppl: 1.9388014078140259
[eval_ScienceQA loss, ppl] step:76.5, 	loss: 1.110992670059204, 	ppl: 3.03812837600708
[eval_NumGLUEds loss, ppl] step:76.5, 	loss: 1.1871579885482788, 	ppl: 6.194366455078125
[eval_Py150 loss, ppl] step:76.5, 	loss: 2.8854568004608154, 	ppl: 16.125694274902344
[eval_MeetingBank loss, ppl] step:76.5, 	loss: 1.65293550491333, 	ppl: 5.098422527313232
saving model to /data1/TAP/model_con/1020/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/5...
[2025-10-21 17:20:48,533] [INFO] [launch.py:351:main] Process 1246908 exits successfully.
[2025-10-21 17:20:48,533] [INFO] [launch.py:351:main] Process 1246909 exits successfully.
[2025-10-21 17:20:49,535] [INFO] [launch.py:351:main] Process 1246907 exits successfully.
Sucessful saving model after epoch 5
[2025-10-21 17:20:55,541] [INFO] [launch.py:351:main] Process 1246906 exits successfully.
