[2025-10-21 22:39:24,857] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 22:39:26,911] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 22:39:27,118] [WARNING] [runner.py:220:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-10-21 22:39:27,118] [INFO] [runner.py:610:main] cmd = /home/TAP/anaconda3/envs/llama2/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=27407 --enable_each_rank_log=None training/main.py --data_path /data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/NumGLUE-cm --model_name_or_path /data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/5 --per_device_train_batch_size 2 --per_device_eval_batch_size 1 --max_prompt_len 1024 --max_ans_len 512 --learning_rate 1e-5 --weight_decay 0. --num_train_epochs 5 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --num_warmup_steps 0 --seed 42 --zero_stage 2 --deepspeed --print_loss --CL_method base --enable_tensorboard --tensorboard_path /data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/log/ --offload --ood_eval_dir /data2/TAP/data/continue_eval_loss_data_small --mask_method ours --top_ratio 0.001 --target_name NumGLUE-cm --output_dir /data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001 --test_file_dir /data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000
[2025-10-21 22:39:29,326] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 22:39:31,373] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 22:39:31,577] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-10-21 22:39:31,577] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-10-21 22:39:31,577] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-10-21 22:39:31,577] [INFO] [launch.py:164:main] dist_world_size=4
[2025-10-21 22:39:31,577] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-10-21 22:39:31,577] [INFO] [launch.py:256:main] process 2055972 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=0', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/NumGLUE-cm', '--model_name_or_path', '/data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/5', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'ours', '--top_ratio', '0.001', '--target_name', 'NumGLUE-cm', '--output_dir', '/data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001', '--test_file_dir', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000']
[2025-10-21 22:39:31,578] [INFO] [launch.py:256:main] process 2055973 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=1', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/NumGLUE-cm', '--model_name_or_path', '/data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/5', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'ours', '--top_ratio', '0.001', '--target_name', 'NumGLUE-cm', '--output_dir', '/data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001', '--test_file_dir', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000']
[2025-10-21 22:39:31,579] [INFO] [launch.py:256:main] process 2055974 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=2', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/NumGLUE-cm', '--model_name_or_path', '/data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/5', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'ours', '--top_ratio', '0.001', '--target_name', 'NumGLUE-cm', '--output_dir', '/data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001', '--test_file_dir', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000']
[2025-10-21 22:39:31,580] [INFO] [launch.py:256:main] process 2055975 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=3', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/NumGLUE-cm', '--model_name_or_path', '/data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/5', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'ours', '--top_ratio', '0.001', '--target_name', 'NumGLUE-cm', '--output_dir', '/data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001', '--test_file_dir', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000']
[2025-10-21 22:39:35,397] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 22:39:35,398] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 22:39:35,430] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 22:39:35,431] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 22:39:37,283] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 22:39:37,285] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 22:39:37,343] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 22:39:37,350] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Using integrations.HfDeepSpeedConfig (new API)
Using integrations.HfDeepSpeedConfig (new API)
Using integrations.HfDeepSpeedConfig (new API)
Using integrations.HfDeepSpeedConfig (new API)
/data1/TAP/model_exp_2b/1020_NumGLUE-cm_ours_parameters_test_epoch1_random_1000/parameters_import_new_2/top0.001
/data1/TAP/model_exp_2b/1020_NumGLUE-cm_ours_parameters_test_epoch1_random_1000/parameters_import_new_2/top0.001
/data1/TAP/model_exp_2b/1020_NumGLUE-cm_ours_parameters_test_epoch1_random_1000/parameters_import_new_2/top0.001
[2025-10-21 22:39:38,524] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-10-21 22:39:38,524] [INFO] [comm.py:706:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/data1/TAP/model_exp_2b/1020_NumGLUE-cm_ours_parameters_test_epoch1_random_1000/parameters_import_new_2/top0.001
[2025-10-21 22:39:38,762] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-10-21 22:39:38,763] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-10-21 22:39:38,786] [INFO] [comm.py:675:init_distributed] cdb=None
ninja: no work to do.
Time to load cpu_adam op: 2.362989902496338 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-10-21 22:42:27,417] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
ninja: no work to do.
Time to load cpu_adam op: 2.4071314334869385 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-10-21 22:42:27,526] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
ninja: no work to do.
Time to load cpu_adam op: 2.4791958332061768 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-10-21 22:42:27,600] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.1, git-hash=unknown, git-branch=unknown
[2025-10-21 22:42:27,600] [INFO] [comm.py:700:init_distributed] Distributed backend already initialized
[2025-10-21 22:42:27,601] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
Time to load cpu_adam op: 2.571904182434082 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-10-21 22:42:27,697] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
[2025-10-21 22:42:30,757] [INFO] [engine.py:1325:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=4
	 self.mp_world_size=1
	 self.seq_dp_world_size=4
	 self.sequence_parallel_size=1
***********************************************
[2025-10-21 22:42:33,304] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-10-21 22:42:33,306] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-10-21 22:42:33,306] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-10-21 22:42:33,318] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-10-21 22:42:33,318] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-10-21 22:42:33,318] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2025-10-21 22:42:33,318] [INFO] [stage_1_and_2.py:151:__init__] Reduce bucket size 500000000
[2025-10-21 22:42:33,318] [INFO] [stage_1_and_2.py:152:__init__] Allgather bucket size 500000000
[2025-10-21 22:42:33,318] [INFO] [stage_1_and_2.py:153:__init__] CPU Offload: True
[2025-10-21 22:42:33,318] [INFO] [stage_1_and_2.py:154:__init__] Round robin gradient partitioning: False
[2025-10-21 22:42:43,927] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-10-21 22:42:43,928] [INFO] [utils.py:782:see_memory_usage] MA 4.91 GB         Max_MA 4.91 GB         CA 4.91 GB         Max_CA 5 GB 
[2025-10-21 22:42:43,928] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 59.09 GB, percent = 5.9%
[2025-10-21 22:42:44,237] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-10-21 22:42:44,238] [INFO] [utils.py:782:see_memory_usage] MA 4.91 GB         Max_MA 4.91 GB         CA 4.91 GB         Max_CA 5 GB 
[2025-10-21 22:42:44,238] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 60.77 GB, percent = 6.0%
[2025-10-21 22:42:44,238] [INFO] [stage_1_and_2.py:573:__init__] optimizer state initialized
[2025-10-21 22:42:44,426] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-10-21 22:42:44,427] [INFO] [utils.py:782:see_memory_usage] MA 4.91 GB         Max_MA 4.91 GB         CA 4.91 GB         Max_CA 5 GB 
[2025-10-21 22:42:44,427] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 60.77 GB, percent = 6.0%
[2025-10-21 22:42:44,429] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-10-21 22:42:44,429] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-10-21 22:42:44,429] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7177e42a9b10>
[2025-10-21 22:42:44,429] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 22:42:44,430] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-10-21 22:42:44,430] [INFO] [config.py:921:print] DeepSpeedEngine configuration:
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   amp_enabled .................. False
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   amp_params ................... False
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   bfloat16_config .............. enabled=False immediate_grad_update=False check_grad_overflow=False
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   checkpoint_parallel_write_pipeline  False
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   checkpoint_tag_validation_enabled  True
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   checkpoint_tag_validation_fail  False
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7177e42a9570>
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   communication_data_type ...... None
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   curriculum_enabled_legacy .... False
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   curriculum_params_legacy ..... False
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   data_efficiency_enabled ...... False
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   dataloader_drop_last ......... False
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   disable_allgather ............ False
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   dump_state ................... False
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   eigenvalue_enabled ........... False
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   eigenvalue_gas_boundary_resolution  1
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   eigenvalue_layer_num ......... 0
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   eigenvalue_max_iter .......... 100
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   eigenvalue_stability ......... 1e-06
[2025-10-21 22:42:44,431] [INFO] [config.py:925:print]   eigenvalue_tol ............... 0.01
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   eigenvalue_verbose ........... False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   elasticity_enabled ........... False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   global_rank .................. 0
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   grad_accum_dtype ............. None
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   gradient_accumulation_steps .. 8
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   gradient_clipping ............ 1.0
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   gradient_predivide_factor .... 1.0
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   graph_harvesting ............. False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   load_universal_checkpoint .... False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   memory_breakdown ............. False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   mics_hierarchial_params_gather  False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   mics_shard_size .............. -1
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=True, output_path='/data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/log//ds_tensorboard_logs/', job_name='v2_sft_tensorboard') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   optimizer_legacy_fusion ...... False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   optimizer_name ............... None
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   optimizer_params ............. None
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   pld_enabled .................. False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   pld_params ................... False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   prescale_gradients ........... False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   scheduler_name ............... None
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   scheduler_params ............. None
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   seq_parallel_communication_data_type  torch.float32
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   sparse_attention ............. None
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   sparse_gradients_enabled ..... False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   steps_per_print .............. 10
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   timers_config ................ enabled=True synchronized=True
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   train_batch_size ............. 64
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   train_micro_batch_size_per_gpu  2
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   use_data_before_expert_parallel_  False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   use_node_local_storage ....... False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   wall_clock_breakdown ......... False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   weight_quantization_config ... None
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   world_size ................... 4
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   zero_allow_untested_optimizer  False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   zero_enabled ................. True
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   zero_force_ds_cpu_optimizer .. True
[2025-10-21 22:42:44,432] [INFO] [config.py:925:print]   zero_optimization_stage ...... 2
[2025-10-21 22:42:44,433] [INFO] [config.py:911:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 2, 
    "steps_per_print": 10, 
    "zero_optimization": {
        "stage": 2, 
        "offload_param": {
            "device": "cpu"
        }, 
        "offload_optimizer": {
            "device": "cpu"
        }, 
        "stage3_param_persistence_threshold": 1.000000e+04, 
        "stage3_max_live_parameters": 3.000000e+07, 
        "stage3_prefetch_bucket_size": 3.000000e+07, 
        "memory_efficient_linear": false
    }, 
    "bfloat16": {
        "enabled": "auto"
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "hybrid_engine": {
        "enabled": false, 
        "max_out_tokens": 512, 
        "inference_tp_size": 1, 
        "release_inference_cache": false, 
        "pin_parameters": true, 
        "tp_gather_partition_size": 8
    }, 
    "tensorboard": {
        "enabled": true, 
        "output_path": "/data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/log//ds_tensorboard_logs/", 
        "job_name": "v2_sft_tensorboard"
    }
}
***** Running training *****
Beginning of Epoch 1/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 1/5, step 0.0 *****
[eval loss, ppl] step:0.0, 	loss: 4.362856388092041, 	ppl: 174.55697631835938
[eval_CSTANCE loss, ppl] step:0.0, 	loss: 0.4608002007007599, 	ppl: 1.7270795106887817
[eval_20Minuten loss, ppl] step:0.0, 	loss: 2.0115315914154053, 	ppl: 7.930330276489258
[eval_FOMC loss, ppl] step:0.0, 	loss: 0.7170357704162598, 	ppl: 1.900101900100708
[eval_NumGLUEcm loss, ppl] step:0.0, 	loss: 4.787375450134277, 	ppl: 192.36782836914062
[eval_ScienceQA loss, ppl] step:0.0, 	loss: 0.9636539816856384, 	ppl: 2.582911729812622
[eval_NumGLUEds loss, ppl] step:0.0, 	loss: 5.895094871520996, 	ppl: 256.04620361328125
[eval_Py150 loss, ppl] step:0.0, 	loss: 3.4836394786834717, 	ppl: 29.529624938964844
[eval_MeetingBank loss, ppl] step:0.0, 	loss: 1.662810206413269, 	ppl: 5.093377113342285
***** Evaluating perplexity, Epoch 1/5, step 1.0 *****
[eval loss, ppl] step:1.0, 	loss: 2.7226693630218506, 	ppl: 25.567123413085938
[eval_CSTANCE loss, ppl] step:1.0, 	loss: 0.4386186897754669, 	ppl: 1.6852941513061523
[eval_20Minuten loss, ppl] step:1.0, 	loss: 1.9988481998443604, 	ppl: 7.814431190490723
[eval_FOMC loss, ppl] step:1.0, 	loss: 0.5982682108879089, 	ppl: 1.693894386291504
[eval_NumGLUEcm loss, ppl] step:1.0, 	loss: 2.9895262718200684, 	ppl: 28.577373504638672
[eval_ScienceQA loss, ppl] step:1.0, 	loss: 0.9633463025093079, 	ppl: 2.5829639434814453
[eval_NumGLUEds loss, ppl] step:1.0, 	loss: 4.153736114501953, 	ppl: 53.59270477294922
[eval_Py150 loss, ppl] step:1.0, 	loss: 3.3792898654937744, 	ppl: 26.666532516479492
[eval_MeetingBank loss, ppl] step:1.0, 	loss: 1.6589300632476807, 	ppl: 5.048470973968506
***** Evaluating perplexity, Epoch 1/5, step 2.0 *****
[eval loss, ppl] step:2.0, 	loss: 1.4307082891464233, 	ppl: 5.214893817901611
[eval_CSTANCE loss, ppl] step:2.0, 	loss: 0.42085516452789307, 	ppl: 1.6560263633728027
[eval_20Minuten loss, ppl] step:2.0, 	loss: 1.9743025302886963, 	ppl: 7.660128593444824
[eval_FOMC loss, ppl] step:2.0, 	loss: 0.5655372738838196, 	ppl: 1.6095538139343262
[eval_NumGLUEcm loss, ppl] step:2.0, 	loss: 1.649744987487793, 	ppl: 5.713885307312012
[eval_ScienceQA loss, ppl] step:2.0, 	loss: 0.9661243557929993, 	ppl: 2.591996192932129
[eval_NumGLUEds loss, ppl] step:2.0, 	loss: 2.2357771396636963, 	ppl: 10.472412109375
[eval_Py150 loss, ppl] step:2.0, 	loss: 3.238196849822998, 	ppl: 23.325735092163086
[eval_MeetingBank loss, ppl] step:2.0, 	loss: 1.654670000076294, 	ppl: 5.005448818206787
***** Evaluating perplexity, Epoch 1/5, step 3.0 *****
[eval loss, ppl] step:3.0, 	loss: 1.2125961780548096, 	ppl: 3.9781932830810547
[eval_CSTANCE loss, ppl] step:3.0, 	loss: 0.41193658113479614, 	ppl: 1.6436476707458496
[eval_20Minuten loss, ppl] step:3.0, 	loss: 1.9663279056549072, 	ppl: 7.5908098220825195
[eval_FOMC loss, ppl] step:3.0, 	loss: 0.552832305431366, 	ppl: 1.5955477952957153
[eval_NumGLUEcm loss, ppl] step:3.0, 	loss: 1.3160454034805298, 	ppl: 4.311900615692139
[eval_ScienceQA loss, ppl] step:3.0, 	loss: 0.9695473313331604, 	ppl: 2.603217601776123
[eval_NumGLUEds loss, ppl] step:3.0, 	loss: 1.7716591358184814, 	ppl: 7.314834117889404
[eval_Py150 loss, ppl] step:3.0, 	loss: 3.139806032180786, 	ppl: 21.300689697265625
[eval_MeetingBank loss, ppl] step:3.0, 	loss: 1.655390977859497, 	ppl: 4.996379852294922
***** Evaluating perplexity, Epoch 1/5, step 4.0 *****
[eval loss, ppl] step:4.0, 	loss: 1.1311311721801758, 	ppl: 3.391984701156616
[eval_CSTANCE loss, ppl] step:4.0, 	loss: 0.4295138716697693, 	ppl: 1.6328418254852295
[eval_20Minuten loss, ppl] step:4.0, 	loss: 1.9550830125808716, 	ppl: 7.500770568847656
[eval_FOMC loss, ppl] step:4.0, 	loss: 0.5402365326881409, 	ppl: 1.5855939388275146
[eval_NumGLUEcm loss, ppl] step:4.0, 	loss: 1.0831670761108398, 	ppl: 3.6419622898101807
[eval_ScienceQA loss, ppl] step:4.0, 	loss: 0.977886974811554, 	ppl: 2.6272544860839844
[eval_NumGLUEds loss, ppl] step:4.0, 	loss: 1.4233027696609497, 	ppl: 5.531557083129883
[eval_Py150 loss, ppl] step:4.0, 	loss: 3.0437262058258057, 	ppl: 19.24341583251953
[eval_MeetingBank loss, ppl] step:4.0, 	loss: 1.654494285583496, 	ppl: 4.9881792068481445
***** Evaluating perplexity, Epoch 1/5, step 5.0 *****
[eval loss, ppl] step:5.0, 	loss: 1.1840898990631104, 	ppl: 3.436612367630005
[eval_CSTANCE loss, ppl] step:5.0, 	loss: 0.41620442271232605, 	ppl: 1.6261177062988281
[eval_20Minuten loss, ppl] step:5.0, 	loss: 1.9506924152374268, 	ppl: 7.457005977630615
[eval_FOMC loss, ppl] step:5.0, 	loss: 0.5476396679878235, 	ppl: 1.585390567779541
[eval_NumGLUEcm loss, ppl] step:5.0, 	loss: 1.0810024738311768, 	ppl: 3.679647445678711
[eval_ScienceQA loss, ppl] step:5.0, 	loss: 0.984981119632721, 	ppl: 2.6475980281829834
[eval_NumGLUEds loss, ppl] step:5.0, 	loss: 1.368457555770874, 	ppl: 5.149179935455322
[eval_Py150 loss, ppl] step:5.0, 	loss: 2.97808575630188, 	ppl: 18.071535110473633
[eval_MeetingBank loss, ppl] step:5.0, 	loss: 1.6562435626983643, 	ppl: 4.994143009185791
***** Evaluating perplexity, Epoch 1/5, step 6.0 *****
[eval loss, ppl] step:6.0, 	loss: 1.2048213481903076, 	ppl: 3.4281325340270996
[eval_CSTANCE loss, ppl] step:6.0, 	loss: 0.4225114583969116, 	ppl: 1.6185343265533447
[eval_20Minuten loss, ppl] step:6.0, 	loss: 1.9490538835525513, 	ppl: 7.4418625831604
[eval_FOMC loss, ppl] step:6.0, 	loss: 0.5368613600730896, 	ppl: 1.5799812078475952
[eval_NumGLUEcm loss, ppl] step:6.0, 	loss: 1.0691215991973877, 	ppl: 3.678818702697754
[eval_ScienceQA loss, ppl] step:6.0, 	loss: 0.9919872879981995, 	ppl: 2.669752597808838
[eval_NumGLUEds loss, ppl] step:6.0, 	loss: 1.3501710891723633, 	ppl: 4.927008152008057
[eval_Py150 loss, ppl] step:6.0, 	loss: 2.921832323074341, 	ppl: 17.10907554626465
[eval_MeetingBank loss, ppl] step:6.0, 	loss: 1.656964898109436, 	ppl: 4.991280555725098
***** Evaluating perplexity, Epoch 1/5, step 7.0 *****
[eval loss, ppl] step:7.0, 	loss: 1.202354907989502, 	ppl: 3.421992063522339
[eval_CSTANCE loss, ppl] step:7.0, 	loss: 0.4198296070098877, 	ppl: 1.613824486732483
[eval_20Minuten loss, ppl] step:7.0, 	loss: 1.9470089673995972, 	ppl: 7.433236598968506
[eval_FOMC loss, ppl] step:7.0, 	loss: 0.535864531993866, 	ppl: 1.5774447917938232
[eval_NumGLUEcm loss, ppl] step:7.0, 	loss: 1.0692299604415894, 	ppl: 3.6729607582092285
[eval_ScienceQA loss, ppl] step:7.0, 	loss: 0.9978057146072388, 	ppl: 2.685300350189209
[eval_NumGLUEds loss, ppl] step:7.0, 	loss: 1.3080817461013794, 	ppl: 4.757249355316162
[eval_Py150 loss, ppl] step:7.0, 	loss: 2.8829972743988037, 	ppl: 16.566917419433594
[eval_MeetingBank loss, ppl] step:7.0, 	loss: 1.6580827236175537, 	ppl: 4.999502182006836
***** Evaluating perplexity, Epoch 1/5, step 8.0 *****
[eval loss, ppl] step:8.0, 	loss: 1.1843318939208984, 	ppl: 3.3459436893463135
[eval_CSTANCE loss, ppl] step:8.0, 	loss: 0.41609424352645874, 	ppl: 1.6092780828475952
[eval_20Minuten loss, ppl] step:8.0, 	loss: 1.9483639001846313, 	ppl: 7.431580543518066
[eval_FOMC loss, ppl] step:8.0, 	loss: 0.5363213419914246, 	ppl: 1.575492024421692
[eval_NumGLUEcm loss, ppl] step:8.0, 	loss: 1.042769432067871, 	ppl: 3.5865602493286133
[eval_ScienceQA loss, ppl] step:8.0, 	loss: 1.0022962093353271, 	ppl: 2.698286533355713
[eval_NumGLUEds loss, ppl] step:8.0, 	loss: 1.2783876657485962, 	ppl: 4.6335272789001465
[eval_Py150 loss, ppl] step:8.0, 	loss: 2.8597302436828613, 	ppl: 16.21480941772461
[eval_MeetingBank loss, ppl] step:8.0, 	loss: 1.659135341644287, 	ppl: 5.003277778625488
***** Evaluating perplexity, Epoch 1/5, step 9.0 *****
[eval loss, ppl] step:9.0, 	loss: 1.1647157669067383, 	ppl: 3.308023691177368
[eval_CSTANCE loss, ppl] step:9.0, 	loss: 0.4176390767097473, 	ppl: 1.6084167957305908
[eval_20Minuten loss, ppl] step:9.0, 	loss: 1.94748854637146, 	ppl: 7.435285568237305
[eval_FOMC loss, ppl] step:9.0, 	loss: 0.5236307382583618, 	ppl: 1.5702197551727295
[eval_NumGLUEcm loss, ppl] step:9.0, 	loss: 1.0316396951675415, 	ppl: 3.5535426139831543
[eval_ScienceQA loss, ppl] step:9.0, 	loss: 1.0071585178375244, 	ppl: 2.7133426666259766
[eval_NumGLUEds loss, ppl] step:9.0, 	loss: 1.2535343170166016, 	ppl: 4.571774005889893
[eval_Py150 loss, ppl] step:9.0, 	loss: 2.8396363258361816, 	ppl: 15.811210632324219
[eval_MeetingBank loss, ppl] step:9.0, 	loss: 1.659977912902832, 	ppl: 5.009483337402344
[2025-10-21 22:46:23,831] [INFO] [logging.py:107:log_dist] [Rank 0] step=10, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 22:46:24,028] [INFO] [timer.py:264:stop] epoch=0/micro_step=80/global_step=10, RunningAvgSamplesPerSec=4.944440204383601, CurrSamplesPerSec=4.9047892201669665, MemAllocated=8.84GB, MaxMemAllocated=13.82GB
***** Evaluating perplexity, Epoch 1/5, step 10.0 *****
[eval loss, ppl] step:10.0, 	loss: 1.1692382097244263, 	ppl: 3.2571935653686523
[eval_CSTANCE loss, ppl] step:10.0, 	loss: 0.42182570695877075, 	ppl: 1.6107627153396606
[eval_20Minuten loss, ppl] step:10.0, 	loss: 1.9465491771697998, 	ppl: 7.441746234893799
[eval_FOMC loss, ppl] step:10.0, 	loss: 0.5277805328369141, 	ppl: 1.5782599449157715
[eval_NumGLUEcm loss, ppl] step:10.0, 	loss: 1.0148693323135376, 	ppl: 3.500124931335449
[eval_ScienceQA loss, ppl] step:10.0, 	loss: 1.0112329721450806, 	ppl: 2.7265090942382812
[eval_NumGLUEds loss, ppl] step:10.0, 	loss: 1.2308586835861206, 	ppl: 4.473733425140381
[eval_Py150 loss, ppl] step:10.0, 	loss: 2.8203983306884766, 	ppl: 15.61526107788086
[eval_MeetingBank loss, ppl] step:10.0, 	loss: 1.6601051092147827, 	ppl: 5.0117011070251465
***** Evaluating perplexity, Epoch 1/5, step 11.0 *****
[eval loss, ppl] step:11.0, 	loss: 1.144126296043396, 	ppl: 3.1881155967712402
[eval_CSTANCE loss, ppl] step:11.0, 	loss: 0.4236156940460205, 	ppl: 1.6050119400024414
[eval_20Minuten loss, ppl] step:11.0, 	loss: 1.9494308233261108, 	ppl: 7.448808670043945
[eval_FOMC loss, ppl] step:11.0, 	loss: 0.5318509340286255, 	ppl: 1.5706193447113037
[eval_NumGLUEcm loss, ppl] step:11.0, 	loss: 0.9853843450546265, 	ppl: 3.4293723106384277
[eval_ScienceQA loss, ppl] step:11.0, 	loss: 1.0162012577056885, 	ppl: 2.742180347442627
[eval_NumGLUEds loss, ppl] step:11.0, 	loss: 1.1927088499069214, 	ppl: 4.349361896514893
[eval_Py150 loss, ppl] step:11.0, 	loss: 2.806262731552124, 	ppl: 15.373976707458496
[eval_MeetingBank loss, ppl] step:11.0, 	loss: 1.6616603136062622, 	ppl: 5.017688751220703
***** Evaluating perplexity, Epoch 1/5, step 12.0 *****
[eval loss, ppl] step:12.0, 	loss: 1.1316192150115967, 	ppl: 3.118858575820923
[eval_CSTANCE loss, ppl] step:12.0, 	loss: 0.42223575711250305, 	ppl: 1.6076709032058716
[eval_20Minuten loss, ppl] step:12.0, 	loss: 1.9496082067489624, 	ppl: 7.458268642425537
[eval_FOMC loss, ppl] step:12.0, 	loss: 0.5349884033203125, 	ppl: 1.5751091241836548
[eval_NumGLUEcm loss, ppl] step:12.0, 	loss: 0.9579414129257202, 	ppl: 3.3563337326049805
[eval_ScienceQA loss, ppl] step:12.0, 	loss: 1.0211701393127441, 	ppl: 2.755232810974121
[eval_NumGLUEds loss, ppl] step:12.0, 	loss: 1.1656928062438965, 	ppl: 4.2815399169921875
[eval_Py150 loss, ppl] step:12.0, 	loss: 2.7911617755889893, 	ppl: 15.112404823303223
[eval_MeetingBank loss, ppl] step:12.0, 	loss: 1.6619559526443481, 	ppl: 5.026363849639893
***** Evaluating perplexity, Epoch 1/5, step 13.0 *****
[eval loss, ppl] step:13.0, 	loss: 1.1123062372207642, 	ppl: 3.032728672027588
[eval_CSTANCE loss, ppl] step:13.0, 	loss: 0.4214816987514496, 	ppl: 1.608487844467163
[eval_20Minuten loss, ppl] step:13.0, 	loss: 1.9506824016571045, 	ppl: 7.461336612701416
[eval_FOMC loss, ppl] step:13.0, 	loss: 0.5245122313499451, 	ppl: 1.5697377920150757
[eval_NumGLUEcm loss, ppl] step:13.0, 	loss: 0.9179953932762146, 	ppl: 3.283031940460205
[eval_ScienceQA loss, ppl] step:13.0, 	loss: 1.0259959697723389, 	ppl: 2.77016019821167
[eval_NumGLUEds loss, ppl] step:13.0, 	loss: 1.1341876983642578, 	ppl: 4.22566032409668
[eval_Py150 loss, ppl] step:13.0, 	loss: 2.7708137035369873, 	ppl: 14.883922576904297
[eval_MeetingBank loss, ppl] step:13.0, 	loss: 1.6629871129989624, 	ppl: 5.034253120422363
***** Evaluating perplexity, Epoch 1/5, step 14.0 *****
[eval loss, ppl] step:14.0, 	loss: 1.111401915550232, 	ppl: 2.9843106269836426
[eval_CSTANCE loss, ppl] step:14.0, 	loss: 0.431749552488327, 	ppl: 1.6076998710632324
[eval_20Minuten loss, ppl] step:14.0, 	loss: 1.952521562576294, 	ppl: 7.470487594604492
[eval_FOMC loss, ppl] step:14.0, 	loss: 0.5257084965705872, 	ppl: 1.5704337358474731
[eval_NumGLUEcm loss, ppl] step:14.0, 	loss: 0.8918927311897278, 	ppl: 3.236767530441284
[eval_ScienceQA loss, ppl] step:14.0, 	loss: 1.0310534238815308, 	ppl: 2.7847959995269775
[eval_NumGLUEds loss, ppl] step:14.0, 	loss: 1.1147067546844482, 	ppl: 4.165398597717285
[eval_Py150 loss, ppl] step:14.0, 	loss: 2.752753973007202, 	ppl: 14.650101661682129
[eval_MeetingBank loss, ppl] step:14.0, 	loss: 1.6644622087478638, 	ppl: 5.041064739227295
saving model to /data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/1...
Sucessful saving model after epoch 1
Beginning of Epoch 2/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 2/5, step 0.0 *****
[eval loss, ppl] step:15.625, 	loss: 1.0874744653701782, 	ppl: 2.9127655029296875
[eval_CSTANCE loss, ppl] step:15.625, 	loss: 0.4246889650821686, 	ppl: 1.6097207069396973
[eval_20Minuten loss, ppl] step:15.625, 	loss: 1.9544686079025269, 	ppl: 7.482133865356445
[eval_FOMC loss, ppl] step:15.625, 	loss: 0.5267910361289978, 	ppl: 1.5688378810882568
[eval_NumGLUEcm loss, ppl] step:15.625, 	loss: 0.858540415763855, 	ppl: 3.172262668609619
[eval_ScienceQA loss, ppl] step:15.625, 	loss: 1.0362248420715332, 	ppl: 2.8035025596618652
[eval_NumGLUEds loss, ppl] step:15.625, 	loss: 1.0823919773101807, 	ppl: 4.13525915145874
[eval_Py150 loss, ppl] step:15.625, 	loss: 2.734454870223999, 	ppl: 14.395567893981934
[eval_MeetingBank loss, ppl] step:15.625, 	loss: 1.666002869606018, 	ppl: 5.0469651222229
***** Evaluating perplexity, Epoch 2/5, step 1.0 *****
[eval loss, ppl] step:16.625, 	loss: 1.0748012065887451, 	ppl: 2.8615994453430176
[eval_CSTANCE loss, ppl] step:16.625, 	loss: 0.4240242540836334, 	ppl: 1.6058881282806396
[eval_20Minuten loss, ppl] step:16.625, 	loss: 1.9557021856307983, 	ppl: 7.49630069732666
[eval_FOMC loss, ppl] step:16.625, 	loss: 0.5308055877685547, 	ppl: 1.573584794998169
[eval_NumGLUEcm loss, ppl] step:16.625, 	loss: 0.8389742970466614, 	ppl: 3.121149778366089
[eval_ScienceQA loss, ppl] step:16.625, 	loss: 1.0400173664093018, 	ppl: 2.8140697479248047
[eval_NumGLUEds loss, ppl] step:16.625, 	loss: 1.0782817602157593, 	ppl: 4.1245622634887695
[eval_Py150 loss, ppl] step:16.625, 	loss: 2.727146625518799, 	ppl: 14.296610832214355
[eval_MeetingBank loss, ppl] step:16.625, 	loss: 1.6652345657348633, 	ppl: 5.050671577453613
***** Evaluating perplexity, Epoch 2/5, step 2.0 *****
[eval loss, ppl] step:17.625, 	loss: 1.0518772602081299, 	ppl: 2.815131187438965
[eval_CSTANCE loss, ppl] step:17.625, 	loss: 0.42227107286453247, 	ppl: 1.6089975833892822
[eval_20Minuten loss, ppl] step:17.625, 	loss: 1.9563848972320557, 	ppl: 7.492785453796387
[eval_FOMC loss, ppl] step:17.625, 	loss: 0.5379308462142944, 	ppl: 1.5753076076507568
[eval_NumGLUEcm loss, ppl] step:17.625, 	loss: 0.8344569802284241, 	ppl: 3.074644088745117
[eval_ScienceQA loss, ppl] step:17.625, 	loss: 1.0416759252548218, 	ppl: 2.8222217559814453
[eval_NumGLUEds loss, ppl] step:17.625, 	loss: 1.0586200952529907, 	ppl: 4.062225341796875
[eval_Py150 loss, ppl] step:17.625, 	loss: 2.718423843383789, 	ppl: 14.202330589294434
[eval_MeetingBank loss, ppl] step:17.625, 	loss: 1.6661038398742676, 	ppl: 5.054606914520264
***** Evaluating perplexity, Epoch 2/5, step 3.0 *****
[eval loss, ppl] step:18.625, 	loss: 1.046695590019226, 	ppl: 2.7975027561187744
[eval_CSTANCE loss, ppl] step:18.625, 	loss: 0.4250907003879547, 	ppl: 1.6069742441177368
[eval_20Minuten loss, ppl] step:18.625, 	loss: 1.95827054977417, 	ppl: 7.498899459838867
[eval_FOMC loss, ppl] step:18.625, 	loss: 0.529811680316925, 	ppl: 1.5719590187072754
[eval_NumGLUEcm loss, ppl] step:18.625, 	loss: 0.8318146467208862, 	ppl: 3.0575649738311768
[eval_ScienceQA loss, ppl] step:18.625, 	loss: 1.0446685552597046, 	ppl: 2.829348087310791
[eval_NumGLUEds loss, ppl] step:18.625, 	loss: 1.0344947576522827, 	ppl: 4.026421546936035
[eval_Py150 loss, ppl] step:18.625, 	loss: 2.719562292098999, 	ppl: 14.1399507522583
[eval_MeetingBank loss, ppl] step:18.625, 	loss: 1.6676064729690552, 	ppl: 5.055655479431152
[2025-10-21 22:49:15,105] [INFO] [logging.py:107:log_dist] [Rank 0] step=20, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 22:49:15,417] [INFO] [timer.py:264:stop] epoch=0/micro_step=160/global_step=20, RunningAvgSamplesPerSec=5.008497485554191, CurrSamplesPerSec=4.968995638070258, MemAllocated=8.8GB, MaxMemAllocated=13.83GB
***** Evaluating perplexity, Epoch 2/5, step 4.0 *****
[eval loss, ppl] step:19.625, 	loss: 1.036725401878357, 	ppl: 2.740602970123291
[eval_CSTANCE loss, ppl] step:19.625, 	loss: 0.4271879494190216, 	ppl: 1.6102157831192017
[eval_20Minuten loss, ppl] step:19.625, 	loss: 1.958990454673767, 	ppl: 7.507689476013184
[eval_FOMC loss, ppl] step:19.625, 	loss: 0.5245760679244995, 	ppl: 1.573484182357788
[eval_NumGLUEcm loss, ppl] step:19.625, 	loss: 0.7981193661689758, 	ppl: 2.991657018661499
[eval_ScienceQA loss, ppl] step:19.625, 	loss: 1.0463958978652954, 	ppl: 2.8363568782806396
[eval_NumGLUEds loss, ppl] step:19.625, 	loss: 1.0400086641311646, 	ppl: 4.046278476715088
[eval_Py150 loss, ppl] step:19.625, 	loss: 2.71583890914917, 	ppl: 14.134723663330078
[eval_MeetingBank loss, ppl] step:19.625, 	loss: 1.667155146598816, 	ppl: 5.0603766441345215
***** Evaluating perplexity, Epoch 2/5, step 5.0 *****
[eval loss, ppl] step:20.625, 	loss: 1.0353857278823853, 	ppl: 2.7212724685668945
[eval_CSTANCE loss, ppl] step:20.625, 	loss: 0.4237034320831299, 	ppl: 1.60880708694458
[eval_20Minuten loss, ppl] step:20.625, 	loss: 1.9591968059539795, 	ppl: 7.5068583488464355
[eval_FOMC loss, ppl] step:20.625, 	loss: 0.5326822400093079, 	ppl: 1.5747630596160889
[eval_NumGLUEcm loss, ppl] step:20.625, 	loss: 0.7820234298706055, 	ppl: 2.987654209136963
[eval_ScienceQA loss, ppl] step:20.625, 	loss: 1.0481312274932861, 	ppl: 2.840115547180176
[eval_NumGLUEds loss, ppl] step:20.625, 	loss: 1.0300929546356201, 	ppl: 4.055342197418213
[eval_Py150 loss, ppl] step:20.625, 	loss: 2.70833158493042, 	ppl: 14.121199607849121
[eval_MeetingBank loss, ppl] step:20.625, 	loss: 1.6693966388702393, 	ppl: 5.067102432250977
***** Evaluating perplexity, Epoch 2/5, step 6.0 *****
[eval loss, ppl] step:21.625, 	loss: 1.0201568603515625, 	ppl: 2.7161030769348145
[eval_CSTANCE loss, ppl] step:21.625, 	loss: 0.4201827645301819, 	ppl: 1.6071139574050903
[eval_20Minuten loss, ppl] step:21.625, 	loss: 1.9613524675369263, 	ppl: 7.517794609069824
[eval_FOMC loss, ppl] step:21.625, 	loss: 0.5267739295959473, 	ppl: 1.5787179470062256
[eval_NumGLUEcm loss, ppl] step:21.625, 	loss: 0.7779881954193115, 	ppl: 2.9850597381591797
[eval_ScienceQA loss, ppl] step:21.625, 	loss: 1.0487257242202759, 	ppl: 2.84291410446167
[eval_NumGLUEds loss, ppl] step:21.625, 	loss: 1.0408306121826172, 	ppl: 4.073189735412598
[eval_Py150 loss, ppl] step:21.625, 	loss: 2.7124156951904297, 	ppl: 14.06869888305664
[eval_MeetingBank loss, ppl] step:21.625, 	loss: 1.6674838066101074, 	ppl: 5.0622968673706055
***** Evaluating perplexity, Epoch 2/5, step 7.0 *****
[eval loss, ppl] step:22.625, 	loss: 1.022676944732666, 	ppl: 2.7038021087646484
[eval_CSTANCE loss, ppl] step:22.625, 	loss: 0.4232686758041382, 	ppl: 1.609749436378479
[eval_20Minuten loss, ppl] step:22.625, 	loss: 1.95984947681427, 	ppl: 7.518354415893555
[eval_FOMC loss, ppl] step:22.625, 	loss: 0.518669605255127, 	ppl: 1.5745408535003662
[eval_NumGLUEcm loss, ppl] step:22.625, 	loss: 0.7573323845863342, 	ppl: 2.9776768684387207
[eval_ScienceQA loss, ppl] step:22.625, 	loss: 1.0486671924591064, 	ppl: 2.845284938812256
[eval_NumGLUEds loss, ppl] step:22.625, 	loss: 1.0455554723739624, 	ppl: 4.114501476287842
[eval_Py150 loss, ppl] step:22.625, 	loss: 2.7054355144500732, 	ppl: 13.979260444641113
[eval_MeetingBank loss, ppl] step:22.625, 	loss: 1.6690117120742798, 	ppl: 5.067795753479004
***** Evaluating perplexity, Epoch 2/5, step 8.0 *****
[eval loss, ppl] step:23.625, 	loss: 1.0195400714874268, 	ppl: 2.691978931427002
[eval_CSTANCE loss, ppl] step:23.625, 	loss: 0.4204445779323578, 	ppl: 1.6086881160736084
[eval_20Minuten loss, ppl] step:23.625, 	loss: 1.9613512754440308, 	ppl: 7.524585723876953
[eval_FOMC loss, ppl] step:23.625, 	loss: 0.5371814966201782, 	ppl: 1.5807242393493652
[eval_NumGLUEcm loss, ppl] step:23.625, 	loss: 0.7398793697357178, 	ppl: 2.9760546684265137
[eval_ScienceQA loss, ppl] step:23.625, 	loss: 1.0507621765136719, 	ppl: 2.8470354080200195
[eval_NumGLUEds loss, ppl] step:23.625, 	loss: 1.0421266555786133, 	ppl: 4.138426303863525
[eval_Py150 loss, ppl] step:23.625, 	loss: 2.701936960220337, 	ppl: 13.925618171691895
[eval_MeetingBank loss, ppl] step:23.625, 	loss: 1.6687817573547363, 	ppl: 5.068699359893799
***** Evaluating perplexity, Epoch 2/5, step 9.0 *****
[eval loss, ppl] step:24.625, 	loss: 0.9900612831115723, 	ppl: 2.6490554809570312
[eval_CSTANCE loss, ppl] step:24.625, 	loss: 0.42718371748924255, 	ppl: 1.6067938804626465
[eval_20Minuten loss, ppl] step:24.625, 	loss: 1.960968255996704, 	ppl: 7.5311279296875
[eval_FOMC loss, ppl] step:24.625, 	loss: 0.5292567014694214, 	ppl: 1.5797432661056519
[eval_NumGLUEcm loss, ppl] step:24.625, 	loss: 0.719414234161377, 	ppl: 2.933826446533203
[eval_ScienceQA loss, ppl] step:24.625, 	loss: 1.0511739253997803, 	ppl: 2.8496201038360596
[eval_NumGLUEds loss, ppl] step:24.625, 	loss: 1.033679723739624, 	ppl: 4.16879415512085
[eval_Py150 loss, ppl] step:24.625, 	loss: 2.7027342319488525, 	ppl: 13.974605560302734
[eval_MeetingBank loss, ppl] step:24.625, 	loss: 1.6687783002853394, 	ppl: 5.070224761962891
***** Evaluating perplexity, Epoch 2/5, step 10.0 *****
[eval loss, ppl] step:25.625, 	loss: 0.9900527596473694, 	ppl: 2.631829023361206
[eval_CSTANCE loss, ppl] step:25.625, 	loss: 0.42706820368766785, 	ppl: 1.6123822927474976
[eval_20Minuten loss, ppl] step:25.625, 	loss: 1.9622570276260376, 	ppl: 7.528207778930664
[eval_FOMC loss, ppl] step:25.625, 	loss: 0.5221768021583557, 	ppl: 1.5721681118011475
[eval_NumGLUEcm loss, ppl] step:25.625, 	loss: 0.7073779106140137, 	ppl: 2.9249322414398193
[eval_ScienceQA loss, ppl] step:25.625, 	loss: 1.0519729852676392, 	ppl: 2.8504841327667236
[eval_NumGLUEds loss, ppl] step:25.625, 	loss: 1.0378813743591309, 	ppl: 4.1802263259887695
[eval_Py150 loss, ppl] step:25.625, 	loss: 2.7069711685180664, 	ppl: 14.015113830566406
[eval_MeetingBank loss, ppl] step:25.625, 	loss: 1.6704963445663452, 	ppl: 5.0728302001953125
***** Evaluating perplexity, Epoch 2/5, step 11.0 *****
[eval loss, ppl] step:26.625, 	loss: 0.9779952764511108, 	ppl: 2.613755941390991
[eval_CSTANCE loss, ppl] step:26.625, 	loss: 0.4217115640640259, 	ppl: 1.6100072860717773
[eval_20Minuten loss, ppl] step:26.625, 	loss: 1.9626376628875732, 	ppl: 7.540969371795654
[eval_FOMC loss, ppl] step:26.625, 	loss: 0.5206044912338257, 	ppl: 1.5725520849227905
[eval_NumGLUEcm loss, ppl] step:26.625, 	loss: 0.693695604801178, 	ppl: 2.903759479522705
[eval_ScienceQA loss, ppl] step:26.625, 	loss: 1.0527087450027466, 	ppl: 2.853264570236206
[eval_NumGLUEds loss, ppl] step:26.625, 	loss: 1.0323257446289062, 	ppl: 4.204414367675781
[eval_Py150 loss, ppl] step:26.625, 	loss: 2.711050033569336, 	ppl: 14.076004028320312
[eval_MeetingBank loss, ppl] step:26.625, 	loss: 1.6710320711135864, 	ppl: 5.071976184844971
***** Evaluating perplexity, Epoch 2/5, step 12.0 *****
[eval loss, ppl] step:27.625, 	loss: 0.959749698638916, 	ppl: 2.582186460494995
[eval_CSTANCE loss, ppl] step:27.625, 	loss: 0.41951265931129456, 	ppl: 1.610314130783081
[eval_20Minuten loss, ppl] step:27.625, 	loss: 1.962280511856079, 	ppl: 7.538618564605713
[eval_FOMC loss, ppl] step:27.625, 	loss: 0.5342668890953064, 	ppl: 1.5800912380218506
[eval_NumGLUEcm loss, ppl] step:27.625, 	loss: 0.6769835948944092, 	ppl: 2.882534980773926
[eval_ScienceQA loss, ppl] step:27.625, 	loss: 1.0531889200210571, 	ppl: 2.854562759399414
[eval_NumGLUEds loss, ppl] step:27.625, 	loss: 1.0194203853607178, 	ppl: 4.163443088531494
[eval_Py150 loss, ppl] step:27.625, 	loss: 2.713991165161133, 	ppl: 14.110857009887695
[eval_MeetingBank loss, ppl] step:27.625, 	loss: 1.6682857275009155, 	ppl: 5.071965217590332
***** Evaluating perplexity, Epoch 2/5, step 13.0 *****
[eval loss, ppl] step:28.625, 	loss: 0.9377450346946716, 	ppl: 2.5484304428100586
[eval_CSTANCE loss, ppl] step:28.625, 	loss: 0.4192689061164856, 	ppl: 1.613223910331726
[eval_20Minuten loss, ppl] step:28.625, 	loss: 1.9660271406173706, 	ppl: 7.553546905517578
[eval_FOMC loss, ppl] step:28.625, 	loss: 0.5281444787979126, 	ppl: 1.5757182836532593
[eval_NumGLUEcm loss, ppl] step:28.625, 	loss: 0.6508209109306335, 	ppl: 2.8613874912261963
[eval_ScienceQA loss, ppl] step:28.625, 	loss: 1.053957223892212, 	ppl: 2.856477737426758
[eval_NumGLUEds loss, ppl] step:28.625, 	loss: 1.017319917678833, 	ppl: 4.162472724914551
[eval_Py150 loss, ppl] step:28.625, 	loss: 2.713444471359253, 	ppl: 14.139089584350586
[eval_MeetingBank loss, ppl] step:28.625, 	loss: 1.6698766946792603, 	ppl: 5.077618598937988
[2025-10-21 22:52:01,840] [INFO] [logging.py:107:log_dist] [Rank 0] step=30, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 22:52:02,066] [INFO] [timer.py:264:stop] epoch=0/micro_step=240/global_step=30, RunningAvgSamplesPerSec=5.039845060629524, CurrSamplesPerSec=5.178253355714001, MemAllocated=8.92GB, MaxMemAllocated=13.83GB
***** Evaluating perplexity, Epoch 2/5, step 14.0 *****
[eval loss, ppl] step:29.625, 	loss: 0.9354027509689331, 	ppl: 2.550844192504883
[eval_CSTANCE loss, ppl] step:29.625, 	loss: 0.4225897490978241, 	ppl: 1.6180059909820557
[eval_20Minuten loss, ppl] step:29.625, 	loss: 1.9657797813415527, 	ppl: 7.554444313049316
[eval_FOMC loss, ppl] step:29.625, 	loss: 0.5333327651023865, 	ppl: 1.5801887512207031
[eval_NumGLUEcm loss, ppl] step:29.625, 	loss: 0.6520099639892578, 	ppl: 2.871277332305908
[eval_ScienceQA loss, ppl] step:29.625, 	loss: 1.0537607669830322, 	ppl: 2.8576927185058594
[eval_NumGLUEds loss, ppl] step:29.625, 	loss: 1.014992356300354, 	ppl: 4.181901931762695
[eval_Py150 loss, ppl] step:29.625, 	loss: 2.7196736335754395, 	ppl: 14.164857864379883
[eval_MeetingBank loss, ppl] step:29.625, 	loss: 1.6690523624420166, 	ppl: 5.0778326988220215
saving model to /data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/2...
Sucessful saving model after epoch 2
Beginning of Epoch 3/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 3/5, step 0.0 *****
[eval loss, ppl] step:31.25, 	loss: 0.9285753965377808, 	ppl: 2.536748170852661
[eval_CSTANCE loss, ppl] step:31.25, 	loss: 0.41737115383148193, 	ppl: 1.6098103523254395
[eval_20Minuten loss, ppl] step:31.25, 	loss: 1.967497706413269, 	ppl: 7.565264701843262
[eval_FOMC loss, ppl] step:31.25, 	loss: 0.5245283842086792, 	ppl: 1.5783261060714722
[eval_NumGLUEcm loss, ppl] step:31.25, 	loss: 0.6282211542129517, 	ppl: 2.8628058433532715
[eval_ScienceQA loss, ppl] step:31.25, 	loss: 1.0546802282333374, 	ppl: 2.8581995964050293
[eval_NumGLUEds loss, ppl] step:31.25, 	loss: 1.0240172147750854, 	ppl: 4.220081329345703
[eval_Py150 loss, ppl] step:31.25, 	loss: 2.7142295837402344, 	ppl: 14.20402717590332
[eval_MeetingBank loss, ppl] step:31.25, 	loss: 1.6696712970733643, 	ppl: 5.080547332763672
***** Evaluating perplexity, Epoch 3/5, step 1.0 *****
[eval loss, ppl] step:32.25, 	loss: 0.9161996245384216, 	ppl: 2.5342164039611816
[eval_CSTANCE loss, ppl] step:32.25, 	loss: 0.42135554552078247, 	ppl: 1.6139791011810303
[eval_20Minuten loss, ppl] step:32.25, 	loss: 1.9661368131637573, 	ppl: 7.556839466094971
[eval_FOMC loss, ppl] step:32.25, 	loss: 0.5225067138671875, 	ppl: 1.5743005275726318
[eval_NumGLUEcm loss, ppl] step:32.25, 	loss: 0.6169147491455078, 	ppl: 2.8680710792541504
[eval_ScienceQA loss, ppl] step:32.25, 	loss: 1.053733229637146, 	ppl: 2.8538661003112793
[eval_NumGLUEds loss, ppl] step:32.25, 	loss: 1.040565013885498, 	ppl: 4.27170991897583
[eval_Py150 loss, ppl] step:32.25, 	loss: 2.721731424331665, 	ppl: 14.225701332092285
[eval_MeetingBank loss, ppl] step:32.25, 	loss: 1.6716629266738892, 	ppl: 5.083898544311523
***** Evaluating perplexity, Epoch 3/5, step 2.0 *****
[eval loss, ppl] step:33.25, 	loss: 0.9141297936439514, 	ppl: 2.5353076457977295
[eval_CSTANCE loss, ppl] step:33.25, 	loss: 0.417203813791275, 	ppl: 1.6165854930877686
[eval_20Minuten loss, ppl] step:33.25, 	loss: 1.967732310295105, 	ppl: 7.557351589202881
[eval_FOMC loss, ppl] step:33.25, 	loss: 0.5222657322883606, 	ppl: 1.5779211521148682
[eval_NumGLUEcm loss, ppl] step:33.25, 	loss: 0.6157214045524597, 	ppl: 2.869720220565796
[eval_ScienceQA loss, ppl] step:33.25, 	loss: 1.0548616647720337, 	ppl: 2.8546462059020996
[eval_NumGLUEds loss, ppl] step:33.25, 	loss: 1.0344711542129517, 	ppl: 4.2867350578308105
[eval_Py150 loss, ppl] step:33.25, 	loss: 2.7161643505096436, 	ppl: 14.193559646606445
[eval_MeetingBank loss, ppl] step:33.25, 	loss: 1.6695585250854492, 	ppl: 5.079779624938965
***** Evaluating perplexity, Epoch 3/5, step 3.0 *****
[eval loss, ppl] step:34.25, 	loss: 0.926188051700592, 	ppl: 2.545802116394043
[eval_CSTANCE loss, ppl] step:34.25, 	loss: 0.41669920086860657, 	ppl: 1.6146845817565918
[eval_20Minuten loss, ppl] step:34.25, 	loss: 1.9657059907913208, 	ppl: 7.557653903961182
[eval_FOMC loss, ppl] step:34.25, 	loss: 0.5316354036331177, 	ppl: 1.5810155868530273
[eval_NumGLUEcm loss, ppl] step:34.25, 	loss: 0.6224408149719238, 	ppl: 2.8869380950927734
[eval_ScienceQA loss, ppl] step:34.25, 	loss: 1.0535142421722412, 	ppl: 2.853078603744507
[eval_NumGLUEds loss, ppl] step:34.25, 	loss: 1.046226143836975, 	ppl: 4.304790496826172
[eval_Py150 loss, ppl] step:34.25, 	loss: 2.712158679962158, 	ppl: 14.146089553833008
[eval_MeetingBank loss, ppl] step:34.25, 	loss: 1.669665813446045, 	ppl: 5.073054313659668
***** Evaluating perplexity, Epoch 3/5, step 4.0 *****
[eval loss, ppl] step:35.25, 	loss: 0.9387685656547546, 	ppl: 2.556410074234009
[eval_CSTANCE loss, ppl] step:35.25, 	loss: 0.4166671931743622, 	ppl: 1.6124796867370605
[eval_20Minuten loss, ppl] step:35.25, 	loss: 1.965073585510254, 	ppl: 7.55150032043457
[eval_FOMC loss, ppl] step:35.25, 	loss: 0.5174555778503418, 	ppl: 1.580154538154602
[eval_NumGLUEcm loss, ppl] step:35.25, 	loss: 0.6267314553260803, 	ppl: 2.8939504623413086
[eval_ScienceQA loss, ppl] step:35.25, 	loss: 1.0531646013259888, 	ppl: 2.852637767791748
[eval_NumGLUEds loss, ppl] step:35.25, 	loss: 1.0476280450820923, 	ppl: 4.326686859130859
[eval_Py150 loss, ppl] step:35.25, 	loss: 2.701824903488159, 	ppl: 14.013535499572754
[eval_MeetingBank loss, ppl] step:35.25, 	loss: 1.6685291528701782, 	ppl: 5.073703289031982
***** Evaluating perplexity, Epoch 3/5, step 5.0 *****
[eval loss, ppl] step:36.25, 	loss: 0.9466128945350647, 	ppl: 2.5607333183288574
[eval_CSTANCE loss, ppl] step:36.25, 	loss: 0.42711716890335083, 	ppl: 1.614569902420044
[eval_20Minuten loss, ppl] step:36.25, 	loss: 1.9675263166427612, 	ppl: 7.565988540649414
[eval_FOMC loss, ppl] step:36.25, 	loss: 0.5216016173362732, 	ppl: 1.5777872800827026
[eval_NumGLUEcm loss, ppl] step:36.25, 	loss: 0.6227761507034302, 	ppl: 2.8945260047912598
[eval_ScienceQA loss, ppl] step:36.25, 	loss: 1.053261160850525, 	ppl: 2.852057933807373
[eval_NumGLUEds loss, ppl] step:36.25, 	loss: 1.0530281066894531, 	ppl: 4.341720104217529
[eval_Py150 loss, ppl] step:36.25, 	loss: 2.702389717102051, 	ppl: 14.013433456420898
[eval_MeetingBank loss, ppl] step:36.25, 	loss: 1.668923020362854, 	ppl: 5.0701422691345215
***** Evaluating perplexity, Epoch 3/5, step 6.0 *****
[eval loss, ppl] step:37.25, 	loss: 0.9398707747459412, 	ppl: 2.5316720008850098
[eval_CSTANCE loss, ppl] step:37.25, 	loss: 0.4205299913883209, 	ppl: 1.6148985624313354
[eval_20Minuten loss, ppl] step:37.25, 	loss: 1.9676051139831543, 	ppl: 7.573446750640869
[eval_FOMC loss, ppl] step:37.25, 	loss: 0.5107671618461609, 	ppl: 1.5740617513656616
[eval_NumGLUEcm loss, ppl] step:37.25, 	loss: 0.6098993420600891, 	ppl: 2.8614232540130615
[eval_ScienceQA loss, ppl] step:37.25, 	loss: 1.052377700805664, 	ppl: 2.8499889373779297
[eval_NumGLUEds loss, ppl] step:37.25, 	loss: 1.0580294132232666, 	ppl: 4.387959957122803
[eval_Py150 loss, ppl] step:37.25, 	loss: 2.7023186683654785, 	ppl: 14.030633926391602
[eval_MeetingBank loss, ppl] step:37.25, 	loss: 1.6689823865890503, 	ppl: 5.068119049072266
***** Evaluating perplexity, Epoch 3/5, step 7.0 *****
[eval loss, ppl] step:38.25, 	loss: 0.9304986000061035, 	ppl: 2.520989418029785
[eval_CSTANCE loss, ppl] step:38.25, 	loss: 0.41713958978652954, 	ppl: 1.6020344495773315
[eval_20Minuten loss, ppl] step:38.25, 	loss: 1.9641592502593994, 	ppl: 7.556269645690918
[eval_FOMC loss, ppl] step:38.25, 	loss: 0.5181098580360413, 	ppl: 1.5783963203430176
[eval_NumGLUEcm loss, ppl] step:38.25, 	loss: 0.6066749691963196, 	ppl: 2.8387022018432617
[eval_ScienceQA loss, ppl] step:38.25, 	loss: 1.0532541275024414, 	ppl: 2.8502414226531982
[eval_NumGLUEds loss, ppl] step:38.25, 	loss: 1.0530050992965698, 	ppl: 4.383217811584473
[eval_Py150 loss, ppl] step:38.25, 	loss: 2.709993839263916, 	ppl: 14.106911659240723
[eval_MeetingBank loss, ppl] step:38.25, 	loss: 1.6679699420928955, 	ppl: 5.067553997039795
[2025-10-21 22:54:49,582] [INFO] [logging.py:107:log_dist] [Rank 0] step=40, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 22:54:49,779] [INFO] [timer.py:264:stop] epoch=0/micro_step=320/global_step=40, RunningAvgSamplesPerSec=5.084767888055163, CurrSamplesPerSec=4.9999912461136065, MemAllocated=8.8GB, MaxMemAllocated=13.83GB
***** Evaluating perplexity, Epoch 3/5, step 8.0 *****
[eval loss, ppl] step:39.25, 	loss: 0.9089945554733276, 	ppl: 2.4893317222595215
[eval_CSTANCE loss, ppl] step:39.25, 	loss: 0.41542282700538635, 	ppl: 1.6116182804107666
[eval_20Minuten loss, ppl] step:39.25, 	loss: 1.9669859409332275, 	ppl: 7.56616735458374
[eval_FOMC loss, ppl] step:39.25, 	loss: 0.5214778780937195, 	ppl: 1.576674461364746
[eval_NumGLUEcm loss, ppl] step:39.25, 	loss: 0.592969536781311, 	ppl: 2.8092026710510254
[eval_ScienceQA loss, ppl] step:39.25, 	loss: 1.0527244806289673, 	ppl: 2.849346399307251
[eval_NumGLUEds loss, ppl] step:39.25, 	loss: 1.0487323999404907, 	ppl: 4.360777378082275
[eval_Py150 loss, ppl] step:39.25, 	loss: 2.706397771835327, 	ppl: 14.072799682617188
[eval_MeetingBank loss, ppl] step:39.25, 	loss: 1.6691343784332275, 	ppl: 5.065465450286865
***** Evaluating perplexity, Epoch 3/5, step 9.0 *****
[eval loss, ppl] step:40.25, 	loss: 0.9095872640609741, 	ppl: 2.46337628364563
[eval_CSTANCE loss, ppl] step:40.25, 	loss: 0.4204726815223694, 	ppl: 1.609364628791809
[eval_20Minuten loss, ppl] step:40.25, 	loss: 1.967164158821106, 	ppl: 7.562503814697266
[eval_FOMC loss, ppl] step:40.25, 	loss: 0.5164092779159546, 	ppl: 1.5745134353637695
[eval_NumGLUEcm loss, ppl] step:40.25, 	loss: 0.6020375490188599, 	ppl: 2.7715914249420166
[eval_ScienceQA loss, ppl] step:40.25, 	loss: 1.0533100366592407, 	ppl: 2.85092830657959
[eval_NumGLUEds loss, ppl] step:40.25, 	loss: 1.0306614637374878, 	ppl: 4.3090972900390625
[eval_Py150 loss, ppl] step:40.25, 	loss: 2.7142250537872314, 	ppl: 14.216026306152344
[eval_MeetingBank loss, ppl] step:40.25, 	loss: 1.669662356376648, 	ppl: 5.0742106437683105
***** Evaluating perplexity, Epoch 3/5, step 10.0 *****
[eval loss, ppl] step:41.25, 	loss: 0.8891830444335938, 	ppl: 2.4272212982177734
[eval_CSTANCE loss, ppl] step:41.25, 	loss: 0.42228642106056213, 	ppl: 1.6143925189971924
[eval_20Minuten loss, ppl] step:41.25, 	loss: 1.9652842283248901, 	ppl: 7.561027526855469
[eval_FOMC loss, ppl] step:41.25, 	loss: 0.5182861089706421, 	ppl: 1.5764251947402954
[eval_NumGLUEcm loss, ppl] step:41.25, 	loss: 0.5910603404045105, 	ppl: 2.7377028465270996
[eval_ScienceQA loss, ppl] step:41.25, 	loss: 1.0544929504394531, 	ppl: 2.852553367614746
[eval_NumGLUEds loss, ppl] step:41.25, 	loss: 1.0146636962890625, 	ppl: 4.304766654968262
[eval_Py150 loss, ppl] step:41.25, 	loss: 2.7216956615448, 	ppl: 14.262517929077148
[eval_MeetingBank loss, ppl] step:41.25, 	loss: 1.6707004308700562, 	ppl: 5.075750350952148
***** Evaluating perplexity, Epoch 3/5, step 11.0 *****
[eval loss, ppl] step:42.25, 	loss: 0.8877352476119995, 	ppl: 2.4115090370178223
[eval_CSTANCE loss, ppl] step:42.25, 	loss: 0.4235353171825409, 	ppl: 1.6085295677185059
[eval_20Minuten loss, ppl] step:42.25, 	loss: 1.9647995233535767, 	ppl: 7.559877395629883
[eval_FOMC loss, ppl] step:42.25, 	loss: 0.5139754414558411, 	ppl: 1.5744279623031616
[eval_NumGLUEcm loss, ppl] step:42.25, 	loss: 0.5820921659469604, 	ppl: 2.7132790088653564
[eval_ScienceQA loss, ppl] step:42.25, 	loss: 1.0543302297592163, 	ppl: 2.8515710830688477
[eval_NumGLUEds loss, ppl] step:42.25, 	loss: 1.0153690576553345, 	ppl: 4.3478779792785645
[eval_Py150 loss, ppl] step:42.25, 	loss: 2.7258005142211914, 	ppl: 14.277576446533203
[eval_MeetingBank loss, ppl] step:42.25, 	loss: 1.6688017845153809, 	ppl: 5.071875095367432
***** Evaluating perplexity, Epoch 3/5, step 12.0 *****
[eval loss, ppl] step:43.25, 	loss: 0.8786603808403015, 	ppl: 2.4224228858947754
[eval_CSTANCE loss, ppl] step:43.25, 	loss: 0.422941654920578, 	ppl: 1.6147048473358154
[eval_20Minuten loss, ppl] step:43.25, 	loss: 1.967336893081665, 	ppl: 7.559937953948975
[eval_FOMC loss, ppl] step:43.25, 	loss: 0.5222947001457214, 	ppl: 1.5817288160324097
[eval_NumGLUEcm loss, ppl] step:43.25, 	loss: 0.5855013132095337, 	ppl: 2.7359869480133057
[eval_ScienceQA loss, ppl] step:43.25, 	loss: 1.054076910018921, 	ppl: 2.852137565612793
[eval_NumGLUEds loss, ppl] step:43.25, 	loss: 1.0251024961471558, 	ppl: 4.35030460357666
[eval_Py150 loss, ppl] step:43.25, 	loss: 2.7241151332855225, 	ppl: 14.31713581085205
[eval_MeetingBank loss, ppl] step:43.25, 	loss: 1.6693120002746582, 	ppl: 5.074651718139648
***** Evaluating perplexity, Epoch 3/5, step 13.0 *****
[eval loss, ppl] step:44.25, 	loss: 0.8630038499832153, 	ppl: 2.3977975845336914
[eval_CSTANCE loss, ppl] step:44.25, 	loss: 0.4196701645851135, 	ppl: 1.6072638034820557
[eval_20Minuten loss, ppl] step:44.25, 	loss: 1.9667861461639404, 	ppl: 7.558472633361816
[eval_FOMC loss, ppl] step:44.25, 	loss: 0.5236321687698364, 	ppl: 1.581075668334961
[eval_NumGLUEcm loss, ppl] step:44.25, 	loss: 0.5817848443984985, 	ppl: 2.7019779682159424
[eval_ScienceQA loss, ppl] step:44.25, 	loss: 1.056465744972229, 	ppl: 2.8548150062561035
[eval_NumGLUEds loss, ppl] step:44.25, 	loss: 1.0235989093780518, 	ppl: 4.362946033477783
[eval_Py150 loss, ppl] step:44.25, 	loss: 2.7305116653442383, 	ppl: 14.341787338256836
[eval_MeetingBank loss, ppl] step:44.25, 	loss: 1.6713985204696655, 	ppl: 5.074394226074219
***** Evaluating perplexity, Epoch 3/5, step 14.0 *****
[eval loss, ppl] step:45.25, 	loss: 0.8638460040092468, 	ppl: 2.37837290763855
[eval_CSTANCE loss, ppl] step:45.25, 	loss: 0.4179235100746155, 	ppl: 1.6162152290344238
[eval_20Minuten loss, ppl] step:45.25, 	loss: 1.9709203243255615, 	ppl: 7.576158046722412
[eval_FOMC loss, ppl] step:45.25, 	loss: 0.5210781097412109, 	ppl: 1.581990361213684
[eval_NumGLUEcm loss, ppl] step:45.25, 	loss: 0.5820386409759521, 	ppl: 2.670229196548462
[eval_ScienceQA loss, ppl] step:45.25, 	loss: 1.0553642511367798, 	ppl: 2.856393814086914
[eval_NumGLUEds loss, ppl] step:45.25, 	loss: 1.0102649927139282, 	ppl: 4.373469352722168
[eval_Py150 loss, ppl] step:45.25, 	loss: 2.725491523742676, 	ppl: 14.376302719116211
[eval_MeetingBank loss, ppl] step:45.25, 	loss: 1.6693726778030396, 	ppl: 5.076597213745117
saving model to /data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/3...
Sucessful saving model after epoch 3
Beginning of Epoch 4/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 4/5, step 0.0 *****
[eval loss, ppl] step:46.875, 	loss: 0.8609323501586914, 	ppl: 2.381561279296875
[eval_CSTANCE loss, ppl] step:46.875, 	loss: 0.42278188467025757, 	ppl: 1.6157606840133667
[eval_20Minuten loss, ppl] step:46.875, 	loss: 1.9685081243515015, 	ppl: 7.564013957977295
[eval_FOMC loss, ppl] step:46.875, 	loss: 0.5229523777961731, 	ppl: 1.582057237625122
[eval_NumGLUEcm loss, ppl] step:46.875, 	loss: 0.5777466297149658, 	ppl: 2.682936429977417
[eval_ScienceQA loss, ppl] step:46.875, 	loss: 1.055901288986206, 	ppl: 2.8583309650421143
[eval_NumGLUEds loss, ppl] step:46.875, 	loss: 1.009206771850586, 	ppl: 4.356045722961426
[eval_Py150 loss, ppl] step:46.875, 	loss: 2.734839916229248, 	ppl: 14.429957389831543
[eval_MeetingBank loss, ppl] step:46.875, 	loss: 1.6700063943862915, 	ppl: 5.08257532119751
***** Evaluating perplexity, Epoch 4/5, step 1.0 *****
[eval loss, ppl] step:47.875, 	loss: 0.8454316854476929, 	ppl: 2.3547170162200928
[eval_CSTANCE loss, ppl] step:47.875, 	loss: 0.4214451014995575, 	ppl: 1.6087863445281982
[eval_20Minuten loss, ppl] step:47.875, 	loss: 1.9701907634735107, 	ppl: 7.5740580558776855
[eval_FOMC loss, ppl] step:47.875, 	loss: 0.5133377909660339, 	ppl: 1.58159601688385
[eval_NumGLUEcm loss, ppl] step:47.875, 	loss: 0.5669045448303223, 	ppl: 2.6509790420532227
[eval_ScienceQA loss, ppl] step:47.875, 	loss: 1.0565038919448853, 	ppl: 2.859386444091797
[eval_NumGLUEds loss, ppl] step:47.875, 	loss: 0.9929430484771729, 	ppl: 4.317493915557861
[eval_Py150 loss, ppl] step:47.875, 	loss: 2.733822822570801, 	ppl: 14.466789245605469
[eval_MeetingBank loss, ppl] step:47.875, 	loss: 1.67013418674469, 	ppl: 5.078213691711426
***** Evaluating perplexity, Epoch 4/5, step 2.0 *****
[eval loss, ppl] step:48.875, 	loss: 0.8514005541801453, 	ppl: 2.35073184967041
[eval_CSTANCE loss, ppl] step:48.875, 	loss: 0.42161786556243896, 	ppl: 1.6130132675170898
[eval_20Minuten loss, ppl] step:48.875, 	loss: 1.9700262546539307, 	ppl: 7.572031021118164
[eval_FOMC loss, ppl] step:48.875, 	loss: 0.5277365446090698, 	ppl: 1.5858571529388428
[eval_NumGLUEcm loss, ppl] step:48.875, 	loss: 0.5661016702651978, 	ppl: 2.646331548690796
[eval_ScienceQA loss, ppl] step:48.875, 	loss: 1.0580837726593018, 	ppl: 2.863128185272217
[eval_NumGLUEds loss, ppl] step:48.875, 	loss: 0.9908031225204468, 	ppl: 4.3147406578063965
[eval_Py150 loss, ppl] step:48.875, 	loss: 2.7398195266723633, 	ppl: 14.524070739746094
[eval_MeetingBank loss, ppl] step:48.875, 	loss: 1.6707627773284912, 	ppl: 5.081252098083496
[2025-10-21 22:57:44,102] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 22:57:44,276] [INFO] [timer.py:264:stop] epoch=0/micro_step=400/global_step=50, RunningAvgSamplesPerSec=5.089635360021144, CurrSamplesPerSec=5.2820359601224975, MemAllocated=8.81GB, MaxMemAllocated=13.83GB
***** Evaluating perplexity, Epoch 4/5, step 3.0 *****
[eval loss, ppl] step:49.875, 	loss: 0.8630841374397278, 	ppl: 2.3512046337127686
[eval_CSTANCE loss, ppl] step:49.875, 	loss: 0.4260787069797516, 	ppl: 1.6159265041351318
[eval_20Minuten loss, ppl] step:49.875, 	loss: 1.9672967195510864, 	ppl: 7.567322731018066
[eval_FOMC loss, ppl] step:49.875, 	loss: 0.5183393359184265, 	ppl: 1.5824042558670044
[eval_NumGLUEcm loss, ppl] step:49.875, 	loss: 0.574343204498291, 	ppl: 2.6462438106536865
[eval_ScienceQA loss, ppl] step:49.875, 	loss: 1.0595486164093018, 	ppl: 2.8662991523742676
[eval_NumGLUEds loss, ppl] step:49.875, 	loss: 0.996369481086731, 	ppl: 4.345010757446289
[eval_Py150 loss, ppl] step:49.875, 	loss: 2.7360401153564453, 	ppl: 14.522326469421387
[eval_MeetingBank loss, ppl] step:49.875, 	loss: 1.6703273057937622, 	ppl: 5.079401969909668
***** Evaluating perplexity, Epoch 4/5, step 4.0 *****
[eval loss, ppl] step:50.875, 	loss: 0.8551844358444214, 	ppl: 2.3394103050231934
[eval_CSTANCE loss, ppl] step:50.875, 	loss: 0.42325344681739807, 	ppl: 1.6133042573928833
[eval_20Minuten loss, ppl] step:50.875, 	loss: 1.9686579704284668, 	ppl: 7.569358825683594
[eval_FOMC loss, ppl] step:50.875, 	loss: 0.5220648050308228, 	ppl: 1.5882529020309448
[eval_NumGLUEcm loss, ppl] step:50.875, 	loss: 0.5732784867286682, 	ppl: 2.627244710922241
[eval_ScienceQA loss, ppl] step:50.875, 	loss: 1.058532476425171, 	ppl: 2.8643500804901123
[eval_NumGLUEds loss, ppl] step:50.875, 	loss: 1.0098426342010498, 	ppl: 4.355282783508301
[eval_Py150 loss, ppl] step:50.875, 	loss: 2.7362868785858154, 	ppl: 14.546563148498535
[eval_MeetingBank loss, ppl] step:50.875, 	loss: 1.6704659461975098, 	ppl: 5.082862854003906
***** Evaluating perplexity, Epoch 4/5, step 5.0 *****
[eval loss, ppl] step:51.875, 	loss: 0.8539494276046753, 	ppl: 2.331458330154419
[eval_CSTANCE loss, ppl] step:51.875, 	loss: 0.4294615685939789, 	ppl: 1.6119325160980225
[eval_20Minuten loss, ppl] step:51.875, 	loss: 1.969234824180603, 	ppl: 7.5806450843811035
[eval_FOMC loss, ppl] step:51.875, 	loss: 0.5172566175460815, 	ppl: 1.5789437294006348
[eval_NumGLUEcm loss, ppl] step:51.875, 	loss: 0.5633851289749146, 	ppl: 2.614346981048584
[eval_ScienceQA loss, ppl] step:51.875, 	loss: 1.0588949918746948, 	ppl: 2.863633394241333
[eval_NumGLUEds loss, ppl] step:51.875, 	loss: 1.0025166273117065, 	ppl: 4.352596282958984
[eval_Py150 loss, ppl] step:51.875, 	loss: 2.7328667640686035, 	ppl: 14.540063858032227
[eval_MeetingBank loss, ppl] step:51.875, 	loss: 1.669871211051941, 	ppl: 5.0795369148254395
***** Evaluating perplexity, Epoch 4/5, step 6.0 *****
[eval loss, ppl] step:52.875, 	loss: 0.8498696088790894, 	ppl: 2.319653034210205
[eval_CSTANCE loss, ppl] step:52.875, 	loss: 0.42811834812164307, 	ppl: 1.611562728881836
[eval_20Minuten loss, ppl] step:52.875, 	loss: 1.971175193786621, 	ppl: 7.571398735046387
[eval_FOMC loss, ppl] step:52.875, 	loss: 0.5124371647834778, 	ppl: 1.585204839706421
[eval_NumGLUEcm loss, ppl] step:52.875, 	loss: 0.5551744699478149, 	ppl: 2.5965442657470703
[eval_ScienceQA loss, ppl] step:52.875, 	loss: 1.0585501194000244, 	ppl: 2.864187240600586
[eval_NumGLUEds loss, ppl] step:52.875, 	loss: 1.0123660564422607, 	ppl: 4.400513648986816
[eval_Py150 loss, ppl] step:52.875, 	loss: 2.744675874710083, 	ppl: 14.613656997680664
[eval_MeetingBank loss, ppl] step:52.875, 	loss: 1.6711722612380981, 	ppl: 5.083111763000488
***** Evaluating perplexity, Epoch 4/5, step 7.0 *****
[eval loss, ppl] step:53.875, 	loss: 0.8375200033187866, 	ppl: 2.3071184158325195
[eval_CSTANCE loss, ppl] step:53.875, 	loss: 0.4180110692977905, 	ppl: 1.6005572080612183
[eval_20Minuten loss, ppl] step:53.875, 	loss: 1.96785569190979, 	ppl: 7.5711283683776855
[eval_FOMC loss, ppl] step:53.875, 	loss: 0.5295163989067078, 	ppl: 1.5866395235061646
[eval_NumGLUEcm loss, ppl] step:53.875, 	loss: 0.5521973371505737, 	ppl: 2.5822033882141113
[eval_ScienceQA loss, ppl] step:53.875, 	loss: 1.0589860677719116, 	ppl: 2.865474224090576
[eval_NumGLUEds loss, ppl] step:53.875, 	loss: 1.0110172033309937, 	ppl: 4.433564186096191
[eval_Py150 loss, ppl] step:53.875, 	loss: 2.7422163486480713, 	ppl: 14.59163761138916
[eval_MeetingBank loss, ppl] step:53.875, 	loss: 1.6696721315383911, 	ppl: 5.076857089996338
***** Evaluating perplexity, Epoch 4/5, step 8.0 *****
[eval loss, ppl] step:54.875, 	loss: 0.8582247495651245, 	ppl: 2.332681179046631
[eval_CSTANCE loss, ppl] step:54.875, 	loss: 0.4190371334552765, 	ppl: 1.6087740659713745
[eval_20Minuten loss, ppl] step:54.875, 	loss: 1.9703662395477295, 	ppl: 7.58108377456665
[eval_FOMC loss, ppl] step:54.875, 	loss: 0.5155934691429138, 	ppl: 1.589592695236206
[eval_NumGLUEcm loss, ppl] step:54.875, 	loss: 0.5524141192436218, 	ppl: 2.6015453338623047
[eval_ScienceQA loss, ppl] step:54.875, 	loss: 1.0591249465942383, 	ppl: 2.866018533706665
[eval_NumGLUEds loss, ppl] step:54.875, 	loss: 1.0312711000442505, 	ppl: 4.4582295417785645
[eval_Py150 loss, ppl] step:54.875, 	loss: 2.736748218536377, 	ppl: 14.554703712463379
[eval_MeetingBank loss, ppl] step:54.875, 	loss: 1.6691383123397827, 	ppl: 5.077541828155518
***** Evaluating perplexity, Epoch 4/5, step 9.0 *****
[eval loss, ppl] step:55.875, 	loss: 0.8502981066703796, 	ppl: 2.3221139907836914
[eval_CSTANCE loss, ppl] step:55.875, 	loss: 0.41809791326522827, 	ppl: 1.6011273860931396
[eval_20Minuten loss, ppl] step:55.875, 	loss: 1.9719045162200928, 	ppl: 7.574098587036133
[eval_FOMC loss, ppl] step:55.875, 	loss: 0.5194326639175415, 	ppl: 1.5887387990951538
[eval_NumGLUEcm loss, ppl] step:55.875, 	loss: 0.5494287014007568, 	ppl: 2.593398094177246
[eval_ScienceQA loss, ppl] step:55.875, 	loss: 1.0590980052947998, 	ppl: 2.8648722171783447
[eval_NumGLUEds loss, ppl] step:55.875, 	loss: 1.0383832454681396, 	ppl: 4.505984783172607
[eval_Py150 loss, ppl] step:55.875, 	loss: 2.729637861251831, 	ppl: 14.503884315490723
[eval_MeetingBank loss, ppl] step:55.875, 	loss: 1.6713145971298218, 	ppl: 5.080720901489258
***** Evaluating perplexity, Epoch 4/5, step 10.0 *****
[eval loss, ppl] step:56.875, 	loss: 0.8665294051170349, 	ppl: 2.3464651107788086
[eval_CSTANCE loss, ppl] step:56.875, 	loss: 0.4195586144924164, 	ppl: 1.6032309532165527
[eval_20Minuten loss, ppl] step:56.875, 	loss: 1.968996286392212, 	ppl: 7.573889255523682
[eval_FOMC loss, ppl] step:56.875, 	loss: 0.5136364698410034, 	ppl: 1.5892479419708252
[eval_NumGLUEcm loss, ppl] step:56.875, 	loss: 0.5578354597091675, 	ppl: 2.6148197650909424
[eval_ScienceQA loss, ppl] step:56.875, 	loss: 1.0591219663619995, 	ppl: 2.865544080734253
[eval_NumGLUEds loss, ppl] step:56.875, 	loss: 1.06133234500885, 	ppl: 4.570867538452148
[eval_Py150 loss, ppl] step:56.875, 	loss: 2.7331337928771973, 	ppl: 14.501843452453613
[eval_MeetingBank loss, ppl] step:56.875, 	loss: 1.6703745126724243, 	ppl: 5.07786750793457
***** Evaluating perplexity, Epoch 4/5, step 11.0 *****
[eval loss, ppl] step:57.875, 	loss: 0.8806615471839905, 	ppl: 2.3511672019958496
[eval_CSTANCE loss, ppl] step:57.875, 	loss: 0.4147263467311859, 	ppl: 1.6088340282440186
[eval_20Minuten loss, ppl] step:57.875, 	loss: 1.9702374935150146, 	ppl: 7.584198951721191
[eval_FOMC loss, ppl] step:57.875, 	loss: 0.5183141827583313, 	ppl: 1.5880372524261475
[eval_NumGLUEcm loss, ppl] step:57.875, 	loss: 0.5551939606666565, 	ppl: 2.6094789505004883
[eval_ScienceQA loss, ppl] step:57.875, 	loss: 1.0586220026016235, 	ppl: 2.8636293411254883
[eval_NumGLUEds loss, ppl] step:57.875, 	loss: 1.0584259033203125, 	ppl: 4.60026741027832
[eval_Py150 loss, ppl] step:57.875, 	loss: 2.738619327545166, 	ppl: 14.52437973022461
[eval_MeetingBank loss, ppl] step:57.875, 	loss: 1.6676799058914185, 	ppl: 5.069955825805664
***** Evaluating perplexity, Epoch 4/5, step 12.0 *****
[eval loss, ppl] step:58.875, 	loss: 0.8856627345085144, 	ppl: 2.359248638153076
[eval_CSTANCE loss, ppl] step:58.875, 	loss: 0.4124443233013153, 	ppl: 1.6033804416656494
[eval_20Minuten loss, ppl] step:58.875, 	loss: 1.9706531763076782, 	ppl: 7.588158130645752
[eval_FOMC loss, ppl] step:58.875, 	loss: 0.5164689421653748, 	ppl: 1.584100365638733
[eval_NumGLUEcm loss, ppl] step:58.875, 	loss: 0.5527366995811462, 	ppl: 2.6201839447021484
[eval_ScienceQA loss, ppl] step:58.875, 	loss: 1.0589256286621094, 	ppl: 2.8622751235961914
[eval_NumGLUEds loss, ppl] step:58.875, 	loss: 1.0808565616607666, 	ppl: 4.678863525390625
[eval_Py150 loss, ppl] step:58.875, 	loss: 2.7280094623565674, 	ppl: 14.535788536071777
[eval_MeetingBank loss, ppl] step:58.875, 	loss: 1.6677072048187256, 	ppl: 5.067572593688965
[2025-10-21 23:00:27,213] [INFO] [logging.py:107:log_dist] [Rank 0] step=60, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 23:00:27,387] [INFO] [timer.py:264:stop] epoch=0/micro_step=480/global_step=60, RunningAvgSamplesPerSec=5.113638036927182, CurrSamplesPerSec=5.100621262709946, MemAllocated=8.8GB, MaxMemAllocated=13.83GB
***** Evaluating perplexity, Epoch 4/5, step 13.0 *****
[eval loss, ppl] step:59.875, 	loss: 0.8828628063201904, 	ppl: 2.366072416305542
[eval_CSTANCE loss, ppl] step:59.875, 	loss: 0.42100265622138977, 	ppl: 1.608609914779663
[eval_20Minuten loss, ppl] step:59.875, 	loss: 1.9702779054641724, 	ppl: 7.591253280639648
[eval_FOMC loss, ppl] step:59.875, 	loss: 0.5161703824996948, 	ppl: 1.5887597799301147
[eval_NumGLUEcm loss, ppl] step:59.875, 	loss: 0.5444591641426086, 	ppl: 2.632065773010254
[eval_ScienceQA loss, ppl] step:59.875, 	loss: 1.058137059211731, 	ppl: 2.862027645111084
[eval_NumGLUEds loss, ppl] step:59.875, 	loss: 1.0965029001235962, 	ppl: 4.756888389587402
[eval_Py150 loss, ppl] step:59.875, 	loss: 2.73624324798584, 	ppl: 14.649862289428711
[eval_MeetingBank loss, ppl] step:59.875, 	loss: 1.667606234550476, 	ppl: 5.074297904968262
***** Evaluating perplexity, Epoch 4/5, step 14.0 *****
[eval loss, ppl] step:60.875, 	loss: 0.8720167875289917, 	ppl: 2.35322904586792
[eval_CSTANCE loss, ppl] step:60.875, 	loss: 0.4125176668167114, 	ppl: 1.5966966152191162
[eval_20Minuten loss, ppl] step:60.875, 	loss: 1.9723482131958008, 	ppl: 7.596816062927246
[eval_FOMC loss, ppl] step:60.875, 	loss: 0.5192563533782959, 	ppl: 1.5942888259887695
[eval_NumGLUEcm loss, ppl] step:60.875, 	loss: 0.5405083298683167, 	ppl: 2.6185755729675293
[eval_ScienceQA loss, ppl] step:60.875, 	loss: 1.0590695142745972, 	ppl: 2.862914562225342
[eval_NumGLUEds loss, ppl] step:60.875, 	loss: 1.1022218465805054, 	ppl: 4.817025184631348
[eval_Py150 loss, ppl] step:60.875, 	loss: 2.7500545978546143, 	ppl: 14.749519348144531
[eval_MeetingBank loss, ppl] step:60.875, 	loss: 1.667607307434082, 	ppl: 5.073864936828613
saving model to /data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/4...
Sucessful saving model after epoch 4
Beginning of Epoch 5/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 5/5, step 0.0 *****
[eval loss, ppl] step:62.5, 	loss: 0.8361825346946716, 	ppl: 2.2976038455963135
[eval_CSTANCE loss, ppl] step:62.5, 	loss: 0.4071393311023712, 	ppl: 1.6084014177322388
[eval_20Minuten loss, ppl] step:62.5, 	loss: 1.972003698348999, 	ppl: 7.600224018096924
[eval_FOMC loss, ppl] step:62.5, 	loss: 0.5198773741722107, 	ppl: 1.594454288482666
[eval_NumGLUEcm loss, ppl] step:62.5, 	loss: 0.5299480557441711, 	ppl: 2.5664865970611572
[eval_ScienceQA loss, ppl] step:62.5, 	loss: 1.0592814683914185, 	ppl: 2.8653669357299805
[eval_NumGLUEds loss, ppl] step:62.5, 	loss: 1.0778528451919556, 	ppl: 4.8272199630737305
[eval_Py150 loss, ppl] step:62.5, 	loss: 2.767932176589966, 	ppl: 15.035290718078613
[eval_MeetingBank loss, ppl] step:62.5, 	loss: 1.6692557334899902, 	ppl: 5.083131313323975
***** Evaluating perplexity, Epoch 5/5, step 1.0 *****
[eval loss, ppl] step:63.5, 	loss: 0.8140338659286499, 	ppl: 2.2725043296813965
[eval_CSTANCE loss, ppl] step:63.5, 	loss: 0.41157108545303345, 	ppl: 1.6082525253295898
[eval_20Minuten loss, ppl] step:63.5, 	loss: 1.973809838294983, 	ppl: 7.607163429260254
[eval_FOMC loss, ppl] step:63.5, 	loss: 0.5267419815063477, 	ppl: 1.6049422025680542
[eval_NumGLUEcm loss, ppl] step:63.5, 	loss: 0.5109540224075317, 	ppl: 2.5403761863708496
[eval_ScienceQA loss, ppl] step:63.5, 	loss: 1.0600411891937256, 	ppl: 2.8641273975372314
[eval_NumGLUEds loss, ppl] step:63.5, 	loss: 1.0758212804794312, 	ppl: 4.854794979095459
[eval_Py150 loss, ppl] step:63.5, 	loss: 2.778186559677124, 	ppl: 15.156359672546387
[eval_MeetingBank loss, ppl] step:63.5, 	loss: 1.668986439704895, 	ppl: 5.081556797027588
***** Evaluating perplexity, Epoch 5/5, step 2.0 *****
[eval loss, ppl] step:64.5, 	loss: 0.7821933031082153, 	ppl: 2.2649223804473877
[eval_CSTANCE loss, ppl] step:64.5, 	loss: 0.4189840853214264, 	ppl: 1.611303687095642
[eval_20Minuten loss, ppl] step:64.5, 	loss: 1.9715224504470825, 	ppl: 7.611800670623779
[eval_FOMC loss, ppl] step:64.5, 	loss: 0.5332222580909729, 	ppl: 1.6076955795288086
[eval_NumGLUEcm loss, ppl] step:64.5, 	loss: 0.49868953227996826, 	ppl: 2.5568814277648926
[eval_ScienceQA loss, ppl] step:64.5, 	loss: 1.0608892440795898, 	ppl: 2.867208957672119
[eval_NumGLUEds loss, ppl] step:64.5, 	loss: 1.0705962181091309, 	ppl: 4.915272235870361
[eval_Py150 loss, ppl] step:64.5, 	loss: 2.7844090461730957, 	ppl: 15.281396865844727
[eval_MeetingBank loss, ppl] step:64.5, 	loss: 1.669505000114441, 	ppl: 5.086808204650879
***** Evaluating perplexity, Epoch 5/5, step 3.0 *****
[eval loss, ppl] step:65.5, 	loss: 0.7576106190681458, 	ppl: 2.2369942665100098
[eval_CSTANCE loss, ppl] step:65.5, 	loss: 0.4145045876502991, 	ppl: 1.610102653503418
[eval_20Minuten loss, ppl] step:65.5, 	loss: 1.973429560661316, 	ppl: 7.605295181274414
[eval_FOMC loss, ppl] step:65.5, 	loss: 0.5289414525032043, 	ppl: 1.6057089567184448
[eval_NumGLUEcm loss, ppl] step:65.5, 	loss: 0.501526951789856, 	ppl: 2.5241925716400146
[eval_ScienceQA loss, ppl] step:65.5, 	loss: 1.0596555471420288, 	ppl: 2.8645379543304443
[eval_NumGLUEds loss, ppl] step:65.5, 	loss: 1.0801671743392944, 	ppl: 5.021462917327881
[eval_Py150 loss, ppl] step:65.5, 	loss: 2.795947551727295, 	ppl: 15.487600326538086
[eval_MeetingBank loss, ppl] step:65.5, 	loss: 1.6715452671051025, 	ppl: 5.092004776000977
***** Evaluating perplexity, Epoch 5/5, step 4.0 *****
[eval loss, ppl] step:66.5, 	loss: 0.7418465614318848, 	ppl: 2.26589298248291
[eval_CSTANCE loss, ppl] step:66.5, 	loss: 0.41702061891555786, 	ppl: 1.614213228225708
[eval_20Minuten loss, ppl] step:66.5, 	loss: 1.9729344844818115, 	ppl: 7.611248970031738
[eval_FOMC loss, ppl] step:66.5, 	loss: 0.5313017964363098, 	ppl: 1.6073904037475586
[eval_NumGLUEcm loss, ppl] step:66.5, 	loss: 0.4976426959037781, 	ppl: 2.5831685066223145
[eval_ScienceQA loss, ppl] step:66.5, 	loss: 1.0610631704330444, 	ppl: 2.865784168243408
[eval_NumGLUEds loss, ppl] step:66.5, 	loss: 1.0805505514144897, 	ppl: 5.089022636413574
[eval_Py150 loss, ppl] step:66.5, 	loss: 2.806952476501465, 	ppl: 15.626463890075684
[eval_MeetingBank loss, ppl] step:66.5, 	loss: 1.668968677520752, 	ppl: 5.087286949157715
***** Evaluating perplexity, Epoch 5/5, step 5.0 *****
[eval loss, ppl] step:67.5, 	loss: 0.7258259057998657, 	ppl: 2.251267671585083
[eval_CSTANCE loss, ppl] step:67.5, 	loss: 0.4133301079273224, 	ppl: 1.6049449443817139
[eval_20Minuten loss, ppl] step:67.5, 	loss: 1.9769748449325562, 	ppl: 7.622453212738037
[eval_FOMC loss, ppl] step:67.5, 	loss: 0.5393041968345642, 	ppl: 1.6167857646942139
[eval_NumGLUEcm loss, ppl] step:67.5, 	loss: 0.5114341974258423, 	ppl: 2.564206600189209
[eval_ScienceQA loss, ppl] step:67.5, 	loss: 1.0602563619613647, 	ppl: 2.8640308380126953
[eval_NumGLUEds loss, ppl] step:67.5, 	loss: 1.0932915210723877, 	ppl: 5.230105400085449
[eval_Py150 loss, ppl] step:67.5, 	loss: 2.814817190170288, 	ppl: 15.698182106018066
[eval_MeetingBank loss, ppl] step:67.5, 	loss: 1.6695871353149414, 	ppl: 5.088727951049805
***** Evaluating perplexity, Epoch 5/5, step 6.0 *****
[eval loss, ppl] step:68.5, 	loss: 0.7244552373886108, 	ppl: 2.2627625465393066
[eval_CSTANCE loss, ppl] step:68.5, 	loss: 0.4103783369064331, 	ppl: 1.608627438545227
[eval_20Minuten loss, ppl] step:68.5, 	loss: 1.9737684726715088, 	ppl: 7.614047050476074
[eval_FOMC loss, ppl] step:68.5, 	loss: 0.5360782742500305, 	ppl: 1.6105209589004517
[eval_NumGLUEcm loss, ppl] step:68.5, 	loss: 0.503212571144104, 	ppl: 2.590512752532959
[eval_ScienceQA loss, ppl] step:68.5, 	loss: 1.0598835945129395, 	ppl: 2.8635549545288086
[eval_NumGLUEds loss, ppl] step:68.5, 	loss: 1.1110448837280273, 	ppl: 5.360405445098877
[eval_Py150 loss, ppl] step:68.5, 	loss: 2.817244529724121, 	ppl: 15.819855690002441
[eval_MeetingBank loss, ppl] step:68.5, 	loss: 1.67057466506958, 	ppl: 5.087085723876953
[2025-10-21 23:03:12,706] [INFO] [logging.py:107:log_dist] [Rank 0] step=70, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 23:03:12,905] [INFO] [timer.py:264:stop] epoch=0/micro_step=560/global_step=70, RunningAvgSamplesPerSec=5.141928627209996, CurrSamplesPerSec=5.304221540822787, MemAllocated=8.81GB, MaxMemAllocated=13.83GB
***** Evaluating perplexity, Epoch 5/5, step 7.0 *****
[eval loss, ppl] step:69.5, 	loss: 0.7354474663734436, 	ppl: 2.277805805206299
[eval_CSTANCE loss, ppl] step:69.5, 	loss: 0.4175126552581787, 	ppl: 1.616837978363037
[eval_20Minuten loss, ppl] step:69.5, 	loss: 1.9748679399490356, 	ppl: 7.624599933624268
[eval_FOMC loss, ppl] step:69.5, 	loss: 0.5363659262657166, 	ppl: 1.6112364530563354
[eval_NumGLUEcm loss, ppl] step:69.5, 	loss: 0.5147690773010254, 	ppl: 2.600369691848755
[eval_ScienceQA loss, ppl] step:69.5, 	loss: 1.059670329093933, 	ppl: 2.8628547191619873
[eval_NumGLUEds loss, ppl] step:69.5, 	loss: 1.1250871419906616, 	ppl: 5.493231773376465
[eval_Py150 loss, ppl] step:69.5, 	loss: 2.819592237472534, 	ppl: 15.954351425170898
[eval_MeetingBank loss, ppl] step:69.5, 	loss: 1.6695096492767334, 	ppl: 5.092606544494629
***** Evaluating perplexity, Epoch 5/5, step 8.0 *****
[eval loss, ppl] step:70.5, 	loss: 0.7146857976913452, 	ppl: 2.2610280513763428
[eval_CSTANCE loss, ppl] step:70.5, 	loss: 0.4167829155921936, 	ppl: 1.6141114234924316
[eval_20Minuten loss, ppl] step:70.5, 	loss: 1.9769285917282104, 	ppl: 7.633057117462158
[eval_FOMC loss, ppl] step:70.5, 	loss: 0.5436326265335083, 	ppl: 1.614093542098999
[eval_NumGLUEcm loss, ppl] step:70.5, 	loss: 0.5118271112442017, 	ppl: 2.5874340534210205
[eval_ScienceQA loss, ppl] step:70.5, 	loss: 1.0605148077011108, 	ppl: 2.8652238845825195
[eval_NumGLUEds loss, ppl] step:70.5, 	loss: 1.116944670677185, 	ppl: 5.567702293395996
[eval_Py150 loss, ppl] step:70.5, 	loss: 2.8329527378082275, 	ppl: 16.00790786743164
[eval_MeetingBank loss, ppl] step:70.5, 	loss: 1.6700340509414673, 	ppl: 5.0948028564453125
***** Evaluating perplexity, Epoch 5/5, step 9.0 *****
[eval loss, ppl] step:71.5, 	loss: 0.7209393382072449, 	ppl: 2.2593228816986084
[eval_CSTANCE loss, ppl] step:71.5, 	loss: 0.4146493077278137, 	ppl: 1.6150860786437988
[eval_20Minuten loss, ppl] step:71.5, 	loss: 1.9775925874710083, 	ppl: 7.6351847648620605
[eval_FOMC loss, ppl] step:71.5, 	loss: 0.5325577259063721, 	ppl: 1.6087371110916138
[eval_NumGLUEcm loss, ppl] step:71.5, 	loss: 0.5186096429824829, 	ppl: 2.5823919773101807
[eval_ScienceQA loss, ppl] step:71.5, 	loss: 1.0620208978652954, 	ppl: 2.867597818374634
[eval_NumGLUEds loss, ppl] step:71.5, 	loss: 1.1256672143936157, 	ppl: 5.642969608306885
[eval_Py150 loss, ppl] step:71.5, 	loss: 2.8443620204925537, 	ppl: 16.164749145507812
[eval_MeetingBank loss, ppl] step:71.5, 	loss: 1.6703194379806519, 	ppl: 5.097558498382568
***** Evaluating perplexity, Epoch 5/5, step 10.0 *****
[eval loss, ppl] step:72.5, 	loss: 0.7171716094017029, 	ppl: 2.253849744796753
[eval_CSTANCE loss, ppl] step:72.5, 	loss: 0.4125652015209198, 	ppl: 1.6099519729614258
[eval_20Minuten loss, ppl] step:72.5, 	loss: 1.975091814994812, 	ppl: 7.637572765350342
[eval_FOMC loss, ppl] step:72.5, 	loss: 0.5379608273506165, 	ppl: 1.614380121231079
[eval_NumGLUEcm loss, ppl] step:72.5, 	loss: 0.5147958993911743, 	ppl: 2.574843406677246
[eval_ScienceQA loss, ppl] step:72.5, 	loss: 1.063219428062439, 	ppl: 2.8697588443756104
[eval_NumGLUEds loss, ppl] step:72.5, 	loss: 1.1242871284484863, 	ppl: 5.747020721435547
[eval_Py150 loss, ppl] step:72.5, 	loss: 2.8455846309661865, 	ppl: 16.21085548400879
[eval_MeetingBank loss, ppl] step:72.5, 	loss: 1.6705567836761475, 	ppl: 5.103881359100342
***** Evaluating perplexity, Epoch 5/5, step 11.0 *****
[eval loss, ppl] step:73.5, 	loss: 0.7122246623039246, 	ppl: 2.2450900077819824
[eval_CSTANCE loss, ppl] step:73.5, 	loss: 0.4132632911205292, 	ppl: 1.603012204170227
[eval_20Minuten loss, ppl] step:73.5, 	loss: 1.9774597883224487, 	ppl: 7.647989273071289
[eval_FOMC loss, ppl] step:73.5, 	loss: 0.5385664701461792, 	ppl: 1.6140629053115845
[eval_NumGLUEcm loss, ppl] step:73.5, 	loss: 0.5049091577529907, 	ppl: 2.5726442337036133
[eval_ScienceQA loss, ppl] step:73.5, 	loss: 1.0637965202331543, 	ppl: 2.8735804557800293
[eval_NumGLUEds loss, ppl] step:73.5, 	loss: 1.1208417415618896, 	ppl: 5.777235507965088
[eval_Py150 loss, ppl] step:73.5, 	loss: 2.8560638427734375, 	ppl: 16.340463638305664
[eval_MeetingBank loss, ppl] step:73.5, 	loss: 1.6706387996673584, 	ppl: 5.1057538986206055
***** Evaluating perplexity, Epoch 5/5, step 12.0 *****
[eval loss, ppl] step:74.5, 	loss: 0.6927094459533691, 	ppl: 2.238325834274292
[eval_CSTANCE loss, ppl] step:74.5, 	loss: 0.4153840243816376, 	ppl: 1.6107752323150635
[eval_20Minuten loss, ppl] step:74.5, 	loss: 1.9783906936645508, 	ppl: 7.638938903808594
[eval_FOMC loss, ppl] step:74.5, 	loss: 0.5424532294273376, 	ppl: 1.622382402420044
[eval_NumGLUEcm loss, ppl] step:74.5, 	loss: 0.5032818913459778, 	ppl: 2.567394733428955
[eval_ScienceQA loss, ppl] step:74.5, 	loss: 1.0659898519515991, 	ppl: 2.8793087005615234
[eval_NumGLUEds loss, ppl] step:74.5, 	loss: 1.1139599084854126, 	ppl: 5.7762250900268555
[eval_Py150 loss, ppl] step:74.5, 	loss: 2.858328104019165, 	ppl: 16.48509979248047
[eval_MeetingBank loss, ppl] step:74.5, 	loss: 1.6723190546035767, 	ppl: 5.111311435699463
***** Evaluating perplexity, Epoch 5/5, step 13.0 *****
[eval loss, ppl] step:75.5, 	loss: 0.6900137662887573, 	ppl: 2.2329094409942627
[eval_CSTANCE loss, ppl] step:75.5, 	loss: 0.4131243824958801, 	ppl: 1.609600305557251
[eval_20Minuten loss, ppl] step:75.5, 	loss: 1.9773308038711548, 	ppl: 7.651786804199219
[eval_FOMC loss, ppl] step:75.5, 	loss: 0.5546019673347473, 	ppl: 1.6275831460952759
[eval_NumGLUEcm loss, ppl] step:75.5, 	loss: 0.49920251965522766, 	ppl: 2.5577945709228516
[eval_ScienceQA loss, ppl] step:75.5, 	loss: 1.0676935911178589, 	ppl: 2.8839571475982666
[eval_NumGLUEds loss, ppl] step:75.5, 	loss: 1.1067466735839844, 	ppl: 5.78315544128418
[eval_Py150 loss, ppl] step:75.5, 	loss: 2.8624188899993896, 	ppl: 16.47669792175293
[eval_MeetingBank loss, ppl] step:75.5, 	loss: 1.6744968891143799, 	ppl: 5.111413955688477
***** Evaluating perplexity, Epoch 5/5, step 14.0 *****
[eval loss, ppl] step:76.5, 	loss: 0.685462474822998, 	ppl: 2.2073545455932617
[eval_CSTANCE loss, ppl] step:76.5, 	loss: 0.41487857699394226, 	ppl: 1.6146063804626465
[eval_20Minuten loss, ppl] step:76.5, 	loss: 1.9796513319015503, 	ppl: 7.656173229217529
[eval_FOMC loss, ppl] step:76.5, 	loss: 0.5461917519569397, 	ppl: 1.6170823574066162
[eval_NumGLUEcm loss, ppl] step:76.5, 	loss: 0.5013384222984314, 	ppl: 2.511890411376953
[eval_ScienceQA loss, ppl] step:76.5, 	loss: 1.0692065954208374, 	ppl: 2.8892059326171875
[eval_NumGLUEds loss, ppl] step:76.5, 	loss: 1.097351312637329, 	ppl: 5.71371603012085
[eval_Py150 loss, ppl] step:76.5, 	loss: 2.8665077686309814, 	ppl: 16.66958999633789
[eval_MeetingBank loss, ppl] step:76.5, 	loss: 1.6746388673782349, 	ppl: 5.120994567871094
saving model to /data1/TAP/model_con/1020/ours_C-STANCE_FOMC_MeetingBank_ScienceQA_NumGLUE-cm_epoch5_Llama3Exp_0.001/5...
[2025-10-21 23:05:28,280] [INFO] [launch.py:351:main] Process 2055973 exits successfully.
[2025-10-21 23:05:28,281] [INFO] [launch.py:351:main] Process 2055974 exits successfully.
[2025-10-21 23:05:28,281] [INFO] [launch.py:351:main] Process 2055975 exits successfully.
Sucessful saving model after epoch 5
[2025-10-21 23:05:36,290] [INFO] [launch.py:351:main] Process 2055972 exits successfully.
