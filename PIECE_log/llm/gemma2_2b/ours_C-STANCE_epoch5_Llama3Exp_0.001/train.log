[2025-10-21 19:33:26,646] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 19:33:28,688] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 19:33:28,894] [WARNING] [runner.py:220:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-10-21 19:33:28,894] [INFO] [runner.py:610:main] cmd = /home/TAP/anaconda3/envs/llama2/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=27833 --enable_each_rank_log=None training/main.py --data_path /data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/C-STANCE --model_name_or_path /data2/TAP/model/gemma-2-2b-it --per_device_train_batch_size 2 --per_device_eval_batch_size 1 --max_prompt_len 1024 --max_ans_len 512 --learning_rate 1e-5 --weight_decay 0. --num_train_epochs 5 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --num_warmup_steps 0 --seed 42 --zero_stage 2 --deepspeed --print_loss --CL_method base --enable_tensorboard --tensorboard_path /data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001/log/ --offload --ood_eval_dir /data2/TAP/data/continue_eval_loss_data_small --mask_method ours --top_ratio 0.001 --target_name C-STANCE --output_dir /data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001 --test_file_dir /data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000
[2025-10-21 19:33:30,949] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 19:33:33,016] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 19:33:33,222] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-10-21 19:33:33,222] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-10-21 19:33:33,222] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-10-21 19:33:33,222] [INFO] [launch.py:164:main] dist_world_size=4
[2025-10-21 19:33:33,222] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-10-21 19:33:33,223] [INFO] [launch.py:256:main] process 1615443 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=0', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/C-STANCE', '--model_name_or_path', '/data2/TAP/model/gemma-2-2b-it', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'ours', '--top_ratio', '0.001', '--target_name', 'C-STANCE', '--output_dir', '/data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001', '--test_file_dir', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000']
[2025-10-21 19:33:33,223] [INFO] [launch.py:256:main] process 1615444 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=1', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/C-STANCE', '--model_name_or_path', '/data2/TAP/model/gemma-2-2b-it', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'ours', '--top_ratio', '0.001', '--target_name', 'C-STANCE', '--output_dir', '/data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001', '--test_file_dir', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000']
[2025-10-21 19:33:33,224] [INFO] [launch.py:256:main] process 1615445 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=2', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/C-STANCE', '--model_name_or_path', '/data2/TAP/model/gemma-2-2b-it', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'ours', '--top_ratio', '0.001', '--target_name', 'C-STANCE', '--output_dir', '/data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001', '--test_file_dir', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000']
[2025-10-21 19:33:33,225] [INFO] [launch.py:256:main] process 1615446 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=3', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/C-STANCE', '--model_name_or_path', '/data2/TAP/model/gemma-2-2b-it', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'ours', '--top_ratio', '0.001', '--target_name', 'C-STANCE', '--output_dir', '/data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001', '--test_file_dir', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000']
[2025-10-21 19:33:37,089] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 19:33:37,103] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 19:33:37,105] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 19:33:37,116] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-21 19:33:38,999] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 19:33:39,063] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 19:33:39,063] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-21 19:33:39,125] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Using integrations.HfDeepSpeedConfig (new API)
Using integrations.HfDeepSpeedConfig (new API)
Using integrations.HfDeepSpeedConfig (new API)
Using integrations.HfDeepSpeedConfig (new API)
/data1/TAP/model_exp_2b/1020_C-STANCE_ours_parameters_test_epoch1_random_1000/parameters_import_new_2/top0.001
[2025-10-21 19:33:40,057] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-10-21 19:33:40,057] [INFO] [comm.py:706:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/data1/TAP/model_exp_2b/1020_C-STANCE_ours_parameters_test_epoch1_random_1000/parameters_import_new_2/top0.001
/data1/TAP/model_exp_2b/1020_C-STANCE_ours_parameters_test_epoch1_random_1000/parameters_import_new_2/top0.001
/data1/TAP/model_exp_2b/1020_C-STANCE_ours_parameters_test_epoch1_random_1000/parameters_import_new_2/top0.001
[2025-10-21 19:33:40,435] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-10-21 19:33:40,443] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-10-21 19:33:40,469] [INFO] [comm.py:675:init_distributed] cdb=None
ninja: no work to do.
Time to load cpu_adam op: 2.3341965675354004 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-10-21 19:36:29,187] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.1, git-hash=unknown, git-branch=unknown
[2025-10-21 19:36:29,188] [INFO] [comm.py:700:init_distributed] Distributed backend already initialized
[2025-10-21 19:36:29,188] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
ninja: no work to do.
Time to load cpu_adam op: 2.411691665649414 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-10-21 19:36:29,307] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
ninja: no work to do.
Time to load cpu_adam op: 2.4418303966522217 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-10-21 19:36:29,338] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
Time to load cpu_adam op: 2.5254697799682617 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-10-21 19:36:29,426] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
[2025-10-21 19:36:31,048] [INFO] [engine.py:1325:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=4
	 self.mp_world_size=1
	 self.seq_dp_world_size=4
	 self.sequence_parallel_size=1
***********************************************
[2025-10-21 19:36:34,893] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-10-21 19:36:34,896] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-10-21 19:36:34,896] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-10-21 19:36:34,915] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-10-21 19:36:34,915] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-10-21 19:36:34,915] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2025-10-21 19:36:34,915] [INFO] [stage_1_and_2.py:151:__init__] Reduce bucket size 500000000
[2025-10-21 19:36:34,915] [INFO] [stage_1_and_2.py:152:__init__] Allgather bucket size 500000000
[2025-10-21 19:36:34,915] [INFO] [stage_1_and_2.py:153:__init__] CPU Offload: True
[2025-10-21 19:36:34,915] [INFO] [stage_1_and_2.py:154:__init__] Round robin gradient partitioning: False
[2025-10-21 19:36:42,726] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-10-21 19:36:42,727] [INFO] [utils.py:782:see_memory_usage] MA 4.91 GB         Max_MA 4.91 GB         CA 4.91 GB         Max_CA 5 GB 
[2025-10-21 19:36:42,727] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 61.46 GB, percent = 6.1%
[2025-10-21 19:36:43,013] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-10-21 19:36:43,014] [INFO] [utils.py:782:see_memory_usage] MA 4.91 GB         Max_MA 4.91 GB         CA 4.91 GB         Max_CA 5 GB 
[2025-10-21 19:36:43,014] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 63.34 GB, percent = 6.3%
[2025-10-21 19:36:43,014] [INFO] [stage_1_and_2.py:573:__init__] optimizer state initialized
[2025-10-21 19:36:43,181] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-10-21 19:36:43,181] [INFO] [utils.py:782:see_memory_usage] MA 4.91 GB         Max_MA 4.91 GB         CA 4.91 GB         Max_CA 5 GB 
[2025-10-21 19:36:43,181] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 63.34 GB, percent = 6.3%
[2025-10-21 19:36:43,184] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-10-21 19:36:43,184] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-10-21 19:36:43,184] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x730c7c4dce80>
[2025-10-21 19:36:43,184] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 19:36:43,185] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-10-21 19:36:43,185] [INFO] [config.py:921:print] DeepSpeedEngine configuration:
[2025-10-21 19:36:43,185] [INFO] [config.py:925:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-10-21 19:36:43,185] [INFO] [config.py:925:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-10-21 19:36:43,185] [INFO] [config.py:925:print]   amp_enabled .................. False
[2025-10-21 19:36:43,185] [INFO] [config.py:925:print]   amp_params ................... False
[2025-10-21 19:36:43,185] [INFO] [config.py:925:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-10-21 19:36:43,185] [INFO] [config.py:925:print]   bfloat16_config .............. enabled=False immediate_grad_update=False check_grad_overflow=False
[2025-10-21 19:36:43,185] [INFO] [config.py:925:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-10-21 19:36:43,185] [INFO] [config.py:925:print]   checkpoint_parallel_write_pipeline  False
[2025-10-21 19:36:43,185] [INFO] [config.py:925:print]   checkpoint_tag_validation_enabled  True
[2025-10-21 19:36:43,185] [INFO] [config.py:925:print]   checkpoint_tag_validation_fail  False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x730c7c4dcc10>
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   communication_data_type ...... None
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   curriculum_enabled_legacy .... False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   curriculum_params_legacy ..... False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   data_efficiency_enabled ...... False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   dataloader_drop_last ......... False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   disable_allgather ............ False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   dump_state ................... False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   eigenvalue_enabled ........... False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   eigenvalue_gas_boundary_resolution  1
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   eigenvalue_layer_num ......... 0
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   eigenvalue_max_iter .......... 100
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   eigenvalue_stability ......... 1e-06
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   eigenvalue_tol ............... 0.01
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   eigenvalue_verbose ........... False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   elasticity_enabled ........... False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   global_rank .................. 0
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   grad_accum_dtype ............. None
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   gradient_accumulation_steps .. 8
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   gradient_clipping ............ 1.0
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   gradient_predivide_factor .... 1.0
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   graph_harvesting ............. False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   load_universal_checkpoint .... False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   memory_breakdown ............. False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   mics_hierarchial_params_gather  False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   mics_shard_size .............. -1
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=True, output_path='/data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001/log//ds_tensorboard_logs/', job_name='v2_sft_tensorboard') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   optimizer_legacy_fusion ...... False
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   optimizer_name ............... None
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   optimizer_params ............. None
[2025-10-21 19:36:43,186] [INFO] [config.py:925:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   pld_enabled .................. False
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   pld_params ................... False
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   prescale_gradients ........... False
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   scheduler_name ............... None
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   scheduler_params ............. None
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   seq_parallel_communication_data_type  torch.float32
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   sparse_attention ............. None
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   sparse_gradients_enabled ..... False
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   steps_per_print .............. 10
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   timers_config ................ enabled=True synchronized=True
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   train_batch_size ............. 64
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   train_micro_batch_size_per_gpu  2
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   use_data_before_expert_parallel_  False
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   use_node_local_storage ....... False
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   wall_clock_breakdown ......... False
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   weight_quantization_config ... None
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   world_size ................... 4
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   zero_allow_untested_optimizer  False
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   zero_enabled ................. True
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   zero_force_ds_cpu_optimizer .. True
[2025-10-21 19:36:43,187] [INFO] [config.py:925:print]   zero_optimization_stage ...... 2
[2025-10-21 19:36:43,187] [INFO] [config.py:911:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 2, 
    "steps_per_print": 10, 
    "zero_optimization": {
        "stage": 2, 
        "offload_param": {
            "device": "cpu"
        }, 
        "offload_optimizer": {
            "device": "cpu"
        }, 
        "stage3_param_persistence_threshold": 1.000000e+04, 
        "stage3_max_live_parameters": 3.000000e+07, 
        "stage3_prefetch_bucket_size": 3.000000e+07, 
        "memory_efficient_linear": false
    }, 
    "bfloat16": {
        "enabled": "auto"
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "hybrid_engine": {
        "enabled": false, 
        "max_out_tokens": 512, 
        "inference_tp_size": 1, 
        "release_inference_cache": false, 
        "pin_parameters": true, 
        "tp_gather_partition_size": 8
    }, 
    "tensorboard": {
        "enabled": true, 
        "output_path": "/data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001/log//ds_tensorboard_logs/", 
        "job_name": "v2_sft_tensorboard"
    }
}
***** Running training *****
Beginning of Epoch 1/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 1/5, step 0.0 *****
[eval loss, ppl] step:0.0, 	loss: 9.53918170928955, 	ppl: 14235.392578125
[eval_CSTANCE loss, ppl] step:0.0, 	loss: 9.436378479003906, 	ppl: 12887.6025390625
[eval_20Minuten loss, ppl] step:0.0, 	loss: 2.4426393508911133, 	ppl: 12.41977310180664
[eval_FOMC loss, ppl] step:0.0, 	loss: 14.35527229309082, 	ppl: 1246127.25
[eval_NumGLUEcm loss, ppl] step:0.0, 	loss: 9.86559772491455, 	ppl: 32841.5546875
[eval_ScienceQA loss, ppl] step:0.0, 	loss: 1.8220826387405396, 	ppl: 6.543968200683594
[eval_NumGLUEds loss, ppl] step:0.0, 	loss: 11.06635856628418, 	ppl: 38091.92578125
[eval_Py150 loss, ppl] step:0.0, 	loss: 4.124669551849365, 	ppl: 59.24882125854492
[eval_MeetingBank loss, ppl] step:0.0, 	loss: 2.5454351902008057, 	ppl: 12.52100658416748
***** Evaluating perplexity, Epoch 1/5, step 1.0 *****
[eval loss, ppl] step:1.0, 	loss: 7.60063362121582, 	ppl: 2082.56591796875
[eval_CSTANCE loss, ppl] step:1.0, 	loss: 7.466812610626221, 	ppl: 1690.2137451171875
[eval_20Minuten loss, ppl] step:1.0, 	loss: 2.4234650135040283, 	ppl: 12.163904190063477
[eval_FOMC loss, ppl] step:1.0, 	loss: 13.703713417053223, 	ppl: 655187.1875
[eval_NumGLUEcm loss, ppl] step:1.0, 	loss: 9.60092544555664, 	ppl: 24315.83203125
[eval_ScienceQA loss, ppl] step:1.0, 	loss: 1.8106801509857178, 	ppl: 6.4743852615356445
[eval_NumGLUEds loss, ppl] step:1.0, 	loss: 10.805367469787598, 	ppl: 29781.6484375
[eval_Py150 loss, ppl] step:1.0, 	loss: 4.058642387390137, 	ppl: 55.73594284057617
[eval_MeetingBank loss, ppl] step:1.0, 	loss: 2.53481125831604, 	ppl: 12.3825101852417
***** Evaluating perplexity, Epoch 1/5, step 2.0 *****
[eval loss, ppl] step:2.0, 	loss: 5.412750720977783, 	ppl: 236.50296020507812
[eval_CSTANCE loss, ppl] step:2.0, 	loss: 5.142818450927734, 	ppl: 171.2159423828125
[eval_20Minuten loss, ppl] step:2.0, 	loss: 2.396580457687378, 	ppl: 11.821611404418945
[eval_FOMC loss, ppl] step:2.0, 	loss: 12.656496047973633, 	ppl: 239031.28125
[eval_NumGLUEcm loss, ppl] step:2.0, 	loss: 9.184422492980957, 	ppl: 15624.7109375
[eval_ScienceQA loss, ppl] step:2.0, 	loss: 1.7957258224487305, 	ppl: 6.364225387573242
[eval_NumGLUEds loss, ppl] step:2.0, 	loss: 10.431472778320312, 	ppl: 20933.59765625
[eval_Py150 loss, ppl] step:2.0, 	loss: 3.9712679386138916, 	ppl: 51.306663513183594
[eval_MeetingBank loss, ppl] step:2.0, 	loss: 2.5232479572296143, 	ppl: 12.200069427490234
***** Evaluating perplexity, Epoch 1/5, step 3.0 *****
[eval loss, ppl] step:3.0, 	loss: 4.608733654022217, 	ppl: 106.02072143554688
[eval_CSTANCE loss, ppl] step:3.0, 	loss: 4.304313659667969, 	ppl: 76.32464599609375
[eval_20Minuten loss, ppl] step:3.0, 	loss: 2.381756067276001, 	ppl: 11.629234313964844
[eval_FOMC loss, ppl] step:3.0, 	loss: 12.058954238891602, 	ppl: 134232.9375
[eval_NumGLUEcm loss, ppl] step:3.0, 	loss: 8.947819709777832, 	ppl: 12094.533203125
[eval_ScienceQA loss, ppl] step:3.0, 	loss: 1.7877838611602783, 	ppl: 6.306967735290527
[eval_NumGLUEds loss, ppl] step:3.0, 	loss: 10.192483901977539, 	ppl: 16615.0234375
[eval_Py150 loss, ppl] step:3.0, 	loss: 3.913571834564209, 	ppl: 48.414241790771484
[eval_MeetingBank loss, ppl] step:3.0, 	loss: 2.5168862342834473, 	ppl: 12.082999229431152
***** Evaluating perplexity, Epoch 1/5, step 4.0 *****
[eval loss, ppl] step:4.0, 	loss: 3.1030962467193604, 	ppl: 23.358501434326172
[eval_CSTANCE loss, ppl] step:4.0, 	loss: 2.647742509841919, 	ppl: 17.475757598876953
[eval_20Minuten loss, ppl] step:4.0, 	loss: 2.35660457611084, 	ppl: 11.323724746704102
[eval_FOMC loss, ppl] step:4.0, 	loss: 10.92483139038086, 	ppl: 45436.578125
[eval_NumGLUEcm loss, ppl] step:4.0, 	loss: 8.538262367248535, 	ppl: 7779.283203125
[eval_ScienceQA loss, ppl] step:4.0, 	loss: 1.7723034620285034, 	ppl: 6.205280303955078
[eval_NumGLUEds loss, ppl] step:4.0, 	loss: 9.790311813354492, 	ppl: 11434.798828125
[eval_Py150 loss, ppl] step:4.0, 	loss: 3.8383145332336426, 	ppl: 44.746707916259766
[eval_MeetingBank loss, ppl] step:4.0, 	loss: 2.503239393234253, 	ppl: 11.904911041259766
***** Evaluating perplexity, Epoch 1/5, step 5.0 *****
[eval loss, ppl] step:5.0, 	loss: 2.3303985595703125, 	ppl: 10.77472972869873
[eval_CSTANCE loss, ppl] step:5.0, 	loss: 1.8227205276489258, 	ppl: 8.379003524780273
[eval_20Minuten loss, ppl] step:5.0, 	loss: 2.3423666954040527, 	ppl: 11.144142150878906
[eval_FOMC loss, ppl] step:5.0, 	loss: 10.211397171020508, 	ppl: 22104.87890625
[eval_NumGLUEcm loss, ppl] step:5.0, 	loss: 8.241998672485352, 	ppl: 5846.46435546875
[eval_ScienceQA loss, ppl] step:5.0, 	loss: 1.7618646621704102, 	ppl: 6.143032073974609
[eval_NumGLUEds loss, ppl] step:5.0, 	loss: 9.521405220031738, 	ppl: 8860.9208984375
[eval_Py150 loss, ppl] step:5.0, 	loss: 3.7702796459198, 	ppl: 42.04560852050781
[eval_MeetingBank loss, ppl] step:5.0, 	loss: 2.4973323345184326, 	ppl: 11.799015045166016
***** Evaluating perplexity, Epoch 1/5, step 6.0 *****
[eval loss, ppl] step:6.0, 	loss: 1.8756552934646606, 	ppl: 6.8337883949279785
[eval_CSTANCE loss, ppl] step:6.0, 	loss: 1.3524521589279175, 	ppl: 5.451222896575928
[eval_20Minuten loss, ppl] step:6.0, 	loss: 2.3303067684173584, 	ppl: 11.00541877746582
[eval_FOMC loss, ppl] step:6.0, 	loss: 9.719795227050781, 	ppl: 13944.4873046875
[eval_NumGLUEcm loss, ppl] step:6.0, 	loss: 8.06969165802002, 	ppl: 4737.7822265625
[eval_ScienceQA loss, ppl] step:6.0, 	loss: 1.755281925201416, 	ppl: 6.096928119659424
[eval_NumGLUEds loss, ppl] step:6.0, 	loss: 9.328167915344238, 	ppl: 7425.87890625
[eval_Py150 loss, ppl] step:6.0, 	loss: 3.7229018211364746, 	ppl: 40.0984001159668
[eval_MeetingBank loss, ppl] step:6.0, 	loss: 2.489758014678955, 	ppl: 11.708414077758789
***** Evaluating perplexity, Epoch 1/5, step 7.0 *****
[eval loss, ppl] step:7.0, 	loss: 1.410783052444458, 	ppl: 4.2625837326049805
[eval_CSTANCE loss, ppl] step:7.0, 	loss: 0.9009207487106323, 	ppl: 3.5201776027679443
[eval_20Minuten loss, ppl] step:7.0, 	loss: 2.319875478744507, 	ppl: 10.892168045043945
[eval_FOMC loss, ppl] step:7.0, 	loss: 9.207627296447754, 	ppl: 8168.37255859375
[eval_NumGLUEcm loss, ppl] step:7.0, 	loss: 7.879099369049072, 	ppl: 3937.17236328125
[eval_ScienceQA loss, ppl] step:7.0, 	loss: 1.750097393989563, 	ppl: 6.0627875328063965
[eval_NumGLUEds loss, ppl] step:7.0, 	loss: 9.12706184387207, 	ppl: 6203.37109375
[eval_Py150 loss, ppl] step:7.0, 	loss: 3.677135944366455, 	ppl: 38.63795471191406
[eval_MeetingBank loss, ppl] step:7.0, 	loss: 2.484313488006592, 	ppl: 11.634490966796875
***** Evaluating perplexity, Epoch 1/5, step 8.0 *****
[eval loss, ppl] step:8.0, 	loss: 1.02578604221344, 	ppl: 2.893960952758789
[eval_CSTANCE loss, ppl] step:8.0, 	loss: 0.5819194912910461, 	ppl: 2.4656636714935303
[eval_20Minuten loss, ppl] step:8.0, 	loss: 2.309164047241211, 	ppl: 10.771255493164062
[eval_FOMC loss, ppl] step:8.0, 	loss: 8.694180488586426, 	ppl: 4905.6826171875
[eval_NumGLUEcm loss, ppl] step:8.0, 	loss: 7.702178001403809, 	ppl: 3236.914794921875
[eval_ScienceQA loss, ppl] step:8.0, 	loss: 1.743148684501648, 	ppl: 6.020232200622559
[eval_NumGLUEds loss, ppl] step:8.0, 	loss: 8.927217483520508, 	ppl: 5118.8291015625
[eval_Py150 loss, ppl] step:8.0, 	loss: 3.6388111114501953, 	ppl: 36.973026275634766
[eval_MeetingBank loss, ppl] step:8.0, 	loss: 2.48112154006958, 	ppl: 11.564172744750977
***** Evaluating perplexity, Epoch 1/5, step 9.0 *****
[eval loss, ppl] step:9.0, 	loss: 0.8240993618965149, 	ppl: 2.3592820167541504
[eval_CSTANCE loss, ppl] step:9.0, 	loss: 0.4335140883922577, 	ppl: 2.0177056789398193
[eval_20Minuten loss, ppl] step:9.0, 	loss: 2.302241563796997, 	ppl: 10.684356689453125
[eval_FOMC loss, ppl] step:9.0, 	loss: 8.24486255645752, 	ppl: 3141.67822265625
[eval_NumGLUEcm loss, ppl] step:9.0, 	loss: 7.519808292388916, 	ppl: 2670.114990234375
[eval_ScienceQA loss, ppl] step:9.0, 	loss: 1.7396308183670044, 	ppl: 5.9867963790893555
[eval_NumGLUEds loss, ppl] step:9.0, 	loss: 8.774543762207031, 	ppl: 4411.078125
[eval_Py150 loss, ppl] step:9.0, 	loss: 3.5922820568084717, 	ppl: 35.54345703125
[eval_MeetingBank loss, ppl] step:9.0, 	loss: 2.4758410453796387, 	ppl: 11.50246810913086
[2025-10-21 19:42:36,176] [INFO] [logging.py:107:log_dist] [Rank 0] step=10, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 19:42:36,413] [INFO] [timer.py:264:stop] epoch=0/micro_step=80/global_step=10, RunningAvgSamplesPerSec=4.851115572273881, CurrSamplesPerSec=4.983903043130702, MemAllocated=9.05GB, MaxMemAllocated=13.95GB
***** Evaluating perplexity, Epoch 1/5, step 10.0 *****
[eval loss, ppl] step:10.0, 	loss: 0.6661790609359741, 	ppl: 2.0085644721984863
[eval_CSTANCE loss, ppl] step:10.0, 	loss: 0.3626089096069336, 	ppl: 1.71053946018219
[eval_20Minuten loss, ppl] step:10.0, 	loss: 2.293375253677368, 	ppl: 10.581007957458496
[eval_FOMC loss, ppl] step:10.0, 	loss: 7.7561869621276855, 	ppl: 1936.3782958984375
[eval_NumGLUEcm loss, ppl] step:10.0, 	loss: 7.350703239440918, 	ppl: 2270.860595703125
[eval_ScienceQA loss, ppl] step:10.0, 	loss: 1.7348759174346924, 	ppl: 5.953519344329834
[eval_NumGLUEds loss, ppl] step:10.0, 	loss: 8.598458290100098, 	ppl: 3777.7041015625
[eval_Py150 loss, ppl] step:10.0, 	loss: 3.556701183319092, 	ppl: 34.29323959350586
[eval_MeetingBank loss, ppl] step:10.0, 	loss: 2.472444772720337, 	ppl: 11.446906089782715
***** Evaluating perplexity, Epoch 1/5, step 11.0 *****
[eval loss, ppl] step:11.0, 	loss: 0.5952826738357544, 	ppl: 1.862070083618164
[eval_CSTANCE loss, ppl] step:11.0, 	loss: 0.34526917338371277, 	ppl: 1.618778944015503
[eval_20Minuten loss, ppl] step:11.0, 	loss: 2.2862770557403564, 	ppl: 10.495951652526855
[eval_FOMC loss, ppl] step:11.0, 	loss: 7.3436970710754395, 	ppl: 1266.59375
[eval_NumGLUEcm loss, ppl] step:11.0, 	loss: 7.207019805908203, 	ppl: 1970.998779296875
[eval_ScienceQA loss, ppl] step:11.0, 	loss: 1.729607105255127, 	ppl: 5.92778205871582
[eval_NumGLUEds loss, ppl] step:11.0, 	loss: 8.44913101196289, 	ppl: 3268.7021484375
[eval_Py150 loss, ppl] step:11.0, 	loss: 3.5224082469940186, 	ppl: 33.21328353881836
[eval_MeetingBank loss, ppl] step:11.0, 	loss: 2.4685513973236084, 	ppl: 11.38974666595459
***** Evaluating perplexity, Epoch 1/5, step 12.0 *****
[eval loss, ppl] step:12.0, 	loss: 0.5529202818870544, 	ppl: 1.7732024192810059
[eval_CSTANCE loss, ppl] step:12.0, 	loss: 0.3774595856666565, 	ppl: 1.5592678785324097
[eval_20Minuten loss, ppl] step:12.0, 	loss: 2.2805135250091553, 	ppl: 10.418594360351562
[eval_FOMC loss, ppl] step:12.0, 	loss: 7.025036334991455, 	ppl: 904.137939453125
[eval_NumGLUEcm loss, ppl] step:12.0, 	loss: 7.064265251159668, 	ppl: 1693.5633544921875
[eval_ScienceQA loss, ppl] step:12.0, 	loss: 1.7266162633895874, 	ppl: 5.905552387237549
[eval_NumGLUEds loss, ppl] step:12.0, 	loss: 8.318485260009766, 	ppl: 2908.01806640625
[eval_Py150 loss, ppl] step:12.0, 	loss: 3.4943668842315674, 	ppl: 32.109004974365234
[eval_MeetingBank loss, ppl] step:12.0, 	loss: 2.465467929840088, 	ppl: 11.338735580444336
***** Evaluating perplexity, Epoch 1/5, step 13.0 *****
[eval loss, ppl] step:13.0, 	loss: 0.533425509929657, 	ppl: 1.7281579971313477
[eval_CSTANCE loss, ppl] step:13.0, 	loss: 0.4006086587905884, 	ppl: 1.5420846939086914
[eval_20Minuten loss, ppl] step:13.0, 	loss: 2.2745094299316406, 	ppl: 10.357516288757324
[eval_FOMC loss, ppl] step:13.0, 	loss: 6.797725200653076, 	ppl: 714.1192016601562
[eval_NumGLUEcm loss, ppl] step:13.0, 	loss: 6.981822967529297, 	ppl: 1545.822509765625
[eval_ScienceQA loss, ppl] step:13.0, 	loss: 1.7235394716262817, 	ppl: 5.887875080108643
[eval_NumGLUEds loss, ppl] step:13.0, 	loss: 8.22065258026123, 	ppl: 2663.8349609375
[eval_Py150 loss, ppl] step:13.0, 	loss: 3.4712748527526855, 	ppl: 31.534465789794922
[eval_MeetingBank loss, ppl] step:13.0, 	loss: 2.461918830871582, 	ppl: 11.31157112121582
***** Evaluating perplexity, Epoch 1/5, step 14.0 *****
[eval loss, ppl] step:14.0, 	loss: 0.5245226621627808, 	ppl: 1.707080364227295
[eval_CSTANCE loss, ppl] step:14.0, 	loss: 0.4262860417366028, 	ppl: 1.5492174625396729
[eval_20Minuten loss, ppl] step:14.0, 	loss: 2.2673892974853516, 	ppl: 10.306649208068848
[eval_FOMC loss, ppl] step:14.0, 	loss: 6.648380279541016, 	ppl: 603.498779296875
[eval_NumGLUEcm loss, ppl] step:14.0, 	loss: 6.896590709686279, 	ppl: 1416.5994873046875
[eval_ScienceQA loss, ppl] step:14.0, 	loss: 1.7212393283843994, 	ppl: 5.871604919433594
[eval_NumGLUEds loss, ppl] step:14.0, 	loss: 8.130369186401367, 	ppl: 2456.814697265625
[eval_Py150 loss, ppl] step:14.0, 	loss: 3.4459338188171387, 	ppl: 30.79764747619629
[eval_MeetingBank loss, ppl] step:14.0, 	loss: 2.4604156017303467, 	ppl: 11.279312133789062
saving model to /data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001/1...
Sucessful saving model after epoch 1
Beginning of Epoch 2/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 2/5, step 0.0 *****
[eval loss, ppl] step:15.625, 	loss: 0.5522992014884949, 	ppl: 1.726928472518921
[eval_CSTANCE loss, ppl] step:15.625, 	loss: 0.5080021023750305, 	ppl: 1.6488720178604126
[eval_20Minuten loss, ppl] step:15.625, 	loss: 2.2565078735351562, 	ppl: 10.195699691772461
[eval_FOMC loss, ppl] step:15.625, 	loss: 6.279353618621826, 	ppl: 409.12835693359375
[eval_NumGLUEcm loss, ppl] step:15.625, 	loss: 6.723188400268555, 	ppl: 1195.6678466796875
[eval_ScienceQA loss, ppl] step:15.625, 	loss: 1.7159572839736938, 	ppl: 5.840964317321777
[eval_NumGLUEds loss, ppl] step:15.625, 	loss: 7.976761341094971, 	ppl: 2123.12939453125
[eval_Py150 loss, ppl] step:15.625, 	loss: 3.4136240482330322, 	ppl: 29.791400909423828
[eval_MeetingBank loss, ppl] step:15.625, 	loss: 2.454282522201538, 	ppl: 11.20007610321045
***** Evaluating perplexity, Epoch 2/5, step 1.0 *****
[eval loss, ppl] step:16.625, 	loss: 0.5672814249992371, 	ppl: 1.746074914932251
[eval_CSTANCE loss, ppl] step:16.625, 	loss: 0.5413144826889038, 	ppl: 1.7111607789993286
[eval_20Minuten loss, ppl] step:16.625, 	loss: 2.251542091369629, 	ppl: 10.145315170288086
[eval_FOMC loss, ppl] step:16.625, 	loss: 6.095104694366455, 	ppl: 343.002685546875
[eval_NumGLUEcm loss, ppl] step:16.625, 	loss: 6.631101131439209, 	ppl: 1104.898193359375
[eval_ScienceQA loss, ppl] step:16.625, 	loss: 1.7134957313537598, 	ppl: 5.826969623565674
[eval_NumGLUEds loss, ppl] step:16.625, 	loss: 7.8811936378479, 	ppl: 1960.0185546875
[eval_Py150 loss, ppl] step:16.625, 	loss: 3.3933637142181396, 	ppl: 29.151073455810547
[eval_MeetingBank loss, ppl] step:16.625, 	loss: 2.452594041824341, 	ppl: 11.16873550415039
***** Evaluating perplexity, Epoch 2/5, step 2.0 *****
[eval loss, ppl] step:17.625, 	loss: 0.5874986052513123, 	ppl: 1.7750225067138672
[eval_CSTANCE loss, ppl] step:17.625, 	loss: 0.5546166300773621, 	ppl: 1.7777438163757324
[eval_20Minuten loss, ppl] step:17.625, 	loss: 2.2506520748138428, 	ppl: 10.110519409179688
[eval_FOMC loss, ppl] step:17.625, 	loss: 5.897449016571045, 	ppl: 280.95550537109375
[eval_NumGLUEcm loss, ppl] step:17.625, 	loss: 6.561934471130371, 	ppl: 1027.436767578125
[eval_ScienceQA loss, ppl] step:17.625, 	loss: 1.7126612663269043, 	ppl: 5.814216613769531
[eval_NumGLUEds loss, ppl] step:17.625, 	loss: 7.811981678009033, 	ppl: 1828.484130859375
[eval_Py150 loss, ppl] step:17.625, 	loss: 3.3720428943634033, 	ppl: 28.653661727905273
[eval_MeetingBank loss, ppl] step:17.625, 	loss: 2.451860189437866, 	ppl: 11.154525756835938
***** Evaluating perplexity, Epoch 2/5, step 3.0 *****
[eval loss, ppl] step:18.625, 	loss: 0.5840529799461365, 	ppl: 1.7701985836029053
[eval_CSTANCE loss, ppl] step:18.625, 	loss: 0.5488693714141846, 	ppl: 1.7957885265350342
[eval_20Minuten loss, ppl] step:18.625, 	loss: 2.247173309326172, 	ppl: 10.06031608581543
[eval_FOMC loss, ppl] step:18.625, 	loss: 5.728024005889893, 	ppl: 241.99725341796875
[eval_NumGLUEcm loss, ppl] step:18.625, 	loss: 6.489416122436523, 	ppl: 946.450439453125
[eval_ScienceQA loss, ppl] step:18.625, 	loss: 1.7101413011550903, 	ppl: 5.7996039390563965
[eval_NumGLUEds loss, ppl] step:18.625, 	loss: 7.743218421936035, 	ppl: 1719.3828125
[eval_Py150 loss, ppl] step:18.625, 	loss: 3.3573083877563477, 	ppl: 28.228740692138672
[eval_MeetingBank loss, ppl] step:18.625, 	loss: 2.4494640827178955, 	ppl: 11.122251510620117
[2025-10-21 19:47:29,394] [INFO] [logging.py:107:log_dist] [Rank 0] step=20, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 19:47:29,612] [INFO] [timer.py:264:stop] epoch=0/micro_step=160/global_step=20, RunningAvgSamplesPerSec=4.93411555013126, CurrSamplesPerSec=4.841266218138998, MemAllocated=8.89GB, MaxMemAllocated=13.95GB
***** Evaluating perplexity, Epoch 2/5, step 4.0 *****
[eval loss, ppl] step:19.625, 	loss: 0.5675742626190186, 	ppl: 1.7440438270568848
[eval_CSTANCE loss, ppl] step:19.625, 	loss: 0.5342868566513062, 	ppl: 1.768294095993042
[eval_20Minuten loss, ppl] step:19.625, 	loss: 2.242687702178955, 	ppl: 10.017891883850098
[eval_FOMC loss, ppl] step:19.625, 	loss: 5.593577861785889, 	ppl: 214.01858520507812
[eval_NumGLUEcm loss, ppl] step:19.625, 	loss: 6.442360877990723, 	ppl: 900.2666015625
[eval_ScienceQA loss, ppl] step:19.625, 	loss: 1.7083836793899536, 	ppl: 5.784762859344482
[eval_NumGLUEds loss, ppl] step:19.625, 	loss: 7.683238983154297, 	ppl: 1616.71728515625
[eval_Py150 loss, ppl] step:19.625, 	loss: 3.334536552429199, 	ppl: 27.688692092895508
[eval_MeetingBank loss, ppl] step:19.625, 	loss: 2.447582960128784, 	ppl: 11.10668659210205
***** Evaluating perplexity, Epoch 2/5, step 5.0 *****
[eval loss, ppl] step:20.625, 	loss: 0.5563094019889832, 	ppl: 1.723620891571045
[eval_CSTANCE loss, ppl] step:20.625, 	loss: 0.5147180557250977, 	ppl: 1.7599742412567139
[eval_20Minuten loss, ppl] step:20.625, 	loss: 2.2383100986480713, 	ppl: 9.984282493591309
[eval_FOMC loss, ppl] step:20.625, 	loss: 5.445491313934326, 	ppl: 187.7191162109375
[eval_NumGLUEcm loss, ppl] step:20.625, 	loss: 6.37411642074585, 	ppl: 845.2854614257812
[eval_ScienceQA loss, ppl] step:20.625, 	loss: 1.7065342664718628, 	ppl: 5.774721622467041
[eval_NumGLUEds loss, ppl] step:20.625, 	loss: 7.614739418029785, 	ppl: 1520.7506103515625
[eval_Py150 loss, ppl] step:20.625, 	loss: 3.3296399116516113, 	ppl: 27.493804931640625
[eval_MeetingBank loss, ppl] step:20.625, 	loss: 2.446018695831299, 	ppl: 11.07292652130127
***** Evaluating perplexity, Epoch 2/5, step 6.0 *****
[eval loss, ppl] step:21.625, 	loss: 0.5309380292892456, 	ppl: 1.686999797821045
[eval_CSTANCE loss, ppl] step:21.625, 	loss: 0.47597894072532654, 	ppl: 1.6969033479690552
[eval_20Minuten loss, ppl] step:21.625, 	loss: 2.2374119758605957, 	ppl: 9.963054656982422
[eval_FOMC loss, ppl] step:21.625, 	loss: 5.355237007141113, 	ppl: 169.2479248046875
[eval_NumGLUEcm loss, ppl] step:21.625, 	loss: 6.329366683959961, 	ppl: 804.8911743164062
[eval_ScienceQA loss, ppl] step:21.625, 	loss: 1.7057890892028809, 	ppl: 5.769461631774902
[eval_NumGLUEds loss, ppl] step:21.625, 	loss: 7.561183452606201, 	ppl: 1444.076171875
[eval_Py150 loss, ppl] step:21.625, 	loss: 3.315325975418091, 	ppl: 27.113637924194336
[eval_MeetingBank loss, ppl] step:21.625, 	loss: 2.4442639350891113, 	ppl: 11.04659652709961
***** Evaluating perplexity, Epoch 2/5, step 7.0 *****
[eval loss, ppl] step:22.625, 	loss: 0.5081956386566162, 	ppl: 1.6548452377319336
[eval_CSTANCE loss, ppl] step:22.625, 	loss: 0.4405611753463745, 	ppl: 1.6316869258880615
[eval_20Minuten loss, ppl] step:22.625, 	loss: 2.2365949153900146, 	ppl: 9.942338943481445
[eval_FOMC loss, ppl] step:22.625, 	loss: 5.226317882537842, 	ppl: 152.56732177734375
[eval_NumGLUEcm loss, ppl] step:22.625, 	loss: 6.278994560241699, 	ppl: 764.7556762695312
[eval_ScienceQA loss, ppl] step:22.625, 	loss: 1.7040263414382935, 	ppl: 5.756558418273926
[eval_NumGLUEds loss, ppl] step:22.625, 	loss: 7.518205165863037, 	ppl: 1391.439208984375
[eval_Py150 loss, ppl] step:22.625, 	loss: 3.305664539337158, 	ppl: 26.862340927124023
[eval_MeetingBank loss, ppl] step:22.625, 	loss: 2.4422519207000732, 	ppl: 11.028427124023438
***** Evaluating perplexity, Epoch 2/5, step 8.0 *****
[eval loss, ppl] step:23.625, 	loss: 0.4887770116329193, 	ppl: 1.6312206983566284
[eval_CSTANCE loss, ppl] step:23.625, 	loss: 0.4104223847389221, 	ppl: 1.5917350053787231
[eval_20Minuten loss, ppl] step:23.625, 	loss: 2.2295429706573486, 	ppl: 9.906139373779297
[eval_FOMC loss, ppl] step:23.625, 	loss: 5.120070934295654, 	ppl: 137.470703125
[eval_NumGLUEcm loss, ppl] step:23.625, 	loss: 6.226264476776123, 	ppl: 730.82421875
[eval_ScienceQA loss, ppl] step:23.625, 	loss: 1.7022370100021362, 	ppl: 5.748957633972168
[eval_NumGLUEds loss, ppl] step:23.625, 	loss: 7.461406707763672, 	ppl: 1321.207763671875
[eval_Py150 loss, ppl] step:23.625, 	loss: 3.290879011154175, 	ppl: 26.599239349365234
[eval_MeetingBank loss, ppl] step:23.625, 	loss: 2.4424328804016113, 	ppl: 11.008759498596191
***** Evaluating perplexity, Epoch 2/5, step 9.0 *****
[eval loss, ppl] step:24.625, 	loss: 0.48145657777786255, 	ppl: 1.6247138977050781
[eval_CSTANCE loss, ppl] step:24.625, 	loss: 0.40915751457214355, 	ppl: 1.5728304386138916
[eval_20Minuten loss, ppl] step:24.625, 	loss: 2.22831654548645, 	ppl: 9.884227752685547
[eval_FOMC loss, ppl] step:24.625, 	loss: 5.027553081512451, 	ppl: 125.81307983398438
[eval_NumGLUEcm loss, ppl] step:24.625, 	loss: 6.2062530517578125, 	ppl: 709.0741577148438
[eval_ScienceQA loss, ppl] step:24.625, 	loss: 1.7005482912063599, 	ppl: 5.740237712860107
[eval_NumGLUEds loss, ppl] step:24.625, 	loss: 7.420070648193359, 	ppl: 1267.1495361328125
[eval_Py150 loss, ppl] step:24.625, 	loss: 3.2876782417297363, 	ppl: 26.402219772338867
[eval_MeetingBank loss, ppl] step:24.625, 	loss: 2.441187858581543, 	ppl: 10.999122619628906
***** Evaluating perplexity, Epoch 2/5, step 10.0 *****
[eval loss, ppl] step:25.625, 	loss: 0.477752149105072, 	ppl: 1.6260175704956055
[eval_CSTANCE loss, ppl] step:25.625, 	loss: 0.4067976176738739, 	ppl: 1.5608017444610596
[eval_20Minuten loss, ppl] step:25.625, 	loss: 2.2287464141845703, 	ppl: 9.87086009979248
[eval_FOMC loss, ppl] step:25.625, 	loss: 4.9125800132751465, 	ppl: 115.62246704101562
[eval_NumGLUEcm loss, ppl] step:25.625, 	loss: 6.151655673980713, 	ppl: 681.7380981445312
[eval_ScienceQA loss, ppl] step:25.625, 	loss: 1.7007864713668823, 	ppl: 5.732804775238037
[eval_NumGLUEds loss, ppl] step:25.625, 	loss: 7.374639511108398, 	ppl: 1235.9932861328125
[eval_Py150 loss, ppl] step:25.625, 	loss: 3.282378911972046, 	ppl: 26.195825576782227
[eval_MeetingBank loss, ppl] step:25.625, 	loss: 2.4408459663391113, 	ppl: 10.978717803955078
***** Evaluating perplexity, Epoch 2/5, step 11.0 *****
[eval loss, ppl] step:26.625, 	loss: 0.4832057058811188, 	ppl: 1.6369214057922363
[eval_CSTANCE loss, ppl] step:26.625, 	loss: 0.4220142662525177, 	ppl: 1.5598750114440918
[eval_20Minuten loss, ppl] step:26.625, 	loss: 2.2263481616973877, 	ppl: 9.851158142089844
[eval_FOMC loss, ppl] step:26.625, 	loss: 4.842530727386475, 	ppl: 107.1970443725586
[eval_NumGLUEcm loss, ppl] step:26.625, 	loss: 6.1317009925842285, 	ppl: 659.2189331054688
[eval_ScienceQA loss, ppl] step:26.625, 	loss: 1.6994956731796265, 	ppl: 5.729635238647461
[eval_NumGLUEds loss, ppl] step:26.625, 	loss: 7.348295211791992, 	ppl: 1197.27392578125
[eval_Py150 loss, ppl] step:26.625, 	loss: 3.271916151046753, 	ppl: 26.037181854248047
[eval_MeetingBank loss, ppl] step:26.625, 	loss: 2.438119649887085, 	ppl: 10.983543395996094
***** Evaluating perplexity, Epoch 2/5, step 12.0 *****
[eval loss, ppl] step:27.625, 	loss: 0.4859238564968109, 	ppl: 1.6458616256713867
[eval_CSTANCE loss, ppl] step:27.625, 	loss: 0.4414364993572235, 	ppl: 1.5487234592437744
[eval_20Minuten loss, ppl] step:27.625, 	loss: 2.225064992904663, 	ppl: 9.827266693115234
[eval_FOMC loss, ppl] step:27.625, 	loss: 4.804766654968262, 	ppl: 101.33399963378906
[eval_NumGLUEcm loss, ppl] step:27.625, 	loss: 6.121711254119873, 	ppl: 643.4808959960938
[eval_ScienceQA loss, ppl] step:27.625, 	loss: 1.697699785232544, 	ppl: 5.725543022155762
[eval_NumGLUEds loss, ppl] step:27.625, 	loss: 7.350039005279541, 	ppl: 1177.16064453125
[eval_Py150 loss, ppl] step:27.625, 	loss: 3.2672605514526367, 	ppl: 25.877464294433594
[eval_MeetingBank loss, ppl] step:27.625, 	loss: 2.439195394515991, 	ppl: 10.971598625183105
***** Evaluating perplexity, Epoch 2/5, step 13.0 *****
[eval loss, ppl] step:28.625, 	loss: 0.48939093947410583, 	ppl: 1.6561561822891235
[eval_CSTANCE loss, ppl] step:28.625, 	loss: 0.44452452659606934, 	ppl: 1.542759895324707
[eval_20Minuten loss, ppl] step:28.625, 	loss: 2.224578619003296, 	ppl: 9.828248977661133
[eval_FOMC loss, ppl] step:28.625, 	loss: 4.721608638763428, 	ppl: 95.04048919677734
[eval_NumGLUEcm loss, ppl] step:28.625, 	loss: 6.068530082702637, 	ppl: 623.1823120117188
[eval_ScienceQA loss, ppl] step:28.625, 	loss: 1.6973326206207275, 	ppl: 5.722114562988281
[eval_NumGLUEds loss, ppl] step:28.625, 	loss: 7.323957443237305, 	ppl: 1153.8330078125
[eval_Py150 loss, ppl] step:28.625, 	loss: 3.2639026641845703, 	ppl: 25.81062889099121
[eval_MeetingBank loss, ppl] step:28.625, 	loss: 2.4399640560150146, 	ppl: 10.976932525634766
[2025-10-21 19:52:25,622] [INFO] [logging.py:107:log_dist] [Rank 0] step=30, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 19:52:25,816] [INFO] [timer.py:264:stop] epoch=0/micro_step=240/global_step=30, RunningAvgSamplesPerSec=5.034105014624634, CurrSamplesPerSec=5.364779291025776, MemAllocated=9.01GB, MaxMemAllocated=13.95GB
***** Evaluating perplexity, Epoch 2/5, step 14.0 *****
[eval loss, ppl] step:29.625, 	loss: 0.48780784010887146, 	ppl: 1.6576170921325684
[eval_CSTANCE loss, ppl] step:29.625, 	loss: 0.4476971924304962, 	ppl: 1.5448908805847168
[eval_20Minuten loss, ppl] step:29.625, 	loss: 2.2245266437530518, 	ppl: 9.82307243347168
[eval_FOMC loss, ppl] step:29.625, 	loss: 4.654122829437256, 	ppl: 90.60752868652344
[eval_NumGLUEcm loss, ppl] step:29.625, 	loss: 6.067229747772217, 	ppl: 615.3916625976562
[eval_ScienceQA loss, ppl] step:29.625, 	loss: 1.6962250471115112, 	ppl: 5.718884468078613
[eval_NumGLUEds loss, ppl] step:29.625, 	loss: 7.294803619384766, 	ppl: 1133.283447265625
[eval_Py150 loss, ppl] step:29.625, 	loss: 3.255181074142456, 	ppl: 25.61147689819336
[eval_MeetingBank loss, ppl] step:29.625, 	loss: 2.4368839263916016, 	ppl: 10.955587387084961
saving model to /data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001/2...
Sucessful saving model after epoch 2
Beginning of Epoch 3/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 3/5, step 0.0 *****
[eval loss, ppl] step:31.25, 	loss: 0.4777343273162842, 	ppl: 1.6427271366119385
[eval_CSTANCE loss, ppl] step:31.25, 	loss: 0.41480523347854614, 	ppl: 1.5337841510772705
[eval_20Minuten loss, ppl] step:31.25, 	loss: 2.2219157218933105, 	ppl: 9.811006546020508
[eval_FOMC loss, ppl] step:31.25, 	loss: 4.537181377410889, 	ppl: 80.3633041381836
[eval_NumGLUEcm loss, ppl] step:31.25, 	loss: 6.0496745109558105, 	ppl: 594.2927856445312
[eval_ScienceQA loss, ppl] step:31.25, 	loss: 1.6953238248825073, 	ppl: 5.710353851318359
[eval_NumGLUEds loss, ppl] step:31.25, 	loss: 7.27223539352417, 	ppl: 1107.0347900390625
[eval_Py150 loss, ppl] step:31.25, 	loss: 3.249929904937744, 	ppl: 25.50728988647461
[eval_MeetingBank loss, ppl] step:31.25, 	loss: 2.4378066062927246, 	ppl: 10.948596954345703
***** Evaluating perplexity, Epoch 3/5, step 1.0 *****
[eval loss, ppl] step:32.25, 	loss: 0.4708128869533539, 	ppl: 1.6293160915374756
[eval_CSTANCE loss, ppl] step:32.25, 	loss: 0.3837961256504059, 	ppl: 1.5212984085083008
[eval_20Minuten loss, ppl] step:32.25, 	loss: 2.2226192951202393, 	ppl: 9.80855655670166
[eval_FOMC loss, ppl] step:32.25, 	loss: 4.444417476654053, 	ppl: 75.39122009277344
[eval_NumGLUEcm loss, ppl] step:32.25, 	loss: 6.0191450119018555, 	ppl: 581.461181640625
[eval_ScienceQA loss, ppl] step:32.25, 	loss: 1.696164608001709, 	ppl: 5.713013648986816
[eval_NumGLUEds loss, ppl] step:32.25, 	loss: 7.242761611938477, 	ppl: 1078.240478515625
[eval_Py150 loss, ppl] step:32.25, 	loss: 3.249802350997925, 	ppl: 25.37845802307129
[eval_MeetingBank loss, ppl] step:32.25, 	loss: 2.437673568725586, 	ppl: 10.948562622070312
***** Evaluating perplexity, Epoch 3/5, step 2.0 *****
[eval loss, ppl] step:33.25, 	loss: 0.4652947783470154, 	ppl: 1.6202499866485596
[eval_CSTANCE loss, ppl] step:33.25, 	loss: 0.36424174904823303, 	ppl: 1.5183415412902832
[eval_20Minuten loss, ppl] step:33.25, 	loss: 2.2212841510772705, 	ppl: 9.797022819519043
[eval_FOMC loss, ppl] step:33.25, 	loss: 4.398700714111328, 	ppl: 71.65217590332031
[eval_NumGLUEcm loss, ppl] step:33.25, 	loss: 5.989826202392578, 	ppl: 571.5057373046875
[eval_ScienceQA loss, ppl] step:33.25, 	loss: 1.6960159540176392, 	ppl: 5.705739974975586
[eval_NumGLUEds loss, ppl] step:33.25, 	loss: 7.241349220275879, 	ppl: 1079.3272705078125
[eval_Py150 loss, ppl] step:33.25, 	loss: 3.2538866996765137, 	ppl: 25.414833068847656
[eval_MeetingBank loss, ppl] step:33.25, 	loss: 2.437501907348633, 	ppl: 10.951820373535156
***** Evaluating perplexity, Epoch 3/5, step 3.0 *****
[eval loss, ppl] step:34.25, 	loss: 0.46457383036613464, 	ppl: 1.6168575286865234
[eval_CSTANCE loss, ppl] step:34.25, 	loss: 0.3361794948577881, 	ppl: 1.5283229351043701
[eval_20Minuten loss, ppl] step:34.25, 	loss: 2.2207987308502197, 	ppl: 9.79433822631836
[eval_FOMC loss, ppl] step:34.25, 	loss: 4.324485778808594, 	ppl: 67.80241394042969
[eval_NumGLUEcm loss, ppl] step:34.25, 	loss: 5.983431816101074, 	ppl: 572.0194702148438
[eval_ScienceQA loss, ppl] step:34.25, 	loss: 1.6970689296722412, 	ppl: 5.709691047668457
[eval_NumGLUEds loss, ppl] step:34.25, 	loss: 7.231734752655029, 	ppl: 1071.675537109375
[eval_Py150 loss, ppl] step:34.25, 	loss: 3.2468273639678955, 	ppl: 25.31426429748535
[eval_MeetingBank loss, ppl] step:34.25, 	loss: 2.43613338470459, 	ppl: 10.93215274810791
***** Evaluating perplexity, Epoch 3/5, step 4.0 *****
[eval loss, ppl] step:35.25, 	loss: 0.4666754901409149, 	ppl: 1.6222896575927734
[eval_CSTANCE loss, ppl] step:35.25, 	loss: 0.3092525899410248, 	ppl: 1.5489970445632935
[eval_20Minuten loss, ppl] step:35.25, 	loss: 2.221818447113037, 	ppl: 9.815892219543457
[eval_FOMC loss, ppl] step:35.25, 	loss: 4.246798038482666, 	ppl: 63.097381591796875
[eval_NumGLUEcm loss, ppl] step:35.25, 	loss: 5.974385738372803, 	ppl: 567.33056640625
[eval_ScienceQA loss, ppl] step:35.25, 	loss: 1.696002721786499, 	ppl: 5.7089643478393555
[eval_NumGLUEds loss, ppl] step:35.25, 	loss: 7.236357688903809, 	ppl: 1067.08447265625
[eval_Py150 loss, ppl] step:35.25, 	loss: 3.248403787612915, 	ppl: 25.32908058166504
[eval_MeetingBank loss, ppl] step:35.25, 	loss: 2.436418294906616, 	ppl: 10.950130462646484
***** Evaluating perplexity, Epoch 3/5, step 5.0 *****
[eval loss, ppl] step:36.25, 	loss: 0.4719458520412445, 	ppl: 1.6279664039611816
[eval_CSTANCE loss, ppl] step:36.25, 	loss: 0.3001876771450043, 	ppl: 1.5640943050384521
[eval_20Minuten loss, ppl] step:36.25, 	loss: 2.221492052078247, 	ppl: 9.79763126373291
[eval_FOMC loss, ppl] step:36.25, 	loss: 4.2242608070373535, 	ppl: 61.24784851074219
[eval_NumGLUEcm loss, ppl] step:36.25, 	loss: 5.960137844085693, 	ppl: 560.2772216796875
[eval_ScienceQA loss, ppl] step:36.25, 	loss: 1.6961380243301392, 	ppl: 5.707187175750732
[eval_NumGLUEds loss, ppl] step:36.25, 	loss: 7.227038860321045, 	ppl: 1059.56787109375
[eval_Py150 loss, ppl] step:36.25, 	loss: 3.244316816329956, 	ppl: 25.254274368286133
[eval_MeetingBank loss, ppl] step:36.25, 	loss: 2.4370994567871094, 	ppl: 10.945618629455566
***** Evaluating perplexity, Epoch 3/5, step 6.0 *****
[eval loss, ppl] step:37.25, 	loss: 0.48264843225479126, 	ppl: 1.6438722610473633
[eval_CSTANCE loss, ppl] step:37.25, 	loss: 0.2905121445655823, 	ppl: 1.5922155380249023
[eval_20Minuten loss, ppl] step:37.25, 	loss: 2.2205543518066406, 	ppl: 9.799302101135254
[eval_FOMC loss, ppl] step:37.25, 	loss: 4.178247928619385, 	ppl: 58.58647155761719
[eval_NumGLUEcm loss, ppl] step:37.25, 	loss: 5.968326568603516, 	ppl: 558.2040405273438
[eval_ScienceQA loss, ppl] step:37.25, 	loss: 1.695754885673523, 	ppl: 5.707735538482666
[eval_NumGLUEds loss, ppl] step:37.25, 	loss: 7.222517013549805, 	ppl: 1054.386962890625
[eval_Py150 loss, ppl] step:37.25, 	loss: 3.240246534347534, 	ppl: 25.24576759338379
[eval_MeetingBank loss, ppl] step:37.25, 	loss: 2.4352619647979736, 	ppl: 10.946861267089844
***** Evaluating perplexity, Epoch 3/5, step 7.0 *****
[eval loss, ppl] step:38.25, 	loss: 0.49355509877204895, 	ppl: 1.6610227823257446
[eval_CSTANCE loss, ppl] step:38.25, 	loss: 0.2857110798358917, 	ppl: 1.6212626695632935
[eval_20Minuten loss, ppl] step:38.25, 	loss: 2.2204830646514893, 	ppl: 9.792549133300781
[eval_FOMC loss, ppl] step:38.25, 	loss: 4.126031398773193, 	ppl: 56.63620376586914
[eval_NumGLUEcm loss, ppl] step:38.25, 	loss: 5.977933883666992, 	ppl: 559.232177734375
[eval_ScienceQA loss, ppl] step:38.25, 	loss: 1.6956483125686646, 	ppl: 5.709892749786377
[eval_NumGLUEds loss, ppl] step:38.25, 	loss: 7.2078633308410645, 	ppl: 1046.19970703125
[eval_Py150 loss, ppl] step:38.25, 	loss: 3.2432732582092285, 	ppl: 25.284494400024414
[eval_MeetingBank loss, ppl] step:38.25, 	loss: 2.4373817443847656, 	ppl: 10.944295883178711
[2025-10-21 19:57:13,298] [INFO] [logging.py:107:log_dist] [Rank 0] step=40, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 19:57:13,500] [INFO] [timer.py:264:stop] epoch=0/micro_step=320/global_step=40, RunningAvgSamplesPerSec=5.070067000809695, CurrSamplesPerSec=5.324989761864013, MemAllocated=8.92GB, MaxMemAllocated=13.95GB
***** Evaluating perplexity, Epoch 3/5, step 8.0 *****
[eval loss, ppl] step:39.25, 	loss: 0.49837517738342285, 	ppl: 1.6701617240905762
[eval_CSTANCE loss, ppl] step:39.25, 	loss: 0.28772708773612976, 	ppl: 1.6337730884552002
[eval_20Minuten loss, ppl] step:39.25, 	loss: 2.220613718032837, 	ppl: 9.79971694946289
[eval_FOMC loss, ppl] step:39.25, 	loss: 4.096330642700195, 	ppl: 55.5067024230957
[eval_NumGLUEcm loss, ppl] step:39.25, 	loss: 5.97364616394043, 	ppl: 555.2088012695312
[eval_ScienceQA loss, ppl] step:39.25, 	loss: 1.6952022314071655, 	ppl: 5.705699920654297
[eval_NumGLUEds loss, ppl] step:39.25, 	loss: 7.212647914886475, 	ppl: 1046.996337890625
[eval_Py150 loss, ppl] step:39.25, 	loss: 3.242159128189087, 	ppl: 25.260128021240234
[eval_MeetingBank loss, ppl] step:39.25, 	loss: 2.4369866847991943, 	ppl: 10.947589874267578
***** Evaluating perplexity, Epoch 3/5, step 9.0 *****
[eval loss, ppl] step:40.25, 	loss: 0.5002099871635437, 	ppl: 1.6715835332870483
[eval_CSTANCE loss, ppl] step:40.25, 	loss: 0.28961053490638733, 	ppl: 1.6416096687316895
[eval_20Minuten loss, ppl] step:40.25, 	loss: 2.223851203918457, 	ppl: 9.797904968261719
[eval_FOMC loss, ppl] step:40.25, 	loss: 4.089119911193848, 	ppl: 54.946502685546875
[eval_NumGLUEcm loss, ppl] step:40.25, 	loss: 5.947190284729004, 	ppl: 546.0897216796875
[eval_ScienceQA loss, ppl] step:40.25, 	loss: 1.6953128576278687, 	ppl: 5.703227996826172
[eval_NumGLUEds loss, ppl] step:40.25, 	loss: 7.199521064758301, 	ppl: 1032.7318115234375
[eval_Py150 loss, ppl] step:40.25, 	loss: 3.2416486740112305, 	ppl: 25.174945831298828
[eval_MeetingBank loss, ppl] step:40.25, 	loss: 2.4360296726226807, 	ppl: 10.934309005737305
***** Evaluating perplexity, Epoch 3/5, step 10.0 *****
[eval loss, ppl] step:41.25, 	loss: 0.495418906211853, 	ppl: 1.6648852825164795
[eval_CSTANCE loss, ppl] step:41.25, 	loss: 0.2836582660675049, 	ppl: 1.6305650472640991
[eval_20Minuten loss, ppl] step:41.25, 	loss: 2.2193338871002197, 	ppl: 9.782550811767578
[eval_FOMC loss, ppl] step:41.25, 	loss: 4.096818447113037, 	ppl: 54.611412048339844
[eval_NumGLUEcm loss, ppl] step:41.25, 	loss: 5.929227352142334, 	ppl: 542.9114990234375
[eval_ScienceQA loss, ppl] step:41.25, 	loss: 1.695154070854187, 	ppl: 5.703105449676514
[eval_NumGLUEds loss, ppl] step:41.25, 	loss: 7.189911365509033, 	ppl: 1032.98583984375
[eval_Py150 loss, ppl] step:41.25, 	loss: 3.239079713821411, 	ppl: 25.09153938293457
[eval_MeetingBank loss, ppl] step:41.25, 	loss: 2.435009479522705, 	ppl: 10.918227195739746
***** Evaluating perplexity, Epoch 3/5, step 11.0 *****
[eval loss, ppl] step:42.25, 	loss: 0.49060073494911194, 	ppl: 1.654637098312378
[eval_CSTANCE loss, ppl] step:42.25, 	loss: 0.29018473625183105, 	ppl: 1.613884449005127
[eval_20Minuten loss, ppl] step:42.25, 	loss: 2.220841646194458, 	ppl: 9.781671524047852
[eval_FOMC loss, ppl] step:42.25, 	loss: 4.117065906524658, 	ppl: 54.92131423950195
[eval_NumGLUEcm loss, ppl] step:42.25, 	loss: 5.9316887855529785, 	ppl: 528.9052734375
[eval_ScienceQA loss, ppl] step:42.25, 	loss: 1.695035457611084, 	ppl: 5.701244831085205
[eval_NumGLUEds loss, ppl] step:42.25, 	loss: 7.173962593078613, 	ppl: 1014.454345703125
[eval_Py150 loss, ppl] step:42.25, 	loss: 3.2332029342651367, 	ppl: 25.001291275024414
[eval_MeetingBank loss, ppl] step:42.25, 	loss: 2.435568332672119, 	ppl: 10.925739288330078
***** Evaluating perplexity, Epoch 3/5, step 12.0 *****
[eval loss, ppl] step:43.25, 	loss: 0.48051682114601135, 	ppl: 1.6375987529754639
[eval_CSTANCE loss, ppl] step:43.25, 	loss: 0.30014339089393616, 	ppl: 1.5911535024642944
[eval_20Minuten loss, ppl] step:43.25, 	loss: 2.2202587127685547, 	ppl: 9.775123596191406
[eval_FOMC loss, ppl] step:43.25, 	loss: 4.112817287445068, 	ppl: 54.875736236572266
[eval_NumGLUEcm loss, ppl] step:43.25, 	loss: 5.930357456207275, 	ppl: 534.692138671875
[eval_ScienceQA loss, ppl] step:43.25, 	loss: 1.695022702217102, 	ppl: 5.695868968963623
[eval_NumGLUEds loss, ppl] step:43.25, 	loss: 7.171680450439453, 	ppl: 1014.5111083984375
[eval_Py150 loss, ppl] step:43.25, 	loss: 3.2311182022094727, 	ppl: 25.02303695678711
[eval_MeetingBank loss, ppl] step:43.25, 	loss: 2.4338226318359375, 	ppl: 10.914268493652344
***** Evaluating perplexity, Epoch 3/5, step 13.0 *****
[eval loss, ppl] step:44.25, 	loss: 0.47288256883621216, 	ppl: 1.6254081726074219
[eval_CSTANCE loss, ppl] step:44.25, 	loss: 0.3065371811389923, 	ppl: 1.5737793445587158
[eval_20Minuten loss, ppl] step:44.25, 	loss: 2.2177743911743164, 	ppl: 9.76089096069336
[eval_FOMC loss, ppl] step:44.25, 	loss: 4.12880802154541, 	ppl: 55.53378677368164
[eval_NumGLUEcm loss, ppl] step:44.25, 	loss: 5.9097208976745605, 	ppl: 519.4183349609375
[eval_ScienceQA loss, ppl] step:44.25, 	loss: 1.6942229270935059, 	ppl: 5.6907782554626465
[eval_NumGLUEds loss, ppl] step:44.25, 	loss: 7.181457996368408, 	ppl: 1017.0106201171875
[eval_Py150 loss, ppl] step:44.25, 	loss: 3.230741262435913, 	ppl: 24.922134399414062
[eval_MeetingBank loss, ppl] step:44.25, 	loss: 2.4347407817840576, 	ppl: 10.91784954071045
***** Evaluating perplexity, Epoch 3/5, step 14.0 *****
[eval loss, ppl] step:45.25, 	loss: 0.4635359048843384, 	ppl: 1.6133841276168823
[eval_CSTANCE loss, ppl] step:45.25, 	loss: 0.32140517234802246, 	ppl: 1.5610231161117554
[eval_20Minuten loss, ppl] step:45.25, 	loss: 2.2168214321136475, 	ppl: 9.746678352355957
[eval_FOMC loss, ppl] step:45.25, 	loss: 4.122848033905029, 	ppl: 55.96104431152344
[eval_NumGLUEcm loss, ppl] step:45.25, 	loss: 5.908921718597412, 	ppl: 515.670166015625
[eval_ScienceQA loss, ppl] step:45.25, 	loss: 1.6932356357574463, 	ppl: 5.686379909515381
[eval_NumGLUEds loss, ppl] step:45.25, 	loss: 7.162814617156982, 	ppl: 998.065185546875
[eval_Py150 loss, ppl] step:45.25, 	loss: 3.228508710861206, 	ppl: 24.831172943115234
[eval_MeetingBank loss, ppl] step:45.25, 	loss: 2.433948278427124, 	ppl: 10.90611743927002
saving model to /data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001/3...
Sucessful saving model after epoch 3
Beginning of Epoch 4/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 4/5, step 0.0 *****
[eval loss, ppl] step:46.875, 	loss: 0.45935216546058655, 	ppl: 1.6061203479766846
[eval_CSTANCE loss, ppl] step:46.875, 	loss: 0.32994237542152405, 	ppl: 1.5431019067764282
[eval_20Minuten loss, ppl] step:46.875, 	loss: 2.2162113189697266, 	ppl: 9.74541187286377
[eval_FOMC loss, ppl] step:46.875, 	loss: 4.159868240356445, 	ppl: 57.681785583496094
[eval_NumGLUEcm loss, ppl] step:46.875, 	loss: 5.896796703338623, 	ppl: 513.3840942382812
[eval_ScienceQA loss, ppl] step:46.875, 	loss: 1.6917200088500977, 	ppl: 5.6802568435668945
[eval_NumGLUEds loss, ppl] step:46.875, 	loss: 7.163933753967285, 	ppl: 992.749267578125
[eval_Py150 loss, ppl] step:46.875, 	loss: 3.2352888584136963, 	ppl: 24.91054916381836
[eval_MeetingBank loss, ppl] step:46.875, 	loss: 2.434981107711792, 	ppl: 10.904638290405273
***** Evaluating perplexity, Epoch 4/5, step 1.0 *****
[eval loss, ppl] step:47.875, 	loss: 0.4590996503829956, 	ppl: 1.60474693775177
[eval_CSTANCE loss, ppl] step:47.875, 	loss: 0.33970212936401367, 	ppl: 1.5327603816986084
[eval_20Minuten loss, ppl] step:47.875, 	loss: 2.217322826385498, 	ppl: 9.7432861328125
[eval_FOMC loss, ppl] step:47.875, 	loss: 4.179872989654541, 	ppl: 58.43828582763672
[eval_NumGLUEcm loss, ppl] step:47.875, 	loss: 5.884008407592773, 	ppl: 513.546630859375
[eval_ScienceQA loss, ppl] step:47.875, 	loss: 1.6923667192459106, 	ppl: 5.680914402008057
[eval_NumGLUEds loss, ppl] step:47.875, 	loss: 7.153578281402588, 	ppl: 989.371826171875
[eval_Py150 loss, ppl] step:47.875, 	loss: 3.2232537269592285, 	ppl: 24.797651290893555
[eval_MeetingBank loss, ppl] step:47.875, 	loss: 2.434931993484497, 	ppl: 10.90720272064209
***** Evaluating perplexity, Epoch 4/5, step 2.0 *****
[eval loss, ppl] step:48.875, 	loss: 0.45751649141311646, 	ppl: 1.603123664855957
[eval_CSTANCE loss, ppl] step:48.875, 	loss: 0.35247379541397095, 	ppl: 1.539182424545288
[eval_20Minuten loss, ppl] step:48.875, 	loss: 2.2163922786712646, 	ppl: 9.735076904296875
[eval_FOMC loss, ppl] step:48.875, 	loss: 4.2269110679626465, 	ppl: 60.54638671875
[eval_NumGLUEcm loss, ppl] step:48.875, 	loss: 5.881707191467285, 	ppl: 507.96441650390625
[eval_ScienceQA loss, ppl] step:48.875, 	loss: 1.69222092628479, 	ppl: 5.679802894592285
[eval_NumGLUEds loss, ppl] step:48.875, 	loss: 7.148630619049072, 	ppl: 985.2899169921875
[eval_Py150 loss, ppl] step:48.875, 	loss: 3.2273740768432617, 	ppl: 24.75145149230957
[eval_MeetingBank loss, ppl] step:48.875, 	loss: 2.433108329772949, 	ppl: 10.893861770629883
[2025-10-21 20:02:21,764] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 20:02:21,975] [INFO] [timer.py:264:stop] epoch=0/micro_step=400/global_step=50, RunningAvgSamplesPerSec=5.066046576900403, CurrSamplesPerSec=4.986332492607088, MemAllocated=8.9GB, MaxMemAllocated=13.95GB
***** Evaluating perplexity, Epoch 4/5, step 3.0 *****
[eval loss, ppl] step:49.875, 	loss: 0.45524755120277405, 	ppl: 1.6023454666137695
[eval_CSTANCE loss, ppl] step:49.875, 	loss: 0.3648088574409485, 	ppl: 1.5223720073699951
[eval_20Minuten loss, ppl] step:49.875, 	loss: 2.2160847187042236, 	ppl: 9.734243392944336
[eval_FOMC loss, ppl] step:49.875, 	loss: 4.2430195808410645, 	ppl: 61.69339370727539
[eval_NumGLUEcm loss, ppl] step:49.875, 	loss: 5.88241720199585, 	ppl: 509.6386413574219
[eval_ScienceQA loss, ppl] step:49.875, 	loss: 1.6928175687789917, 	ppl: 5.682095527648926
[eval_NumGLUEds loss, ppl] step:49.875, 	loss: 7.145565032958984, 	ppl: 989.7427978515625
[eval_Py150 loss, ppl] step:49.875, 	loss: 3.2230324745178223, 	ppl: 24.773447036743164
[eval_MeetingBank loss, ppl] step:49.875, 	loss: 2.4316463470458984, 	ppl: 10.888676643371582
***** Evaluating perplexity, Epoch 4/5, step 4.0 *****
[eval loss, ppl] step:50.875, 	loss: 0.45532944798469543, 	ppl: 1.6046593189239502
[eval_CSTANCE loss, ppl] step:50.875, 	loss: 0.36322546005249023, 	ppl: 1.5185755491256714
[eval_20Minuten loss, ppl] step:50.875, 	loss: 2.2154362201690674, 	ppl: 9.739026069641113
[eval_FOMC loss, ppl] step:50.875, 	loss: 4.2554216384887695, 	ppl: 62.52262878417969
[eval_NumGLUEcm loss, ppl] step:50.875, 	loss: 5.894210338592529, 	ppl: 512.9103393554688
[eval_ScienceQA loss, ppl] step:50.875, 	loss: 1.6928256750106812, 	ppl: 5.681210994720459
[eval_NumGLUEds loss, ppl] step:50.875, 	loss: 7.177987098693848, 	ppl: 999.732177734375
[eval_Py150 loss, ppl] step:50.875, 	loss: 3.2251908779144287, 	ppl: 24.750974655151367
[eval_MeetingBank loss, ppl] step:50.875, 	loss: 2.433176040649414, 	ppl: 10.890997886657715
***** Evaluating perplexity, Epoch 4/5, step 5.0 *****
[eval loss, ppl] step:51.875, 	loss: 0.4546259641647339, 	ppl: 1.6022471189498901
[eval_CSTANCE loss, ppl] step:51.875, 	loss: 0.3667606711387634, 	ppl: 1.5189814567565918
[eval_20Minuten loss, ppl] step:51.875, 	loss: 2.2163641452789307, 	ppl: 9.751458168029785
[eval_FOMC loss, ppl] step:51.875, 	loss: 4.250400543212891, 	ppl: 62.72734069824219
[eval_NumGLUEcm loss, ppl] step:51.875, 	loss: 5.883549213409424, 	ppl: 506.1039733886719
[eval_ScienceQA loss, ppl] step:51.875, 	loss: 1.6925383806228638, 	ppl: 5.680122375488281
[eval_NumGLUEds loss, ppl] step:51.875, 	loss: 7.153183937072754, 	ppl: 986.737548828125
[eval_Py150 loss, ppl] step:51.875, 	loss: 3.222200632095337, 	ppl: 24.75128746032715
[eval_MeetingBank loss, ppl] step:51.875, 	loss: 2.434575319290161, 	ppl: 10.902151107788086
***** Evaluating perplexity, Epoch 4/5, step 6.0 *****
[eval loss, ppl] step:52.875, 	loss: 0.4514690041542053, 	ppl: 1.597707748413086
[eval_CSTANCE loss, ppl] step:52.875, 	loss: 0.36926379799842834, 	ppl: 1.516040563583374
[eval_20Minuten loss, ppl] step:52.875, 	loss: 2.2154362201690674, 	ppl: 9.736600875854492
[eval_FOMC loss, ppl] step:52.875, 	loss: 4.288354396820068, 	ppl: 63.81404495239258
[eval_NumGLUEcm loss, ppl] step:52.875, 	loss: 5.882598400115967, 	ppl: 512.9453735351562
[eval_ScienceQA loss, ppl] step:52.875, 	loss: 1.6929396390914917, 	ppl: 5.679928779602051
[eval_NumGLUEds loss, ppl] step:52.875, 	loss: 7.142775535583496, 	ppl: 979.9813842773438
[eval_Py150 loss, ppl] step:52.875, 	loss: 3.22585129737854, 	ppl: 24.76776123046875
[eval_MeetingBank loss, ppl] step:52.875, 	loss: 2.4333088397979736, 	ppl: 10.896360397338867
***** Evaluating perplexity, Epoch 4/5, step 7.0 *****
[eval loss, ppl] step:53.875, 	loss: 0.4494967758655548, 	ppl: 1.5917770862579346
[eval_CSTANCE loss, ppl] step:53.875, 	loss: 0.36629465222358704, 	ppl: 1.5088300704956055
[eval_20Minuten loss, ppl] step:53.875, 	loss: 2.216057538986206, 	ppl: 9.743497848510742
[eval_FOMC loss, ppl] step:53.875, 	loss: 4.308190822601318, 	ppl: 64.59607696533203
[eval_NumGLUEcm loss, ppl] step:53.875, 	loss: 5.8873090744018555, 	ppl: 511.4993896484375
[eval_ScienceQA loss, ppl] step:53.875, 	loss: 1.692293405532837, 	ppl: 5.679256439208984
[eval_NumGLUEds loss, ppl] step:53.875, 	loss: 7.164211273193359, 	ppl: 993.730712890625
[eval_Py150 loss, ppl] step:53.875, 	loss: 3.229792594909668, 	ppl: 24.79607391357422
[eval_MeetingBank loss, ppl] step:53.875, 	loss: 2.4319634437561035, 	ppl: 10.889626502990723
***** Evaluating perplexity, Epoch 4/5, step 8.0 *****
[eval loss, ppl] step:54.875, 	loss: 0.4439281225204468, 	ppl: 1.5834661722183228
[eval_CSTANCE loss, ppl] step:54.875, 	loss: 0.3649716079235077, 	ppl: 1.5013597011566162
[eval_20Minuten loss, ppl] step:54.875, 	loss: 2.2124481201171875, 	ppl: 9.733969688415527
[eval_FOMC loss, ppl] step:54.875, 	loss: 4.326197147369385, 	ppl: 65.10906219482422
[eval_NumGLUEcm loss, ppl] step:54.875, 	loss: 5.894599914550781, 	ppl: 507.05096435546875
[eval_ScienceQA loss, ppl] step:54.875, 	loss: 1.6925461292266846, 	ppl: 5.681475639343262
[eval_NumGLUEds loss, ppl] step:54.875, 	loss: 7.162049293518066, 	ppl: 993.2943115234375
[eval_Py150 loss, ppl] step:54.875, 	loss: 3.221414566040039, 	ppl: 24.73499870300293
[eval_MeetingBank loss, ppl] step:54.875, 	loss: 2.4335086345672607, 	ppl: 10.883164405822754
***** Evaluating perplexity, Epoch 4/5, step 9.0 *****
[eval loss, ppl] step:55.875, 	loss: 0.4395332634449005, 	ppl: 1.577488899230957
[eval_CSTANCE loss, ppl] step:55.875, 	loss: 0.36482754349708557, 	ppl: 1.4988491535186768
[eval_20Minuten loss, ppl] step:55.875, 	loss: 2.215536117553711, 	ppl: 9.738075256347656
[eval_FOMC loss, ppl] step:55.875, 	loss: 4.325221061706543, 	ppl: 65.00621032714844
[eval_NumGLUEcm loss, ppl] step:55.875, 	loss: 5.894453525543213, 	ppl: 510.6397705078125
[eval_ScienceQA loss, ppl] step:55.875, 	loss: 1.6925039291381836, 	ppl: 5.68540096282959
[eval_NumGLUEds loss, ppl] step:55.875, 	loss: 7.159984111785889, 	ppl: 998.552001953125
[eval_Py150 loss, ppl] step:55.875, 	loss: 3.2254278659820557, 	ppl: 24.793981552124023
[eval_MeetingBank loss, ppl] step:55.875, 	loss: 2.4326012134552, 	ppl: 10.894887924194336
***** Evaluating perplexity, Epoch 4/5, step 10.0 *****
[eval loss, ppl] step:56.875, 	loss: 0.4356285035610199, 	ppl: 1.5703763961791992
[eval_CSTANCE loss, ppl] step:56.875, 	loss: 0.37129640579223633, 	ppl: 1.4862239360809326
[eval_20Minuten loss, ppl] step:56.875, 	loss: 2.215808391571045, 	ppl: 9.746376037597656
[eval_FOMC loss, ppl] step:56.875, 	loss: 4.380704402923584, 	ppl: 67.22901916503906
[eval_NumGLUEcm loss, ppl] step:56.875, 	loss: 5.896827697753906, 	ppl: 516.2755737304688
[eval_ScienceQA loss, ppl] step:56.875, 	loss: 1.6925594806671143, 	ppl: 5.684889793395996
[eval_NumGLUEds loss, ppl] step:56.875, 	loss: 7.161104202270508, 	ppl: 1002.9699096679688
[eval_Py150 loss, ppl] step:56.875, 	loss: 3.2292394638061523, 	ppl: 24.858135223388672
[eval_MeetingBank loss, ppl] step:56.875, 	loss: 2.4328725337982178, 	ppl: 10.895206451416016
***** Evaluating perplexity, Epoch 4/5, step 11.0 *****
[eval loss, ppl] step:57.875, 	loss: 0.43262168765068054, 	ppl: 1.5646439790725708
[eval_CSTANCE loss, ppl] step:57.875, 	loss: 0.3735198676586151, 	ppl: 1.4823658466339111
[eval_20Minuten loss, ppl] step:57.875, 	loss: 2.217097759246826, 	ppl: 9.750316619873047
[eval_FOMC loss, ppl] step:57.875, 	loss: 4.380105495452881, 	ppl: 67.58305358886719
[eval_NumGLUEcm loss, ppl] step:57.875, 	loss: 5.898699760437012, 	ppl: 513.8226318359375
[eval_ScienceQA loss, ppl] step:57.875, 	loss: 1.6932191848754883, 	ppl: 5.686805725097656
[eval_NumGLUEds loss, ppl] step:57.875, 	loss: 7.175381660461426, 	ppl: 1009.5662841796875
[eval_Py150 loss, ppl] step:57.875, 	loss: 3.231045961380005, 	ppl: 24.89167594909668
[eval_MeetingBank loss, ppl] step:57.875, 	loss: 2.4314792156219482, 	ppl: 10.89015007019043
***** Evaluating perplexity, Epoch 4/5, step 12.0 *****
[eval loss, ppl] step:58.875, 	loss: 0.43039485812187195, 	ppl: 1.5613902807235718
[eval_CSTANCE loss, ppl] step:58.875, 	loss: 0.369134783744812, 	ppl: 1.4889605045318604
[eval_20Minuten loss, ppl] step:58.875, 	loss: 2.2174079418182373, 	ppl: 9.757076263427734
[eval_FOMC loss, ppl] step:58.875, 	loss: 4.42090368270874, 	ppl: 68.90414428710938
[eval_NumGLUEcm loss, ppl] step:58.875, 	loss: 5.9181437492370605, 	ppl: 520.900146484375
[eval_ScienceQA loss, ppl] step:58.875, 	loss: 1.6946502923965454, 	ppl: 5.690248489379883
[eval_NumGLUEds loss, ppl] step:58.875, 	loss: 7.192651748657227, 	ppl: 1030.565673828125
[eval_Py150 loss, ppl] step:58.875, 	loss: 3.233550786972046, 	ppl: 24.99658203125
[eval_MeetingBank loss, ppl] step:58.875, 	loss: 2.434558391571045, 	ppl: 10.906265258789062
[2025-10-21 20:07:17,767] [INFO] [logging.py:107:log_dist] [Rank 0] step=60, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 20:07:17,960] [INFO] [timer.py:264:stop] epoch=0/micro_step=480/global_step=60, RunningAvgSamplesPerSec=5.105827082409651, CurrSamplesPerSec=5.3432556460448435, MemAllocated=8.92GB, MaxMemAllocated=13.95GB
***** Evaluating perplexity, Epoch 4/5, step 13.0 *****
[eval loss, ppl] step:59.875, 	loss: 0.43354523181915283, 	ppl: 1.5627976655960083
[eval_CSTANCE loss, ppl] step:59.875, 	loss: 0.3587450683116913, 	ppl: 1.491011142730713
[eval_20Minuten loss, ppl] step:59.875, 	loss: 2.2178292274475098, 	ppl: 9.75501823425293
[eval_FOMC loss, ppl] step:59.875, 	loss: 4.4712347984313965, 	ppl: 70.19631958007812
[eval_NumGLUEcm loss, ppl] step:59.875, 	loss: 5.925785541534424, 	ppl: 528.0094604492188
[eval_ScienceQA loss, ppl] step:59.875, 	loss: 1.693456768989563, 	ppl: 5.693853378295898
[eval_NumGLUEds loss, ppl] step:59.875, 	loss: 7.186249732971191, 	ppl: 1031.8243408203125
[eval_Py150 loss, ppl] step:59.875, 	loss: 3.232675790786743, 	ppl: 25.009647369384766
[eval_MeetingBank loss, ppl] step:59.875, 	loss: 2.4337944984436035, 	ppl: 10.906549453735352
***** Evaluating perplexity, Epoch 4/5, step 14.0 *****
[eval loss, ppl] step:60.875, 	loss: 0.4406408369541168, 	ppl: 1.5708389282226562
[eval_CSTANCE loss, ppl] step:60.875, 	loss: 0.36394548416137695, 	ppl: 1.522933840751648
[eval_20Minuten loss, ppl] step:60.875, 	loss: 2.219050168991089, 	ppl: 9.76353931427002
[eval_FOMC loss, ppl] step:60.875, 	loss: 4.438566207885742, 	ppl: 69.01349639892578
[eval_NumGLUEcm loss, ppl] step:60.875, 	loss: 5.932507038116455, 	ppl: 536.8585815429688
[eval_ScienceQA loss, ppl] step:60.875, 	loss: 1.6952297687530518, 	ppl: 5.694064140319824
[eval_NumGLUEds loss, ppl] step:60.875, 	loss: 7.205002307891846, 	ppl: 1038.5606689453125
[eval_Py150 loss, ppl] step:60.875, 	loss: 3.2362301349639893, 	ppl: 25.056499481201172
[eval_MeetingBank loss, ppl] step:60.875, 	loss: 2.4344699382781982, 	ppl: 10.915010452270508
saving model to /data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001/4...
Sucessful saving model after epoch 4
Beginning of Epoch 5/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 5/5, step 0.0 *****
[eval loss, ppl] step:62.5, 	loss: 0.4536726772785187, 	ppl: 1.5892068147659302
[eval_CSTANCE loss, ppl] step:62.5, 	loss: 0.3644725978374481, 	ppl: 1.5462065935134888
[eval_20Minuten loss, ppl] step:62.5, 	loss: 2.218813419342041, 	ppl: 9.769364356994629
[eval_FOMC loss, ppl] step:62.5, 	loss: 4.449796199798584, 	ppl: 69.57337188720703
[eval_NumGLUEcm loss, ppl] step:62.5, 	loss: 5.946340560913086, 	ppl: 532.9310913085938
[eval_ScienceQA loss, ppl] step:62.5, 	loss: 1.6954046487808228, 	ppl: 5.6983819007873535
[eval_NumGLUEds loss, ppl] step:62.5, 	loss: 7.211242198944092, 	ppl: 1050.24365234375
[eval_Py150 loss, ppl] step:62.5, 	loss: 3.2458977699279785, 	ppl: 25.229291915893555
[eval_MeetingBank loss, ppl] step:62.5, 	loss: 2.435908555984497, 	ppl: 10.910116195678711
***** Evaluating perplexity, Epoch 5/5, step 1.0 *****
[eval loss, ppl] step:63.5, 	loss: 0.4565697908401489, 	ppl: 1.59337317943573
[eval_CSTANCE loss, ppl] step:63.5, 	loss: 0.3686302900314331, 	ppl: 1.5517868995666504
[eval_20Minuten loss, ppl] step:63.5, 	loss: 2.2193398475646973, 	ppl: 9.772162437438965
[eval_FOMC loss, ppl] step:63.5, 	loss: 4.49653959274292, 	ppl: 70.88050079345703
[eval_NumGLUEcm loss, ppl] step:63.5, 	loss: 5.94284200668335, 	ppl: 544.301513671875
[eval_ScienceQA loss, ppl] step:63.5, 	loss: 1.6965563297271729, 	ppl: 5.705077648162842
[eval_NumGLUEds loss, ppl] step:63.5, 	loss: 7.216678619384766, 	ppl: 1060.2275390625
[eval_Py150 loss, ppl] step:63.5, 	loss: 3.246371269226074, 	ppl: 25.2454891204834
[eval_MeetingBank loss, ppl] step:63.5, 	loss: 2.436025619506836, 	ppl: 10.924565315246582
***** Evaluating perplexity, Epoch 5/5, step 2.0 *****
[eval loss, ppl] step:64.5, 	loss: 0.4564427435398102, 	ppl: 1.594269037246704
[eval_CSTANCE loss, ppl] step:64.5, 	loss: 0.36030763387680054, 	ppl: 1.5493286848068237
[eval_20Minuten loss, ppl] step:64.5, 	loss: 2.2222800254821777, 	ppl: 9.791340827941895
[eval_FOMC loss, ppl] step:64.5, 	loss: 4.488560199737549, 	ppl: 70.93585205078125
[eval_NumGLUEcm loss, ppl] step:64.5, 	loss: 5.957181453704834, 	ppl: 543.0406494140625
[eval_ScienceQA loss, ppl] step:64.5, 	loss: 1.6963423490524292, 	ppl: 5.7036614418029785
[eval_NumGLUEds loss, ppl] step:64.5, 	loss: 7.2358598709106445, 	ppl: 1069.9129638671875
[eval_Py150 loss, ppl] step:64.5, 	loss: 3.2450404167175293, 	ppl: 25.2362060546875
[eval_MeetingBank loss, ppl] step:64.5, 	loss: 2.436201333999634, 	ppl: 10.934672355651855
***** Evaluating perplexity, Epoch 5/5, step 3.0 *****
[eval loss, ppl] step:65.5, 	loss: 0.45331528782844543, 	ppl: 1.5891269445419312
[eval_CSTANCE loss, ppl] step:65.5, 	loss: 0.3647423982620239, 	ppl: 1.544913649559021
[eval_20Minuten loss, ppl] step:65.5, 	loss: 2.2198891639709473, 	ppl: 9.785562515258789
[eval_FOMC loss, ppl] step:65.5, 	loss: 4.54491662979126, 	ppl: 73.23612213134766
[eval_NumGLUEcm loss, ppl] step:65.5, 	loss: 5.9531707763671875, 	ppl: 551.0784912109375
[eval_ScienceQA loss, ppl] step:65.5, 	loss: 1.696772813796997, 	ppl: 5.706071853637695
[eval_NumGLUEds loss, ppl] step:65.5, 	loss: 7.23109769821167, 	ppl: 1071.128173828125
[eval_Py150 loss, ppl] step:65.5, 	loss: 3.250394582748413, 	ppl: 25.320446014404297
[eval_MeetingBank loss, ppl] step:65.5, 	loss: 2.4358341693878174, 	ppl: 10.923243522644043
***** Evaluating perplexity, Epoch 5/5, step 4.0 *****
[eval loss, ppl] step:66.5, 	loss: 0.44734281301498413, 	ppl: 1.5818628072738647
[eval_CSTANCE loss, ppl] step:66.5, 	loss: 0.36778709292411804, 	ppl: 1.5357366800308228
[eval_20Minuten loss, ppl] step:66.5, 	loss: 2.221006155014038, 	ppl: 9.7941312789917
[eval_FOMC loss, ppl] step:66.5, 	loss: 4.5556321144104, 	ppl: 73.64521026611328
[eval_NumGLUEcm loss, ppl] step:66.5, 	loss: 5.949265956878662, 	ppl: 544.0160522460938
[eval_ScienceQA loss, ppl] step:66.5, 	loss: 1.6970566511154175, 	ppl: 5.710607051849365
[eval_NumGLUEds loss, ppl] step:66.5, 	loss: 7.238364219665527, 	ppl: 1083.041259765625
[eval_Py150 loss, ppl] step:66.5, 	loss: 3.2475576400756836, 	ppl: 25.26921844482422
[eval_MeetingBank loss, ppl] step:66.5, 	loss: 2.436209201812744, 	ppl: 10.93156623840332
***** Evaluating perplexity, Epoch 5/5, step 5.0 *****
[eval loss, ppl] step:67.5, 	loss: 0.44120100140571594, 	ppl: 1.5733747482299805
[eval_CSTANCE loss, ppl] step:67.5, 	loss: 0.36528968811035156, 	ppl: 1.5200178623199463
[eval_20Minuten loss, ppl] step:67.5, 	loss: 2.2222163677215576, 	ppl: 9.80154800415039
[eval_FOMC loss, ppl] step:67.5, 	loss: 4.521738529205322, 	ppl: 73.06871795654297
[eval_NumGLUEcm loss, ppl] step:67.5, 	loss: 5.9624857902526855, 	ppl: 543.5232543945312
[eval_ScienceQA loss, ppl] step:67.5, 	loss: 1.6965305805206299, 	ppl: 5.710008144378662
[eval_NumGLUEds loss, ppl] step:67.5, 	loss: 7.231118679046631, 	ppl: 1076.5506591796875
[eval_Py150 loss, ppl] step:67.5, 	loss: 3.251296043395996, 	ppl: 25.285831451416016
[eval_MeetingBank loss, ppl] step:67.5, 	loss: 2.4359066486358643, 	ppl: 10.925301551818848
***** Evaluating perplexity, Epoch 5/5, step 6.0 *****
[eval loss, ppl] step:68.5, 	loss: 0.43113386631011963, 	ppl: 1.5641279220581055
[eval_CSTANCE loss, ppl] step:68.5, 	loss: 0.35429126024246216, 	ppl: 1.5058735609054565
[eval_20Minuten loss, ppl] step:68.5, 	loss: 2.221646308898926, 	ppl: 9.80301284790039
[eval_FOMC loss, ppl] step:68.5, 	loss: 4.553167343139648, 	ppl: 75.07991027832031
[eval_NumGLUEcm loss, ppl] step:68.5, 	loss: 5.967695236206055, 	ppl: 549.2609252929688
[eval_ScienceQA loss, ppl] step:68.5, 	loss: 1.6977201700210571, 	ppl: 5.711889266967773
[eval_NumGLUEds loss, ppl] step:68.5, 	loss: 7.233255386352539, 	ppl: 1080.23828125
[eval_Py150 loss, ppl] step:68.5, 	loss: 3.248866558074951, 	ppl: 25.309316635131836
[eval_MeetingBank loss, ppl] step:68.5, 	loss: 2.437883138656616, 	ppl: 10.937760353088379
[2025-10-21 20:12:06,471] [INFO] [logging.py:107:log_dist] [Rank 0] step=70, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-10-21 20:12:06,677] [INFO] [timer.py:264:stop] epoch=0/micro_step=560/global_step=70, RunningAvgSamplesPerSec=5.123229955970255, CurrSamplesPerSec=5.162888683912504, MemAllocated=9.04GB, MaxMemAllocated=13.95GB
***** Evaluating perplexity, Epoch 5/5, step 7.0 *****
[eval loss, ppl] step:69.5, 	loss: 0.42746782302856445, 	ppl: 1.5592188835144043
[eval_CSTANCE loss, ppl] step:69.5, 	loss: 0.3377596437931061, 	ppl: 1.4961146116256714
[eval_20Minuten loss, ppl] step:69.5, 	loss: 2.220193386077881, 	ppl: 9.797758102416992
[eval_FOMC loss, ppl] step:69.5, 	loss: 4.54857063293457, 	ppl: 75.37199401855469
[eval_NumGLUEcm loss, ppl] step:69.5, 	loss: 5.952534198760986, 	ppl: 547.3723754882812
[eval_ScienceQA loss, ppl] step:69.5, 	loss: 1.6982073783874512, 	ppl: 5.715094089508057
[eval_NumGLUEds loss, ppl] step:69.5, 	loss: 7.257318496704102, 	ppl: 1092.7506103515625
[eval_Py150 loss, ppl] step:69.5, 	loss: 3.2460153102874756, 	ppl: 25.31195068359375
[eval_MeetingBank loss, ppl] step:69.5, 	loss: 2.4360766410827637, 	ppl: 10.932417869567871
***** Evaluating perplexity, Epoch 5/5, step 8.0 *****
[eval loss, ppl] step:70.5, 	loss: 0.4227367043495178, 	ppl: 1.5569136142730713
[eval_CSTANCE loss, ppl] step:70.5, 	loss: 0.33259448409080505, 	ppl: 1.487468957901001
[eval_20Minuten loss, ppl] step:70.5, 	loss: 2.2231037616729736, 	ppl: 9.820062637329102
[eval_FOMC loss, ppl] step:70.5, 	loss: 4.560179710388184, 	ppl: 75.73896789550781
[eval_NumGLUEcm loss, ppl] step:70.5, 	loss: 5.96991491317749, 	ppl: 554.0824584960938
[eval_ScienceQA loss, ppl] step:70.5, 	loss: 1.6979233026504517, 	ppl: 5.716013431549072
[eval_NumGLUEds loss, ppl] step:70.5, 	loss: 7.228618144989014, 	ppl: 1075.896240234375
[eval_Py150 loss, ppl] step:70.5, 	loss: 3.248659133911133, 	ppl: 25.366180419921875
[eval_MeetingBank loss, ppl] step:70.5, 	loss: 2.437530994415283, 	ppl: 10.930319786071777
***** Evaluating perplexity, Epoch 5/5, step 9.0 *****
[eval loss, ppl] step:71.5, 	loss: 0.42171502113342285, 	ppl: 1.5544099807739258
[eval_CSTANCE loss, ppl] step:71.5, 	loss: 0.333951473236084, 	ppl: 1.482053279876709
[eval_20Minuten loss, ppl] step:71.5, 	loss: 2.2241876125335693, 	ppl: 9.82628059387207
[eval_FOMC loss, ppl] step:71.5, 	loss: 4.577962398529053, 	ppl: 77.55757141113281
[eval_NumGLUEcm loss, ppl] step:71.5, 	loss: 5.968155860900879, 	ppl: 551.5151977539062
[eval_ScienceQA loss, ppl] step:71.5, 	loss: 1.698154091835022, 	ppl: 5.718653678894043
[eval_NumGLUEds loss, ppl] step:71.5, 	loss: 7.248205661773682, 	ppl: 1088.3660888671875
[eval_Py150 loss, ppl] step:71.5, 	loss: 3.250520706176758, 	ppl: 25.358631134033203
[eval_MeetingBank loss, ppl] step:71.5, 	loss: 2.435979127883911, 	ppl: 10.937484741210938
***** Evaluating perplexity, Epoch 5/5, step 10.0 *****
[eval loss, ppl] step:72.5, 	loss: 0.41890543699264526, 	ppl: 1.5537340641021729
[eval_CSTANCE loss, ppl] step:72.5, 	loss: 0.3394708037376404, 	ppl: 1.4715105295181274
[eval_20Minuten loss, ppl] step:72.5, 	loss: 2.2232913970947266, 	ppl: 9.819477081298828
[eval_FOMC loss, ppl] step:72.5, 	loss: 4.600383281707764, 	ppl: 77.94140625
[eval_NumGLUEcm loss, ppl] step:72.5, 	loss: 5.961536884307861, 	ppl: 557.9385375976562
[eval_ScienceQA loss, ppl] step:72.5, 	loss: 1.6989291906356812, 	ppl: 5.719901084899902
[eval_NumGLUEds loss, ppl] step:72.5, 	loss: 7.250692367553711, 	ppl: 1086.8834228515625
[eval_Py150 loss, ppl] step:72.5, 	loss: 3.2500498294830322, 	ppl: 25.39438247680664
[eval_MeetingBank loss, ppl] step:72.5, 	loss: 2.436570644378662, 	ppl: 10.938602447509766
***** Evaluating perplexity, Epoch 5/5, step 11.0 *****
[eval loss, ppl] step:73.5, 	loss: 0.4197339415550232, 	ppl: 1.5578773021697998
[eval_CSTANCE loss, ppl] step:73.5, 	loss: 0.3392513394355774, 	ppl: 1.465667486190796
[eval_20Minuten loss, ppl] step:73.5, 	loss: 2.223388433456421, 	ppl: 9.816398620605469
[eval_FOMC loss, ppl] step:73.5, 	loss: 4.58559513092041, 	ppl: 77.83491516113281
[eval_NumGLUEcm loss, ppl] step:73.5, 	loss: 5.967366695404053, 	ppl: 557.3527221679688
[eval_ScienceQA loss, ppl] step:73.5, 	loss: 1.6998789310455322, 	ppl: 5.721936225891113
[eval_NumGLUEds loss, ppl] step:73.5, 	loss: 7.257141590118408, 	ppl: 1098.91357421875
[eval_Py150 loss, ppl] step:73.5, 	loss: 3.251235008239746, 	ppl: 25.489076614379883
[eval_MeetingBank loss, ppl] step:73.5, 	loss: 2.436495780944824, 	ppl: 10.937723159790039
***** Evaluating perplexity, Epoch 5/5, step 12.0 *****
[eval loss, ppl] step:74.5, 	loss: 0.4228370785713196, 	ppl: 1.566579818725586
[eval_CSTANCE loss, ppl] step:74.5, 	loss: 0.33848926424980164, 	ppl: 1.4704662561416626
[eval_20Minuten loss, ppl] step:74.5, 	loss: 2.2277729511260986, 	ppl: 9.844612121582031
[eval_FOMC loss, ppl] step:74.5, 	loss: 4.590801239013672, 	ppl: 78.64019775390625
[eval_NumGLUEcm loss, ppl] step:74.5, 	loss: 5.971619606018066, 	ppl: 559.04833984375
[eval_ScienceQA loss, ppl] step:74.5, 	loss: 1.6989613771438599, 	ppl: 5.723785877227783
[eval_NumGLUEds loss, ppl] step:74.5, 	loss: 7.2512383460998535, 	ppl: 1095.839599609375
[eval_Py150 loss, ppl] step:74.5, 	loss: 3.2524049282073975, 	ppl: 25.42925453186035
[eval_MeetingBank loss, ppl] step:74.5, 	loss: 2.438403367996216, 	ppl: 10.940694808959961
***** Evaluating perplexity, Epoch 5/5, step 13.0 *****
[eval loss, ppl] step:75.5, 	loss: 0.42769327759742737, 	ppl: 1.578081727027893
[eval_CSTANCE loss, ppl] step:75.5, 	loss: 0.33191564679145813, 	ppl: 1.475888967514038
[eval_20Minuten loss, ppl] step:75.5, 	loss: 2.226198196411133, 	ppl: 9.83788013458252
[eval_FOMC loss, ppl] step:75.5, 	loss: 4.5607123374938965, 	ppl: 76.41932678222656
[eval_NumGLUEcm loss, ppl] step:75.5, 	loss: 5.963218688964844, 	ppl: 551.6055908203125
[eval_ScienceQA loss, ppl] step:75.5, 	loss: 1.7005987167358398, 	ppl: 5.726440906524658
[eval_NumGLUEds loss, ppl] step:75.5, 	loss: 7.265019416809082, 	ppl: 1113.4530029296875
[eval_Py150 loss, ppl] step:75.5, 	loss: 3.2547805309295654, 	ppl: 25.536209106445312
[eval_MeetingBank loss, ppl] step:75.5, 	loss: 2.438424825668335, 	ppl: 10.945272445678711
***** Evaluating perplexity, Epoch 5/5, step 14.0 *****
[eval loss, ppl] step:76.5, 	loss: 0.43765223026275635, 	ppl: 1.595043420791626
[eval_CSTANCE loss, ppl] step:76.5, 	loss: 0.3270960748195648, 	ppl: 1.4925278425216675
[eval_20Minuten loss, ppl] step:76.5, 	loss: 2.2242679595947266, 	ppl: 9.834858894348145
[eval_FOMC loss, ppl] step:76.5, 	loss: 4.525346279144287, 	ppl: 75.13175201416016
[eval_NumGLUEcm loss, ppl] step:76.5, 	loss: 5.964504241943359, 	ppl: 555.8973999023438
[eval_ScienceQA loss, ppl] step:76.5, 	loss: 1.7006651163101196, 	ppl: 5.72562313079834
[eval_NumGLUEds loss, ppl] step:76.5, 	loss: 7.262813091278076, 	ppl: 1108.921875
[eval_Py150 loss, ppl] step:76.5, 	loss: 3.2588083744049072, 	ppl: 25.474035263061523
[eval_MeetingBank loss, ppl] step:76.5, 	loss: 2.4390244483947754, 	ppl: 10.958189964294434
saving model to /data1/TAP/model_con/1020/ours_C-STANCE_epoch5_Llama3Exp_0.001/5...
[2025-10-21 20:16:07,948] [INFO] [launch.py:351:main] Process 1615444 exits successfully.
[2025-10-21 20:16:07,949] [INFO] [launch.py:351:main] Process 1615445 exits successfully.
[2025-10-21 20:16:08,951] [INFO] [launch.py:351:main] Process 1615446 exits successfully.
Sucessful saving model after epoch 5
[2025-10-21 20:16:16,960] [INFO] [launch.py:351:main] Process 1615443 exits successfully.
