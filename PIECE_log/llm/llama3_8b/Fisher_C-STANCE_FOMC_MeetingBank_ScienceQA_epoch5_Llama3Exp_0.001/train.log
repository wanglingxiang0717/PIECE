[2025-09-24 23:49:03,771] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-24 23:49:05,890] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-24 23:49:06,098] [WARNING] [runner.py:220:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-09-24 23:49:06,098] [INFO] [runner.py:610:main] cmd = /home/TAP/anaconda3/envs/llama2/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=25457 --enable_each_rank_log=None training/main.py --data_path /data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/ScienceQA --model_name_or_path /data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_epoch5_Llama3Exp_0.001/5 --per_device_train_batch_size 2 --per_device_eval_batch_size 1 --max_prompt_len 1024 --max_ans_len 512 --learning_rate 1e-5 --weight_decay 0. --num_train_epochs 5 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --num_warmup_steps 0 --seed 42 --zero_stage 2 --deepspeed --print_loss --CL_method base --enable_tensorboard --tensorboard_path /data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/log/ --offload --ood_eval_dir /data2/TAP/data/continue_eval_loss_data_small --mask_method Fisher --top_ratio 0.001 --target_name ScienceQA --output_dir /data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001
[2025-09-24 23:49:08,021] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-24 23:49:10,099] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-24 23:49:10,302] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-09-24 23:49:10,302] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-09-24 23:49:10,302] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-09-24 23:49:10,302] [INFO] [launch.py:164:main] dist_world_size=4
[2025-09-24 23:49:10,303] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-09-24 23:49:10,303] [INFO] [launch.py:256:main] process 2680402 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=0', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/ScienceQA', '--model_name_or_path', '/data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_epoch5_Llama3Exp_0.001/5', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'Fisher', '--top_ratio', '0.001', '--target_name', 'ScienceQA', '--output_dir', '/data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001']
[2025-09-24 23:49:10,304] [INFO] [launch.py:256:main] process 2680403 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=1', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/ScienceQA', '--model_name_or_path', '/data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_epoch5_Llama3Exp_0.001/5', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'Fisher', '--top_ratio', '0.001', '--target_name', 'ScienceQA', '--output_dir', '/data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001']
[2025-09-24 23:49:10,305] [INFO] [launch.py:256:main] process 2680404 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=2', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/ScienceQA', '--model_name_or_path', '/data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_epoch5_Llama3Exp_0.001/5', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'Fisher', '--top_ratio', '0.001', '--target_name', 'ScienceQA', '--output_dir', '/data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001']
[2025-09-24 23:49:10,306] [INFO] [launch.py:256:main] process 2680405 spawned with command: ['/home/TAP/anaconda3/envs/llama2/bin/python', '-u', 'training/main.py', '--local_rank=3', '--data_path', '/data2/TAP/data/TRACE-Benchmark/LLM-CL-Benchmark_1000/ScienceQA', '--model_name_or_path', '/data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_epoch5_Llama3Exp_0.001/5', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--max_prompt_len', '1024', '--max_ans_len', '512', '--learning_rate', '1e-5', '--weight_decay', '0.', '--num_train_epochs', '5', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '42', '--zero_stage', '2', '--deepspeed', '--print_loss', '--CL_method', 'base', '--enable_tensorboard', '--tensorboard_path', '/data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/log/', '--offload', '--ood_eval_dir', '/data2/TAP/data/continue_eval_loss_data_small', '--mask_method', 'Fisher', '--top_ratio', '0.001', '--target_name', 'ScienceQA', '--output_dir', '/data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001']
[2025-09-24 23:49:13,985] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-24 23:49:14,072] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-24 23:49:14,140] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-24 23:49:14,163] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-24 23:49:15,983] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-24 23:49:16,021] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-24 23:49:16,056] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-24 23:49:16,088] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Using integrations.HfDeepSpeedConfig (new API)
Using integrations.HfDeepSpeedConfig (new API)
Using integrations.HfDeepSpeedConfig (new API)
Using integrations.HfDeepSpeedConfig (new API)
/data2/TAP/model_exp/0920_ScienceQA_Fisher_parameters_test_epoch1_random_1000/parameters_grad_2/top0.001
/data2/TAP/model_exp/0920_ScienceQA_Fisher_parameters_test_epoch1_random_1000/parameters_grad_2/top0.001
/data2/TAP/model_exp/0920_ScienceQA_Fisher_parameters_test_epoch1_random_1000/parameters_grad_2/top0.001
/data2/TAP/model_exp/0920_ScienceQA_Fisher_parameters_test_epoch1_random_1000/parameters_grad_2/top0.001
[2025-09-24 23:49:16,870] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-09-24 23:49:16,870] [INFO] [comm.py:706:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-09-24 23:49:17,380] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-09-24 23:49:17,381] [INFO] [comm.py:675:init_distributed] cdb=None
[2025-09-24 23:49:17,382] [INFO] [comm.py:675:init_distributed] cdb=None
ninja: no work to do.
Time to load cpu_adam op: 2.32055401802063 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-09-24 23:52:10,889] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.1, git-hash=unknown, git-branch=unknown
[2025-09-24 23:52:10,890] [INFO] [comm.py:700:init_distributed] Distributed backend already initialized
[2025-09-24 23:52:10,890] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
ninja: no work to do.
Time to load cpu_adam op: 2.352156162261963 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-09-24 23:52:10,955] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
ninja: no work to do.
Time to load cpu_adam op: 2.4555795192718506 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-09-24 23:52:11,070] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
Time to load cpu_adam op: 2.5525057315826416 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
[2025-09-24 23:52:11,167] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 4
[2025-09-24 23:52:16,417] [INFO] [engine.py:1325:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=4
	 self.mp_world_size=1
	 self.seq_dp_world_size=4
	 self.sequence_parallel_size=1
***********************************************
[2025-09-24 23:52:31,068] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-09-24 23:52:31,071] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-09-24 23:52:31,071] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-09-24 23:52:31,090] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-09-24 23:52:31,090] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-09-24 23:52:31,090] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2025-09-24 23:52:31,090] [INFO] [stage_1_and_2.py:151:__init__] Reduce bucket size 500000000
[2025-09-24 23:52:31,090] [INFO] [stage_1_and_2.py:152:__init__] Allgather bucket size 500000000
[2025-09-24 23:52:31,090] [INFO] [stage_1_and_2.py:153:__init__] CPU Offload: True
[2025-09-24 23:52:31,090] [INFO] [stage_1_and_2.py:154:__init__] Round robin gradient partitioning: False
[2025-09-24 23:52:57,538] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-09-24 23:52:57,539] [INFO] [utils.py:782:see_memory_usage] MA 16.06 GB         Max_MA 16.06 GB         CA 16.56 GB         Max_CA 17 GB 
[2025-09-24 23:52:57,539] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 106.44 GB, percent = 10.6%
[2025-09-24 23:52:58,080] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-09-24 23:52:58,081] [INFO] [utils.py:782:see_memory_usage] MA 16.06 GB         Max_MA 16.06 GB         CA 16.56 GB         Max_CA 17 GB 
[2025-09-24 23:52:58,081] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 109.36 GB, percent = 10.9%
[2025-09-24 23:52:58,081] [INFO] [stage_1_and_2.py:573:__init__] optimizer state initialized
[2025-09-24 23:52:58,298] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-09-24 23:52:58,299] [INFO] [utils.py:782:see_memory_usage] MA 16.06 GB         Max_MA 16.06 GB         CA 16.56 GB         Max_CA 17 GB 
[2025-09-24 23:52:58,300] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 108.99 GB, percent = 10.8%
[2025-09-24 23:52:58,307] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-09-24 23:52:58,307] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-09-24 23:52:58,307] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7a4b903b58a0>
[2025-09-24 23:52:58,307] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-09-24 23:52:58,311] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-09-24 23:52:58,311] [INFO] [config.py:921:print] DeepSpeedEngine configuration:
[2025-09-24 23:52:58,312] [INFO] [config.py:925:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-09-24 23:52:58,312] [INFO] [config.py:925:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-09-24 23:52:58,312] [INFO] [config.py:925:print]   amp_enabled .................. False
[2025-09-24 23:52:58,312] [INFO] [config.py:925:print]   amp_params ................... False
[2025-09-24 23:52:58,312] [INFO] [config.py:925:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-09-24 23:52:58,312] [INFO] [config.py:925:print]   bfloat16_config .............. enabled=False immediate_grad_update=False check_grad_overflow=False
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   checkpoint_parallel_write_pipeline  False
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   checkpoint_tag_validation_enabled  True
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   checkpoint_tag_validation_fail  False
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7a4b903b4040>
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   communication_data_type ...... None
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   curriculum_enabled_legacy .... False
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   curriculum_params_legacy ..... False
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   data_efficiency_enabled ...... False
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   dataloader_drop_last ......... False
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   disable_allgather ............ False
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   dump_state ................... False
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   eigenvalue_enabled ........... False
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   eigenvalue_gas_boundary_resolution  1
[2025-09-24 23:52:58,313] [INFO] [config.py:925:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   eigenvalue_layer_num ......... 0
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   eigenvalue_max_iter .......... 100
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   eigenvalue_stability ......... 1e-06
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   eigenvalue_tol ............... 0.01
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   eigenvalue_verbose ........... False
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   elasticity_enabled ........... False
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   global_rank .................. 0
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   grad_accum_dtype ............. None
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   gradient_accumulation_steps .. 8
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   gradient_clipping ............ 1.0
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   gradient_predivide_factor .... 1.0
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   graph_harvesting ............. False
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   load_universal_checkpoint .... False
[2025-09-24 23:52:58,314] [INFO] [config.py:925:print]   memory_breakdown ............. False
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   mics_hierarchial_params_gather  False
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   mics_shard_size .............. -1
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=True, output_path='/data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/log//ds_tensorboard_logs/', job_name='v2_sft_tensorboard') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   optimizer_legacy_fusion ...... False
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   optimizer_name ............... None
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   optimizer_params ............. None
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   pld_enabled .................. False
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   pld_params ................... False
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   prescale_gradients ........... False
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   scheduler_name ............... None
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   scheduler_params ............. None
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   seq_parallel_communication_data_type  torch.float32
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   sparse_attention ............. None
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   sparse_gradients_enabled ..... False
[2025-09-24 23:52:58,315] [INFO] [config.py:925:print]   steps_per_print .............. 10
[2025-09-24 23:52:58,316] [INFO] [config.py:925:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-09-24 23:52:58,316] [INFO] [config.py:925:print]   timers_config ................ enabled=True synchronized=True
[2025-09-24 23:52:58,316] [INFO] [config.py:925:print]   train_batch_size ............. 64
[2025-09-24 23:52:58,316] [INFO] [config.py:925:print]   train_micro_batch_size_per_gpu  2
[2025-09-24 23:52:58,316] [INFO] [config.py:925:print]   use_data_before_expert_parallel_  False
[2025-09-24 23:52:58,316] [INFO] [config.py:925:print]   use_node_local_storage ....... False
[2025-09-24 23:52:58,316] [INFO] [config.py:925:print]   wall_clock_breakdown ......... False
[2025-09-24 23:52:58,316] [INFO] [config.py:925:print]   weight_quantization_config ... None
[2025-09-24 23:52:58,316] [INFO] [config.py:925:print]   world_size ................... 4
[2025-09-24 23:52:58,316] [INFO] [config.py:925:print]   zero_allow_untested_optimizer  False
[2025-09-24 23:52:58,316] [INFO] [config.py:925:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-09-24 23:52:58,316] [INFO] [config.py:925:print]   zero_enabled ................. True
[2025-09-24 23:52:58,316] [INFO] [config.py:925:print]   zero_force_ds_cpu_optimizer .. True
[2025-09-24 23:52:58,316] [INFO] [config.py:925:print]   zero_optimization_stage ...... 2
[2025-09-24 23:52:58,317] [INFO] [config.py:911:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 2, 
    "steps_per_print": 10, 
    "zero_optimization": {
        "stage": 2, 
        "offload_param": {
            "device": "cpu"
        }, 
        "offload_optimizer": {
            "device": "cpu"
        }, 
        "stage3_param_persistence_threshold": 1.000000e+04, 
        "stage3_max_live_parameters": 3.000000e+07, 
        "stage3_prefetch_bucket_size": 3.000000e+07, 
        "memory_efficient_linear": false
    }, 
    "bfloat16": {
        "enabled": "auto"
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "hybrid_engine": {
        "enabled": false, 
        "max_out_tokens": 512, 
        "inference_tp_size": 1, 
        "release_inference_cache": false, 
        "pin_parameters": true, 
        "tp_gather_partition_size": 8
    }, 
    "tensorboard": {
        "enabled": true, 
        "output_path": "/data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/log//ds_tensorboard_logs/", 
        "job_name": "v2_sft_tensorboard"
    }
}
***** Running training *****
Beginning of Epoch 1/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 1/5, step 0.0 *****
[eval loss, ppl] step:0.0, 	loss: 1.4831006526947021, 	ppl: 4.4374518394470215
[eval_CSTANCE loss, ppl] step:0.0, 	loss: 0.5705778002738953, 	ppl: 2.3597707748413086
[eval_20Minuten loss, ppl] step:0.0, 	loss: 1.3573979139328003, 	ppl: 3.9640653133392334
[eval_FOMC loss, ppl] step:0.0, 	loss: 0.4822540879249573, 	ppl: 1.4394166469573975
[eval_NumGLUEcm loss, ppl] step:0.0, 	loss: 1.9093645811080933, 	ppl: 8.458134651184082
[eval_ScienceQA loss, ppl] step:0.0, 	loss: 1.4349299669265747, 	ppl: 4.457928657531738
[eval_NumGLUEds loss, ppl] step:0.0, 	loss: 1.8627568483352661, 	ppl: 14.300447463989258
[eval_Py150 loss, ppl] step:0.0, 	loss: 2.5336015224456787, 	ppl: 13.920506477355957
[eval_MeetingBank loss, ppl] step:0.0, 	loss: 1.303915023803711, 	ppl: 3.366356372833252
***** Evaluating perplexity, Epoch 1/5, step 1.0 *****
[eval loss, ppl] step:1.0, 	loss: 1.4393194913864136, 	ppl: 4.27990198135376
[eval_CSTANCE loss, ppl] step:1.0, 	loss: 0.5741700530052185, 	ppl: 2.3706681728363037
[eval_20Minuten loss, ppl] step:1.0, 	loss: 1.3545818328857422, 	ppl: 3.9591026306152344
[eval_FOMC loss, ppl] step:1.0, 	loss: 0.48474255204200745, 	ppl: 1.43916654586792
[eval_NumGLUEcm loss, ppl] step:1.0, 	loss: 1.975017786026001, 	ppl: 9.324331283569336
[eval_ScienceQA loss, ppl] step:1.0, 	loss: 1.395897388458252, 	ppl: 4.250145435333252
[eval_NumGLUEds loss, ppl] step:1.0, 	loss: 1.9476510286331177, 	ppl: 15.417572975158691
[eval_Py150 loss, ppl] step:1.0, 	loss: 2.560181140899658, 	ppl: 14.186664581298828
[eval_MeetingBank loss, ppl] step:1.0, 	loss: 1.3034002780914307, 	ppl: 3.364224433898926
***** Evaluating perplexity, Epoch 1/5, step 2.0 *****
[eval loss, ppl] step:2.0, 	loss: 1.3920118808746338, 	ppl: 4.101414203643799
[eval_CSTANCE loss, ppl] step:2.0, 	loss: 0.5703825354576111, 	ppl: 2.36873722076416
[eval_20Minuten loss, ppl] step:2.0, 	loss: 1.3515008687973022, 	ppl: 3.954183340072632
[eval_FOMC loss, ppl] step:2.0, 	loss: 0.487483412027359, 	ppl: 1.4389212131500244
[eval_NumGLUEcm loss, ppl] step:2.0, 	loss: 2.0907866954803467, 	ppl: 10.769280433654785
[eval_ScienceQA loss, ppl] step:2.0, 	loss: 1.3542425632476807, 	ppl: 4.048845291137695
[eval_NumGLUEds loss, ppl] step:2.0, 	loss: 2.0623278617858887, 	ppl: 17.37150764465332
[eval_Py150 loss, ppl] step:2.0, 	loss: 2.5955231189727783, 	ppl: 14.827343940734863
[eval_MeetingBank loss, ppl] step:2.0, 	loss: 1.302394151687622, 	ppl: 3.361222982406616
***** Evaluating perplexity, Epoch 1/5, step 3.0 *****
[eval loss, ppl] step:3.0, 	loss: 1.3659676313400269, 	ppl: 4.0105977058410645
[eval_CSTANCE loss, ppl] step:3.0, 	loss: 0.5664172768592834, 	ppl: 2.354393720626831
[eval_20Minuten loss, ppl] step:3.0, 	loss: 1.3495979309082031, 	ppl: 3.949770212173462
[eval_FOMC loss, ppl] step:3.0, 	loss: 0.49350646138191223, 	ppl: 1.4451086521148682
[eval_NumGLUEcm loss, ppl] step:3.0, 	loss: 2.1834373474121094, 	ppl: 11.809990882873535
[eval_ScienceQA loss, ppl] step:3.0, 	loss: 1.332312822341919, 	ppl: 3.9371562004089355
[eval_NumGLUEds loss, ppl] step:3.0, 	loss: 2.1426172256469727, 	ppl: 18.94278335571289
[eval_Py150 loss, ppl] step:3.0, 	loss: 2.6149063110351562, 	ppl: 15.188461303710938
[eval_MeetingBank loss, ppl] step:3.0, 	loss: 1.3037285804748535, 	ppl: 3.365330219268799
***** Evaluating perplexity, Epoch 1/5, step 4.0 *****
[eval loss, ppl] step:4.0, 	loss: 1.3169139623641968, 	ppl: 3.822197437286377
[eval_CSTANCE loss, ppl] step:4.0, 	loss: 0.5702325105667114, 	ppl: 2.3335957527160645
[eval_20Minuten loss, ppl] step:4.0, 	loss: 1.3443589210510254, 	ppl: 3.937810182571411
[eval_FOMC loss, ppl] step:4.0, 	loss: 0.48188698291778564, 	ppl: 1.4293612241744995
[eval_NumGLUEcm loss, ppl] step:4.0, 	loss: 2.436624050140381, 	ppl: 14.640480041503906
[eval_ScienceQA loss, ppl] step:4.0, 	loss: 1.2886813879013062, 	ppl: 3.7353599071502686
[eval_NumGLUEds loss, ppl] step:4.0, 	loss: 2.3383612632751465, 	ppl: 23.28945541381836
[eval_Py150 loss, ppl] step:4.0, 	loss: 2.6587941646575928, 	ppl: 15.974691390991211
[eval_MeetingBank loss, ppl] step:4.0, 	loss: 1.306660771369934, 	ppl: 3.372799873352051
***** Evaluating perplexity, Epoch 1/5, step 5.0 *****
[eval loss, ppl] step:5.0, 	loss: 1.2882510423660278, 	ppl: 3.7031946182250977
[eval_CSTANCE loss, ppl] step:5.0, 	loss: 0.5702211856842041, 	ppl: 2.324932336807251
[eval_20Minuten loss, ppl] step:5.0, 	loss: 1.3434219360351562, 	ppl: 3.9356751441955566
[eval_FOMC loss, ppl] step:5.0, 	loss: 0.4777863025665283, 	ppl: 1.4271736145019531
[eval_NumGLUEcm loss, ppl] step:5.0, 	loss: 2.612058639526367, 	ppl: 17.030723571777344
[eval_ScienceQA loss, ppl] step:5.0, 	loss: 1.2630741596221924, 	ppl: 3.623276710510254
[eval_NumGLUEds loss, ppl] step:5.0, 	loss: 2.4754185676574707, 	ppl: 26.30430793762207
[eval_Py150 loss, ppl] step:5.0, 	loss: 2.6881520748138428, 	ppl: 16.374744415283203
[eval_MeetingBank loss, ppl] step:5.0, 	loss: 1.309259295463562, 	ppl: 3.380039691925049
***** Evaluating perplexity, Epoch 1/5, step 6.0 *****
[eval loss, ppl] step:6.0, 	loss: 1.2670183181762695, 	ppl: 3.618734359741211
[eval_CSTANCE loss, ppl] step:6.0, 	loss: 0.5720066428184509, 	ppl: 2.2920007705688477
[eval_20Minuten loss, ppl] step:6.0, 	loss: 1.3418222665786743, 	ppl: 3.935523509979248
[eval_FOMC loss, ppl] step:6.0, 	loss: 0.4682219624519348, 	ppl: 1.4259799718856812
[eval_NumGLUEcm loss, ppl] step:6.0, 	loss: 2.757181167602539, 	ppl: 18.910205841064453
[eval_ScienceQA loss, ppl] step:6.0, 	loss: 1.2450233697891235, 	ppl: 3.541729211807251
[eval_NumGLUEds loss, ppl] step:6.0, 	loss: 2.587944746017456, 	ppl: 29.295373916625977
[eval_Py150 loss, ppl] step:6.0, 	loss: 2.7038345336914062, 	ppl: 16.71710777282715
[eval_MeetingBank loss, ppl] step:6.0, 	loss: 1.3106166124343872, 	ppl: 3.38664174079895
***** Evaluating perplexity, Epoch 1/5, step 7.0 *****
[eval loss, ppl] step:7.0, 	loss: 1.2473909854888916, 	ppl: 3.5380451679229736
[eval_CSTANCE loss, ppl] step:7.0, 	loss: 0.561855137348175, 	ppl: 2.2780232429504395
[eval_20Minuten loss, ppl] step:7.0, 	loss: 1.3401535749435425, 	ppl: 3.9291348457336426
[eval_FOMC loss, ppl] step:7.0, 	loss: 0.4641960859298706, 	ppl: 1.4235411882400513
[eval_NumGLUEcm loss, ppl] step:7.0, 	loss: 2.892627477645874, 	ppl: 21.63465118408203
[eval_ScienceQA loss, ppl] step:7.0, 	loss: 1.2279144525527954, 	ppl: 3.470313549041748
[eval_NumGLUEds loss, ppl] step:7.0, 	loss: 2.731269359588623, 	ppl: 33.30891799926758
[eval_Py150 loss, ppl] step:7.0, 	loss: 2.722210645675659, 	ppl: 17.1270751953125
[eval_MeetingBank loss, ppl] step:7.0, 	loss: 1.3133214712142944, 	ppl: 3.3950538635253906
***** Evaluating perplexity, Epoch 1/5, step 8.0 *****
[eval loss, ppl] step:8.0, 	loss: 1.2342283725738525, 	ppl: 3.485844373703003
[eval_CSTANCE loss, ppl] step:8.0, 	loss: 0.5725417733192444, 	ppl: 2.2520158290863037
[eval_20Minuten loss, ppl] step:8.0, 	loss: 1.339310884475708, 	ppl: 3.9296462535858154
[eval_FOMC loss, ppl] step:8.0, 	loss: 0.4635585844516754, 	ppl: 1.4204001426696777
[eval_NumGLUEcm loss, ppl] step:8.0, 	loss: 3.000126600265503, 	ppl: 23.906967163085938
[eval_ScienceQA loss, ppl] step:8.0, 	loss: 1.2123230695724487, 	ppl: 3.4176764488220215
[eval_NumGLUEds loss, ppl] step:8.0, 	loss: 2.8355941772460938, 	ppl: 37.00437927246094
[eval_Py150 loss, ppl] step:8.0, 	loss: 2.732229471206665, 	ppl: 17.30846405029297
[eval_MeetingBank loss, ppl] step:8.0, 	loss: 1.3161308765411377, 	ppl: 3.4019973278045654
***** Evaluating perplexity, Epoch 1/5, step 9.0 *****
[eval loss, ppl] step:9.0, 	loss: 1.2231546640396118, 	ppl: 3.4461512565612793
[eval_CSTANCE loss, ppl] step:9.0, 	loss: 0.566124439239502, 	ppl: 2.236531972885132
[eval_20Minuten loss, ppl] step:9.0, 	loss: 1.3363040685653687, 	ppl: 3.926478624343872
[eval_FOMC loss, ppl] step:9.0, 	loss: 0.46553757786750793, 	ppl: 1.4248138666152954
[eval_NumGLUEcm loss, ppl] step:9.0, 	loss: 3.112750291824341, 	ppl: 27.124305725097656
[eval_ScienceQA loss, ppl] step:9.0, 	loss: 1.201684832572937, 	ppl: 3.3769073486328125
[eval_NumGLUEds loss, ppl] step:9.0, 	loss: 2.938417673110962, 	ppl: 40.749420166015625
[eval_Py150 loss, ppl] step:9.0, 	loss: 2.759869337081909, 	ppl: 17.647998809814453
[eval_MeetingBank loss, ppl] step:9.0, 	loss: 1.3184778690338135, 	ppl: 3.410076141357422
[2025-09-25 00:07:50,435] [INFO] [logging.py:107:log_dist] [Rank 0] step=10, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-09-25 00:07:51,086] [INFO] [timer.py:264:stop] epoch=0/micro_step=80/global_step=10, RunningAvgSamplesPerSec=1.3132211966468539, CurrSamplesPerSec=1.3240260598167874, MemAllocated=30.64GB, MaxMemAllocated=46.74GB
***** Evaluating perplexity, Epoch 1/5, step 10.0 *****
[eval loss, ppl] step:10.0, 	loss: 1.2122929096221924, 	ppl: 3.4105963706970215
[eval_CSTANCE loss, ppl] step:10.0, 	loss: 0.5646440386772156, 	ppl: 2.220235824584961
[eval_20Minuten loss, ppl] step:10.0, 	loss: 1.3373003005981445, 	ppl: 3.9277310371398926
[eval_FOMC loss, ppl] step:10.0, 	loss: 0.4631085991859436, 	ppl: 1.4265116453170776
[eval_NumGLUEcm loss, ppl] step:10.0, 	loss: 3.231147527694702, 	ppl: 30.37984848022461
[eval_ScienceQA loss, ppl] step:10.0, 	loss: 1.1889400482177734, 	ppl: 3.3332574367523193
[eval_NumGLUEds loss, ppl] step:10.0, 	loss: 3.0563783645629883, 	ppl: 44.69441223144531
[eval_Py150 loss, ppl] step:10.0, 	loss: 2.7597641944885254, 	ppl: 17.78983497619629
[eval_MeetingBank loss, ppl] step:10.0, 	loss: 1.3204400539398193, 	ppl: 3.411583662033081
***** Evaluating perplexity, Epoch 1/5, step 11.0 *****
[eval loss, ppl] step:11.0, 	loss: 1.1999050378799438, 	ppl: 3.3712120056152344
[eval_CSTANCE loss, ppl] step:11.0, 	loss: 0.5651837587356567, 	ppl: 2.200876235961914
[eval_20Minuten loss, ppl] step:11.0, 	loss: 1.3369553089141846, 	ppl: 3.928892135620117
[eval_FOMC loss, ppl] step:11.0, 	loss: 0.4718644618988037, 	ppl: 1.4413329362869263
[eval_NumGLUEcm loss, ppl] step:11.0, 	loss: 3.3713560104370117, 	ppl: 35.13874816894531
[eval_ScienceQA loss, ppl] step:11.0, 	loss: 1.1732321977615356, 	ppl: 3.280215263366699
[eval_NumGLUEds loss, ppl] step:11.0, 	loss: 3.167476177215576, 	ppl: 49.65711212158203
[eval_Py150 loss, ppl] step:11.0, 	loss: 2.7595574855804443, 	ppl: 17.788785934448242
[eval_MeetingBank loss, ppl] step:11.0, 	loss: 1.3228435516357422, 	ppl: 3.4184398651123047
***** Evaluating perplexity, Epoch 1/5, step 12.0 *****
[eval loss, ppl] step:12.0, 	loss: 1.1861457824707031, 	ppl: 3.3244576454162598
[eval_CSTANCE loss, ppl] step:12.0, 	loss: 0.5635560154914856, 	ppl: 2.1873629093170166
[eval_20Minuten loss, ppl] step:12.0, 	loss: 1.33756685256958, 	ppl: 3.93123459815979
[eval_FOMC loss, ppl] step:12.0, 	loss: 0.47559618949890137, 	ppl: 1.4521433115005493
[eval_NumGLUEcm loss, ppl] step:12.0, 	loss: 3.4810643196105957, 	ppl: 39.06140899658203
[eval_ScienceQA loss, ppl] step:12.0, 	loss: 1.1602561473846436, 	ppl: 3.230933666229248
[eval_NumGLUEds loss, ppl] step:12.0, 	loss: 3.2504289150238037, 	ppl: 53.11017990112305
[eval_Py150 loss, ppl] step:12.0, 	loss: 2.7682437896728516, 	ppl: 17.89441680908203
[eval_MeetingBank loss, ppl] step:12.0, 	loss: 1.323488473892212, 	ppl: 3.4209234714508057
***** Evaluating perplexity, Epoch 1/5, step 13.0 *****
[eval loss, ppl] step:13.0, 	loss: 1.1744250059127808, 	ppl: 3.286994695663452
[eval_CSTANCE loss, ppl] step:13.0, 	loss: 0.5560463666915894, 	ppl: 2.174030065536499
[eval_20Minuten loss, ppl] step:13.0, 	loss: 1.3370919227600098, 	ppl: 3.9321634769439697
[eval_FOMC loss, ppl] step:13.0, 	loss: 0.47944962978363037, 	ppl: 1.4558380842208862
[eval_NumGLUEcm loss, ppl] step:13.0, 	loss: 3.5879642963409424, 	ppl: 43.069698333740234
[eval_ScienceQA loss, ppl] step:13.0, 	loss: 1.1504414081573486, 	ppl: 3.1909677982330322
[eval_NumGLUEds loss, ppl] step:13.0, 	loss: 3.2839221954345703, 	ppl: 56.64519500732422
[eval_Py150 loss, ppl] step:13.0, 	loss: 2.765183210372925, 	ppl: 17.871137619018555
[eval_MeetingBank loss, ppl] step:13.0, 	loss: 1.3240998983383179, 	ppl: 3.4241485595703125
***** Evaluating perplexity, Epoch 1/5, step 14.0 *****
[eval loss, ppl] step:14.0, 	loss: 1.1633914709091187, 	ppl: 3.254754066467285
[eval_CSTANCE loss, ppl] step:14.0, 	loss: 0.558476448059082, 	ppl: 2.1692605018615723
[eval_20Minuten loss, ppl] step:14.0, 	loss: 1.336577296257019, 	ppl: 3.930678367614746
[eval_FOMC loss, ppl] step:14.0, 	loss: 0.4852147698402405, 	ppl: 1.469247579574585
[eval_NumGLUEcm loss, ppl] step:14.0, 	loss: 3.5999393463134766, 	ppl: 44.825965881347656
[eval_ScienceQA loss, ppl] step:14.0, 	loss: 1.140342116355896, 	ppl: 3.1548423767089844
[eval_NumGLUEds loss, ppl] step:14.0, 	loss: 3.3062703609466553, 	ppl: 58.129791259765625
[eval_Py150 loss, ppl] step:14.0, 	loss: 2.763158082962036, 	ppl: 17.831464767456055
[eval_MeetingBank loss, ppl] step:14.0, 	loss: 1.3239771127700806, 	ppl: 3.421670913696289
saving model to /data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/1...
Sucessful saving model after epoch 1
Beginning of Epoch 2/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 2/5, step 0.0 *****
[eval loss, ppl] step:15.625, 	loss: 1.1424593925476074, 	ppl: 3.191680908203125
[eval_CSTANCE loss, ppl] step:15.625, 	loss: 0.554831862449646, 	ppl: 2.163163185119629
[eval_20Minuten loss, ppl] step:15.625, 	loss: 1.338685393333435, 	ppl: 3.935823917388916
[eval_FOMC loss, ppl] step:15.625, 	loss: 0.4953050911426544, 	ppl: 1.4792265892028809
[eval_NumGLUEcm loss, ppl] step:15.625, 	loss: 3.6568243503570557, 	ppl: 46.92593002319336
[eval_ScienceQA loss, ppl] step:15.625, 	loss: 1.1224300861358643, 	ppl: 3.0875329971313477
[eval_NumGLUEds loss, ppl] step:15.625, 	loss: 3.315002202987671, 	ppl: 59.63695526123047
[eval_Py150 loss, ppl] step:15.625, 	loss: 2.75254225730896, 	ppl: 17.739093780517578
[eval_MeetingBank loss, ppl] step:15.625, 	loss: 1.325044870376587, 	ppl: 3.424863815307617
***** Evaluating perplexity, Epoch 2/5, step 1.0 *****
[eval loss, ppl] step:16.625, 	loss: 1.1330139636993408, 	ppl: 3.162513017654419
[eval_CSTANCE loss, ppl] step:16.625, 	loss: 0.5660962462425232, 	ppl: 2.174175977706909
[eval_20Minuten loss, ppl] step:16.625, 	loss: 1.3392839431762695, 	ppl: 3.931948661804199
[eval_FOMC loss, ppl] step:16.625, 	loss: 0.5020297765731812, 	ppl: 1.4939496517181396
[eval_NumGLUEcm loss, ppl] step:16.625, 	loss: 3.605701446533203, 	ppl: 46.46504211425781
[eval_ScienceQA loss, ppl] step:16.625, 	loss: 1.1129487752914429, 	ppl: 3.060394287109375
[eval_NumGLUEds loss, ppl] step:16.625, 	loss: 3.3089540004730225, 	ppl: 59.405086517333984
[eval_Py150 loss, ppl] step:16.625, 	loss: 2.755267858505249, 	ppl: 17.73249053955078
[eval_MeetingBank loss, ppl] step:16.625, 	loss: 1.3243168592453003, 	ppl: 3.4217751026153564
***** Evaluating perplexity, Epoch 2/5, step 2.0 *****
[eval loss, ppl] step:17.625, 	loss: 1.1228445768356323, 	ppl: 3.1318860054016113
[eval_CSTANCE loss, ppl] step:17.625, 	loss: 0.5565876960754395, 	ppl: 2.171992301940918
[eval_20Minuten loss, ppl] step:17.625, 	loss: 1.338571548461914, 	ppl: 3.9284722805023193
[eval_FOMC loss, ppl] step:17.625, 	loss: 0.4938352704048157, 	ppl: 1.4958337545394897
[eval_NumGLUEcm loss, ppl] step:17.625, 	loss: 3.5646255016326904, 	ppl: 45.50547409057617
[eval_ScienceQA loss, ppl] step:17.625, 	loss: 1.1055828332901, 	ppl: 3.030996084213257
[eval_NumGLUEds loss, ppl] step:17.625, 	loss: 3.289127826690674, 	ppl: 59.315147399902344
[eval_Py150 loss, ppl] step:17.625, 	loss: 2.7567121982574463, 	ppl: 17.677181243896484
[eval_MeetingBank loss, ppl] step:17.625, 	loss: 1.3237959146499634, 	ppl: 3.4191055297851562
***** Evaluating perplexity, Epoch 2/5, step 3.0 *****
[eval loss, ppl] step:18.625, 	loss: 1.1129589080810547, 	ppl: 3.1048035621643066
[eval_CSTANCE loss, ppl] step:18.625, 	loss: 0.55816650390625, 	ppl: 2.1685867309570312
[eval_20Minuten loss, ppl] step:18.625, 	loss: 1.3402241468429565, 	ppl: 3.9317893981933594
[eval_FOMC loss, ppl] step:18.625, 	loss: 0.49853765964508057, 	ppl: 1.4970272779464722
[eval_NumGLUEcm loss, ppl] step:18.625, 	loss: 3.5834262371063232, 	ppl: 45.1912727355957
[eval_ScienceQA loss, ppl] step:18.625, 	loss: 1.0968408584594727, 	ppl: 3.0034756660461426
[eval_NumGLUEds loss, ppl] step:18.625, 	loss: 3.2722277641296387, 	ppl: 59.04728698730469
[eval_Py150 loss, ppl] step:18.625, 	loss: 2.7541306018829346, 	ppl: 17.614713668823242
[eval_MeetingBank loss, ppl] step:18.625, 	loss: 1.323441743850708, 	ppl: 3.417823553085327
[2025-09-25 00:20:31,356] [INFO] [logging.py:107:log_dist] [Rank 0] step=20, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-09-25 00:20:32,006] [INFO] [timer.py:264:stop] epoch=0/micro_step=160/global_step=20, RunningAvgSamplesPerSec=1.2968921488536, CurrSamplesPerSec=1.3004375697082589, MemAllocated=30.3GB, MaxMemAllocated=46.74GB
***** Evaluating perplexity, Epoch 2/5, step 4.0 *****
[eval loss, ppl] step:19.625, 	loss: 1.1037577390670776, 	ppl: 3.0794010162353516
[eval_CSTANCE loss, ppl] step:19.625, 	loss: 0.5573176145553589, 	ppl: 2.186030864715576
[eval_20Minuten loss, ppl] step:19.625, 	loss: 1.3403701782226562, 	ppl: 3.934103488922119
[eval_FOMC loss, ppl] step:19.625, 	loss: 0.5004408359527588, 	ppl: 1.4986474514007568
[eval_NumGLUEcm loss, ppl] step:19.625, 	loss: 3.5448904037475586, 	ppl: 44.335906982421875
[eval_ScienceQA loss, ppl] step:19.625, 	loss: 1.0881993770599365, 	ppl: 2.975682497024536
[eval_NumGLUEds loss, ppl] step:19.625, 	loss: 3.2693564891815186, 	ppl: 58.7856330871582
[eval_Py150 loss, ppl] step:19.625, 	loss: 2.75040864944458, 	ppl: 17.56846046447754
[eval_MeetingBank loss, ppl] step:19.625, 	loss: 1.3227150440216064, 	ppl: 3.4160690307617188
***** Evaluating perplexity, Epoch 2/5, step 5.0 *****
[eval loss, ppl] step:20.625, 	loss: 1.0956686735153198, 	ppl: 3.0603060722351074
[eval_CSTANCE loss, ppl] step:20.625, 	loss: 0.5646761059761047, 	ppl: 2.1862406730651855
[eval_20Minuten loss, ppl] step:20.625, 	loss: 1.3403668403625488, 	ppl: 3.931647777557373
[eval_FOMC loss, ppl] step:20.625, 	loss: 0.4982076585292816, 	ppl: 1.5038213729858398
[eval_NumGLUEcm loss, ppl] step:20.625, 	loss: 3.53096604347229, 	ppl: 45.35634231567383
[eval_ScienceQA loss, ppl] step:20.625, 	loss: 1.0800336599349976, 	ppl: 2.951749324798584
[eval_NumGLUEds loss, ppl] step:20.625, 	loss: 3.2544748783111572, 	ppl: 59.12925720214844
[eval_Py150 loss, ppl] step:20.625, 	loss: 2.74960994720459, 	ppl: 17.566173553466797
[eval_MeetingBank loss, ppl] step:20.625, 	loss: 1.3242666721343994, 	ppl: 3.419487953186035
***** Evaluating perplexity, Epoch 2/5, step 6.0 *****
[eval loss, ppl] step:21.625, 	loss: 1.0871524810791016, 	ppl: 3.0389623641967773
[eval_CSTANCE loss, ppl] step:21.625, 	loss: 0.5534411668777466, 	ppl: 2.199232578277588
[eval_20Minuten loss, ppl] step:21.625, 	loss: 1.3384596109390259, 	ppl: 3.9295473098754883
[eval_FOMC loss, ppl] step:21.625, 	loss: 0.500357985496521, 	ppl: 1.507748007774353
[eval_NumGLUEcm loss, ppl] step:21.625, 	loss: 3.5328238010406494, 	ppl: 44.80398941040039
[eval_ScienceQA loss, ppl] step:21.625, 	loss: 1.0720207691192627, 	ppl: 2.9262378215789795
[eval_NumGLUEds loss, ppl] step:21.625, 	loss: 3.2549326419830322, 	ppl: 58.96660614013672
[eval_Py150 loss, ppl] step:21.625, 	loss: 2.7420966625213623, 	ppl: 17.50043296813965
[eval_MeetingBank loss, ppl] step:21.625, 	loss: 1.3237603902816772, 	ppl: 3.419706344604492
***** Evaluating perplexity, Epoch 2/5, step 7.0 *****
[eval loss, ppl] step:22.625, 	loss: 1.0790873765945435, 	ppl: 3.0151052474975586
[eval_CSTANCE loss, ppl] step:22.625, 	loss: 0.5605153441429138, 	ppl: 2.1851940155029297
[eval_20Minuten loss, ppl] step:22.625, 	loss: 1.33905029296875, 	ppl: 3.930147171020508
[eval_FOMC loss, ppl] step:22.625, 	loss: 0.513507604598999, 	ppl: 1.5197744369506836
[eval_NumGLUEcm loss, ppl] step:22.625, 	loss: 3.4861745834350586, 	ppl: 44.69064712524414
[eval_ScienceQA loss, ppl] step:22.625, 	loss: 1.0641300678253174, 	ppl: 2.9023571014404297
[eval_NumGLUEds loss, ppl] step:22.625, 	loss: 3.2745931148529053, 	ppl: 59.37228012084961
[eval_Py150 loss, ppl] step:22.625, 	loss: 2.7453010082244873, 	ppl: 17.474214553833008
[eval_MeetingBank loss, ppl] step:22.625, 	loss: 1.3227895498275757, 	ppl: 3.4166786670684814
***** Evaluating perplexity, Epoch 2/5, step 8.0 *****
[eval loss, ppl] step:23.625, 	loss: 1.0705556869506836, 	ppl: 2.991828441619873
[eval_CSTANCE loss, ppl] step:23.625, 	loss: 0.5662000775337219, 	ppl: 2.1941258907318115
[eval_20Minuten loss, ppl] step:23.625, 	loss: 1.3412938117980957, 	ppl: 3.93703031539917
[eval_FOMC loss, ppl] step:23.625, 	loss: 0.516923189163208, 	ppl: 1.5214630365371704
[eval_NumGLUEcm loss, ppl] step:23.625, 	loss: 3.5096335411071777, 	ppl: 44.688289642333984
[eval_ScienceQA loss, ppl] step:23.625, 	loss: 1.055295467376709, 	ppl: 2.877995491027832
[eval_NumGLUEds loss, ppl] step:23.625, 	loss: 3.2760605812072754, 	ppl: 60.283721923828125
[eval_Py150 loss, ppl] step:23.625, 	loss: 2.747499465942383, 	ppl: 17.49319839477539
[eval_MeetingBank loss, ppl] step:23.625, 	loss: 1.3259910345077515, 	ppl: 3.4180824756622314
***** Evaluating perplexity, Epoch 2/5, step 9.0 *****
[eval loss, ppl] step:24.625, 	loss: 1.0624619722366333, 	ppl: 2.97067928314209
[eval_CSTANCE loss, ppl] step:24.625, 	loss: 0.5632155537605286, 	ppl: 2.2014544010162354
[eval_20Minuten loss, ppl] step:24.625, 	loss: 1.3427814245224, 	ppl: 3.9375126361846924
[eval_FOMC loss, ppl] step:24.625, 	loss: 0.5160415768623352, 	ppl: 1.5298787355422974
[eval_NumGLUEcm loss, ppl] step:24.625, 	loss: 3.505984306335449, 	ppl: 45.05953598022461
[eval_ScienceQA loss, ppl] step:24.625, 	loss: 1.0467890501022339, 	ppl: 2.8507702350616455
[eval_NumGLUEds loss, ppl] step:24.625, 	loss: 3.3093395233154297, 	ppl: 61.220767974853516
[eval_Py150 loss, ppl] step:24.625, 	loss: 2.7579116821289062, 	ppl: 17.53023910522461
[eval_MeetingBank loss, ppl] step:24.625, 	loss: 1.3242638111114502, 	ppl: 3.417870044708252
***** Evaluating perplexity, Epoch 2/5, step 10.0 *****
[eval loss, ppl] step:25.625, 	loss: 1.0546127557754517, 	ppl: 2.9491829872131348
[eval_CSTANCE loss, ppl] step:25.625, 	loss: 0.5538594722747803, 	ppl: 2.1982994079589844
[eval_20Minuten loss, ppl] step:25.625, 	loss: 1.3423794507980347, 	ppl: 3.941434860229492
[eval_FOMC loss, ppl] step:25.625, 	loss: 0.5164927840232849, 	ppl: 1.5257947444915771
[eval_NumGLUEcm loss, ppl] step:25.625, 	loss: 3.5266478061676025, 	ppl: 46.81980514526367
[eval_ScienceQA loss, ppl] step:25.625, 	loss: 1.0391197204589844, 	ppl: 2.8263840675354004
[eval_NumGLUEds loss, ppl] step:25.625, 	loss: 3.325261116027832, 	ppl: 62.42867660522461
[eval_Py150 loss, ppl] step:25.625, 	loss: 2.753394603729248, 	ppl: 17.445302963256836
[eval_MeetingBank loss, ppl] step:25.625, 	loss: 1.3255208730697632, 	ppl: 3.421736717224121
***** Evaluating perplexity, Epoch 2/5, step 11.0 *****
[eval loss, ppl] step:26.625, 	loss: 1.0469980239868164, 	ppl: 2.9321064949035645
[eval_CSTANCE loss, ppl] step:26.625, 	loss: 0.5446687340736389, 	ppl: 2.1872522830963135
[eval_20Minuten loss, ppl] step:26.625, 	loss: 1.3438701629638672, 	ppl: 3.9411635398864746
[eval_FOMC loss, ppl] step:26.625, 	loss: 0.5105339288711548, 	ppl: 1.5234606266021729
[eval_NumGLUEcm loss, ppl] step:26.625, 	loss: 3.51301908493042, 	ppl: 47.05702209472656
[eval_ScienceQA loss, ppl] step:26.625, 	loss: 1.0314288139343262, 	ppl: 2.805171489715576
[eval_NumGLUEds loss, ppl] step:26.625, 	loss: 3.321103096008301, 	ppl: 63.31385803222656
[eval_Py150 loss, ppl] step:26.625, 	loss: 2.751635789871216, 	ppl: 17.42546272277832
[eval_MeetingBank loss, ppl] step:26.625, 	loss: 1.3252264261245728, 	ppl: 3.4195613861083984
***** Evaluating perplexity, Epoch 2/5, step 12.0 *****
[eval loss, ppl] step:27.625, 	loss: 1.0386699438095093, 	ppl: 2.912517547607422
[eval_CSTANCE loss, ppl] step:27.625, 	loss: 0.5460764169692993, 	ppl: 2.1920390129089355
[eval_20Minuten loss, ppl] step:27.625, 	loss: 1.3451460599899292, 	ppl: 3.9487318992614746
[eval_FOMC loss, ppl] step:27.625, 	loss: 0.5246836543083191, 	ppl: 1.5357329845428467
[eval_NumGLUEcm loss, ppl] step:27.625, 	loss: 3.5643343925476074, 	ppl: 48.97557067871094
[eval_ScienceQA loss, ppl] step:27.625, 	loss: 1.023455262184143, 	ppl: 2.781095504760742
[eval_NumGLUEds loss, ppl] step:27.625, 	loss: 3.359675884246826, 	ppl: 64.73312377929688
[eval_Py150 loss, ppl] step:27.625, 	loss: 2.747982978820801, 	ppl: 17.394638061523438
[eval_MeetingBank loss, ppl] step:27.625, 	loss: 1.3247069120407104, 	ppl: 3.4219541549682617
***** Evaluating perplexity, Epoch 2/5, step 13.0 *****
[eval loss, ppl] step:28.625, 	loss: 1.0300060510635376, 	ppl: 2.8877809047698975
[eval_CSTANCE loss, ppl] step:28.625, 	loss: 0.5495244264602661, 	ppl: 2.1914539337158203
[eval_20Minuten loss, ppl] step:28.625, 	loss: 1.345592975616455, 	ppl: 3.9499592781066895
[eval_FOMC loss, ppl] step:28.625, 	loss: 0.5123818516731262, 	ppl: 1.531061053276062
[eval_NumGLUEcm loss, ppl] step:28.625, 	loss: 3.585477113723755, 	ppl: 50.58515167236328
[eval_ScienceQA loss, ppl] step:28.625, 	loss: 1.017333745956421, 	ppl: 2.7572975158691406
[eval_NumGLUEds loss, ppl] step:28.625, 	loss: 3.3820948600769043, 	ppl: 65.1056137084961
[eval_Py150 loss, ppl] step:28.625, 	loss: 2.7452690601348877, 	ppl: 17.358318328857422
[eval_MeetingBank loss, ppl] step:28.625, 	loss: 1.3273028135299683, 	ppl: 3.4242405891418457
[2025-09-25 00:33:04,239] [INFO] [logging.py:107:log_dist] [Rank 0] step=30, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-09-25 00:33:04,892] [INFO] [timer.py:264:stop] epoch=0/micro_step=240/global_step=30, RunningAvgSamplesPerSec=1.3010809489143302, CurrSamplesPerSec=1.34283063293987, MemAllocated=30.27GB, MaxMemAllocated=46.74GB
***** Evaluating perplexity, Epoch 2/5, step 14.0 *****
[eval loss, ppl] step:29.625, 	loss: 1.0213701725006104, 	ppl: 2.860349178314209
[eval_CSTANCE loss, ppl] step:29.625, 	loss: 0.5422159433364868, 	ppl: 2.18399715423584
[eval_20Minuten loss, ppl] step:29.625, 	loss: 1.3466538190841675, 	ppl: 3.9529945850372314
[eval_FOMC loss, ppl] step:29.625, 	loss: 0.5127840042114258, 	ppl: 1.5355802774429321
[eval_NumGLUEcm loss, ppl] step:29.625, 	loss: 3.6366467475891113, 	ppl: 53.63221740722656
[eval_ScienceQA loss, ppl] step:29.625, 	loss: 1.0092926025390625, 	ppl: 2.734920024871826
[eval_NumGLUEds loss, ppl] step:29.625, 	loss: 3.3883659839630127, 	ppl: 66.27877044677734
[eval_Py150 loss, ppl] step:29.625, 	loss: 2.7432851791381836, 	ppl: 17.265195846557617
[eval_MeetingBank loss, ppl] step:29.625, 	loss: 1.3265451192855835, 	ppl: 3.4228386878967285
saving model to /data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/2...
Sucessful saving model after epoch 2
Beginning of Epoch 3/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 3/5, step 0.0 *****
[eval loss, ppl] step:31.25, 	loss: 1.0064976215362549, 	ppl: 2.8179595470428467
[eval_CSTANCE loss, ppl] step:31.25, 	loss: 0.5560340285301208, 	ppl: 2.1892404556274414
[eval_20Minuten loss, ppl] step:31.25, 	loss: 1.347257375717163, 	ppl: 3.960434913635254
[eval_FOMC loss, ppl] step:31.25, 	loss: 0.5060184597969055, 	ppl: 1.5276198387145996
[eval_NumGLUEcm loss, ppl] step:31.25, 	loss: 3.657665252685547, 	ppl: 56.23759460449219
[eval_ScienceQA loss, ppl] step:31.25, 	loss: 0.9993999004364014, 	ppl: 2.7004404067993164
[eval_NumGLUEds loss, ppl] step:31.25, 	loss: 3.42787504196167, 	ppl: 68.07675170898438
[eval_Py150 loss, ppl] step:31.25, 	loss: 2.7364981174468994, 	ppl: 17.13280487060547
[eval_MeetingBank loss, ppl] step:31.25, 	loss: 1.327419638633728, 	ppl: 3.4258086681365967
***** Evaluating perplexity, Epoch 3/5, step 1.0 *****
[eval loss, ppl] step:32.25, 	loss: 0.999712347984314, 	ppl: 2.800238609313965
[eval_CSTANCE loss, ppl] step:32.25, 	loss: 0.5503894686698914, 	ppl: 2.1902379989624023
[eval_20Minuten loss, ppl] step:32.25, 	loss: 1.3475182056427002, 	ppl: 3.9622714519500732
[eval_FOMC loss, ppl] step:32.25, 	loss: 0.5097373723983765, 	ppl: 1.536665439605713
[eval_NumGLUEcm loss, ppl] step:32.25, 	loss: 3.6802258491516113, 	ppl: 58.91572570800781
[eval_ScienceQA loss, ppl] step:32.25, 	loss: 0.9942179322242737, 	ppl: 2.6867690086364746
[eval_NumGLUEds loss, ppl] step:32.25, 	loss: 3.4292023181915283, 	ppl: 67.77496337890625
[eval_Py150 loss, ppl] step:32.25, 	loss: 2.7347912788391113, 	ppl: 17.14543914794922
[eval_MeetingBank loss, ppl] step:32.25, 	loss: 1.3281984329223633, 	ppl: 3.4264297485351562
***** Evaluating perplexity, Epoch 3/5, step 2.0 *****
[eval loss, ppl] step:33.25, 	loss: 0.9929075837135315, 	ppl: 2.7779946327209473
[eval_CSTANCE loss, ppl] step:33.25, 	loss: 0.5403620004653931, 	ppl: 2.189695119857788
[eval_20Minuten loss, ppl] step:33.25, 	loss: 1.3478004932403564, 	ppl: 3.9631240367889404
[eval_FOMC loss, ppl] step:33.25, 	loss: 0.5070077180862427, 	ppl: 1.5372856855392456
[eval_NumGLUEcm loss, ppl] step:33.25, 	loss: 3.70513653755188, 	ppl: 60.2248420715332
[eval_ScienceQA loss, ppl] step:33.25, 	loss: 0.9882357120513916, 	ppl: 2.6697096824645996
[eval_NumGLUEds loss, ppl] step:33.25, 	loss: 3.446340799331665, 	ppl: 68.7356185913086
[eval_Py150 loss, ppl] step:33.25, 	loss: 2.732407808303833, 	ppl: 17.226892471313477
[eval_MeetingBank loss, ppl] step:33.25, 	loss: 1.3282526731491089, 	ppl: 3.424187660217285
***** Evaluating perplexity, Epoch 3/5, step 3.0 *****
[eval loss, ppl] step:34.25, 	loss: 0.9861042499542236, 	ppl: 2.7586817741394043
[eval_CSTANCE loss, ppl] step:34.25, 	loss: 0.5377295613288879, 	ppl: 2.190718650817871
[eval_20Minuten loss, ppl] step:34.25, 	loss: 1.3497447967529297, 	ppl: 3.9626386165618896
[eval_FOMC loss, ppl] step:34.25, 	loss: 0.5162471532821655, 	ppl: 1.5354490280151367
[eval_NumGLUEcm loss, ppl] step:34.25, 	loss: 3.7085623741149902, 	ppl: 60.278934478759766
[eval_ScienceQA loss, ppl] step:34.25, 	loss: 0.9838117957115173, 	ppl: 2.6555771827697754
[eval_NumGLUEds loss, ppl] step:34.25, 	loss: 3.484084367752075, 	ppl: 70.58706665039062
[eval_Py150 loss, ppl] step:34.25, 	loss: 2.7277510166168213, 	ppl: 17.163471221923828
[eval_MeetingBank loss, ppl] step:34.25, 	loss: 1.328171968460083, 	ppl: 3.4262170791625977
***** Evaluating perplexity, Epoch 3/5, step 4.0 *****
[eval loss, ppl] step:35.25, 	loss: 0.9792843461036682, 	ppl: 2.7365806102752686
[eval_CSTANCE loss, ppl] step:35.25, 	loss: 0.5380585789680481, 	ppl: 2.1893529891967773
[eval_20Minuten loss, ppl] step:35.25, 	loss: 1.3486459255218506, 	ppl: 3.963590383529663
[eval_FOMC loss, ppl] step:35.25, 	loss: 0.5080731511116028, 	ppl: 1.535504698753357
[eval_NumGLUEcm loss, ppl] step:35.25, 	loss: 3.712449550628662, 	ppl: 61.685546875
[eval_ScienceQA loss, ppl] step:35.25, 	loss: 0.9787415862083435, 	ppl: 2.6395232677459717
[eval_NumGLUEds loss, ppl] step:35.25, 	loss: 3.504673480987549, 	ppl: 71.32992553710938
[eval_Py150 loss, ppl] step:35.25, 	loss: 2.7355427742004395, 	ppl: 17.220230102539062
[eval_MeetingBank loss, ppl] step:35.25, 	loss: 1.3281023502349854, 	ppl: 3.4261786937713623
***** Evaluating perplexity, Epoch 3/5, step 5.0 *****
[eval loss, ppl] step:36.25, 	loss: 0.9721637964248657, 	ppl: 2.711906909942627
[eval_CSTANCE loss, ppl] step:36.25, 	loss: 0.5347909927368164, 	ppl: 2.2001571655273438
[eval_20Minuten loss, ppl] step:36.25, 	loss: 1.3460725545883179, 	ppl: 3.9566972255706787
[eval_FOMC loss, ppl] step:36.25, 	loss: 0.5055925250053406, 	ppl: 1.5377908945083618
[eval_NumGLUEcm loss, ppl] step:36.25, 	loss: 3.7072012424468994, 	ppl: 61.638214111328125
[eval_ScienceQA loss, ppl] step:36.25, 	loss: 0.9723328351974487, 	ppl: 2.622361898422241
[eval_NumGLUEds loss, ppl] step:36.25, 	loss: 3.5023069381713867, 	ppl: 70.8876953125
[eval_Py150 loss, ppl] step:36.25, 	loss: 2.736851692199707, 	ppl: 17.244688034057617
[eval_MeetingBank loss, ppl] step:36.25, 	loss: 1.3285423517227173, 	ppl: 3.424710273742676
***** Evaluating perplexity, Epoch 3/5, step 6.0 *****
[eval loss, ppl] step:37.25, 	loss: 0.9654157161712646, 	ppl: 2.6929941177368164
[eval_CSTANCE loss, ppl] step:37.25, 	loss: 0.539374828338623, 	ppl: 2.1995949745178223
[eval_20Minuten loss, ppl] step:37.25, 	loss: 1.3466461896896362, 	ppl: 3.9586527347564697
[eval_FOMC loss, ppl] step:37.25, 	loss: 0.5072470307350159, 	ppl: 1.5343410968780518
[eval_NumGLUEcm loss, ppl] step:37.25, 	loss: 3.690340280532837, 	ppl: 61.961570739746094
[eval_ScienceQA loss, ppl] step:37.25, 	loss: 0.9661688208580017, 	ppl: 2.603426218032837
[eval_NumGLUEds loss, ppl] step:37.25, 	loss: 3.500267267227173, 	ppl: 72.49476623535156
[eval_Py150 loss, ppl] step:37.25, 	loss: 2.738699197769165, 	ppl: 17.231060028076172
[eval_MeetingBank loss, ppl] step:37.25, 	loss: 1.3296983242034912, 	ppl: 3.4285736083984375
***** Evaluating perplexity, Epoch 3/5, step 7.0 *****
[eval loss, ppl] step:38.25, 	loss: 0.9584254622459412, 	ppl: 2.6738343238830566
[eval_CSTANCE loss, ppl] step:38.25, 	loss: 0.5461530685424805, 	ppl: 2.197866201400757
[eval_20Minuten loss, ppl] step:38.25, 	loss: 1.3475327491760254, 	ppl: 3.9598288536071777
[eval_FOMC loss, ppl] step:38.25, 	loss: 0.5139516592025757, 	ppl: 1.539491057395935
[eval_NumGLUEcm loss, ppl] step:38.25, 	loss: 3.69683837890625, 	ppl: 61.76261520385742
[eval_ScienceQA loss, ppl] step:38.25, 	loss: 0.9598784446716309, 	ppl: 2.5869531631469727
[eval_NumGLUEds loss, ppl] step:38.25, 	loss: 3.493955373764038, 	ppl: 71.3937759399414
[eval_Py150 loss, ppl] step:38.25, 	loss: 2.7435243129730225, 	ppl: 17.269744873046875
[eval_MeetingBank loss, ppl] step:38.25, 	loss: 1.329872488975525, 	ppl: 3.4278383255004883
[2025-09-25 00:46:09,283] [INFO] [logging.py:107:log_dist] [Rank 0] step=40, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-09-25 00:46:10,067] [INFO] [timer.py:264:stop] epoch=0/micro_step=320/global_step=40, RunningAvgSamplesPerSec=1.2817938012537318, CurrSamplesPerSec=1.2075376789391963, MemAllocated=30.4GB, MaxMemAllocated=46.74GB
***** Evaluating perplexity, Epoch 3/5, step 8.0 *****
[eval loss, ppl] step:39.25, 	loss: 0.9519164562225342, 	ppl: 2.6561665534973145
[eval_CSTANCE loss, ppl] step:39.25, 	loss: 0.5515785217285156, 	ppl: 2.199172258377075
[eval_20Minuten loss, ppl] step:39.25, 	loss: 1.3477849960327148, 	ppl: 3.9572834968566895
[eval_FOMC loss, ppl] step:39.25, 	loss: 0.5167551040649414, 	ppl: 1.5406073331832886
[eval_NumGLUEcm loss, ppl] step:39.25, 	loss: 3.6506500244140625, 	ppl: 60.43318557739258
[eval_ScienceQA loss, ppl] step:39.25, 	loss: 0.9547766447067261, 	ppl: 2.5726089477539062
[eval_NumGLUEds loss, ppl] step:39.25, 	loss: 3.5118539333343506, 	ppl: 72.14543914794922
[eval_Py150 loss, ppl] step:39.25, 	loss: 2.7393484115600586, 	ppl: 17.318248748779297
[eval_MeetingBank loss, ppl] step:39.25, 	loss: 1.3298317193984985, 	ppl: 3.4277873039245605
***** Evaluating perplexity, Epoch 3/5, step 9.0 *****
[eval loss, ppl] step:40.25, 	loss: 0.9450852274894714, 	ppl: 2.637225866317749
[eval_CSTANCE loss, ppl] step:40.25, 	loss: 0.542380690574646, 	ppl: 2.190640449523926
[eval_20Minuten loss, ppl] step:40.25, 	loss: 1.3471052646636963, 	ppl: 3.955982208251953
[eval_FOMC loss, ppl] step:40.25, 	loss: 0.5195827484130859, 	ppl: 1.5461961030960083
[eval_NumGLUEcm loss, ppl] step:40.25, 	loss: 3.636488676071167, 	ppl: 59.080177307128906
[eval_ScienceQA loss, ppl] step:40.25, 	loss: 0.9483468532562256, 	ppl: 2.55403208732605
[eval_NumGLUEds loss, ppl] step:40.25, 	loss: 3.4932234287261963, 	ppl: 70.69856262207031
[eval_Py150 loss, ppl] step:40.25, 	loss: 2.7427525520324707, 	ppl: 17.31865882873535
[eval_MeetingBank loss, ppl] step:40.25, 	loss: 1.3299301862716675, 	ppl: 3.427642345428467
***** Evaluating perplexity, Epoch 3/5, step 10.0 *****
[eval loss, ppl] step:41.25, 	loss: 0.9379481673240662, 	ppl: 2.6126606464385986
[eval_CSTANCE loss, ppl] step:41.25, 	loss: 0.550417423248291, 	ppl: 2.205683469772339
[eval_20Minuten loss, ppl] step:41.25, 	loss: 1.3462024927139282, 	ppl: 3.9582467079162598
[eval_FOMC loss, ppl] step:41.25, 	loss: 0.5143855810165405, 	ppl: 1.5393402576446533
[eval_NumGLUEcm loss, ppl] step:41.25, 	loss: 3.5857861042022705, 	ppl: 56.7136116027832
[eval_ScienceQA loss, ppl] step:41.25, 	loss: 0.9410387277603149, 	ppl: 2.5370354652404785
[eval_NumGLUEds loss, ppl] step:41.25, 	loss: 3.49423885345459, 	ppl: 70.43636322021484
[eval_Py150 loss, ppl] step:41.25, 	loss: 2.743448257446289, 	ppl: 17.366342544555664
[eval_MeetingBank loss, ppl] step:41.25, 	loss: 1.3296622037887573, 	ppl: 3.431588649749756
***** Evaluating perplexity, Epoch 3/5, step 11.0 *****
[eval loss, ppl] step:42.25, 	loss: 0.932296097278595, 	ppl: 2.5928256511688232
[eval_CSTANCE loss, ppl] step:42.25, 	loss: 0.5484285354614258, 	ppl: 2.2121527194976807
[eval_20Minuten loss, ppl] step:42.25, 	loss: 1.3455564975738525, 	ppl: 3.957357168197632
[eval_FOMC loss, ppl] step:42.25, 	loss: 0.5188888311386108, 	ppl: 1.5412688255310059
[eval_NumGLUEcm loss, ppl] step:42.25, 	loss: 3.5117108821868896, 	ppl: 54.430908203125
[eval_ScienceQA loss, ppl] step:42.25, 	loss: 0.9358964562416077, 	ppl: 2.523359775543213
[eval_NumGLUEds loss, ppl] step:42.25, 	loss: 3.4595842361450195, 	ppl: 69.16297912597656
[eval_Py150 loss, ppl] step:42.25, 	loss: 2.7378129959106445, 	ppl: 17.187536239624023
[eval_MeetingBank loss, ppl] step:42.25, 	loss: 1.3301199674606323, 	ppl: 3.432905435562134
***** Evaluating perplexity, Epoch 3/5, step 12.0 *****
[eval loss, ppl] step:43.25, 	loss: 0.9272549152374268, 	ppl: 2.5753326416015625
[eval_CSTANCE loss, ppl] step:43.25, 	loss: 0.5561434030532837, 	ppl: 2.208850622177124
[eval_20Minuten loss, ppl] step:43.25, 	loss: 1.3472847938537598, 	ppl: 3.9579567909240723
[eval_FOMC loss, ppl] step:43.25, 	loss: 0.5252909064292908, 	ppl: 1.542657732963562
[eval_NumGLUEcm loss, ppl] step:43.25, 	loss: 3.5020205974578857, 	ppl: 51.733516693115234
[eval_ScienceQA loss, ppl] step:43.25, 	loss: 0.930277943611145, 	ppl: 2.5102930068969727
[eval_NumGLUEds loss, ppl] step:43.25, 	loss: 3.4329779148101807, 	ppl: 68.25070190429688
[eval_Py150 loss, ppl] step:43.25, 	loss: 2.7370595932006836, 	ppl: 17.289268493652344
[eval_MeetingBank loss, ppl] step:43.25, 	loss: 1.3312214612960815, 	ppl: 3.434896469116211
***** Evaluating perplexity, Epoch 3/5, step 13.0 *****
[eval loss, ppl] step:44.25, 	loss: 0.9220712780952454, 	ppl: 2.5611000061035156
[eval_CSTANCE loss, ppl] step:44.25, 	loss: 0.5560706257820129, 	ppl: 2.2213728427886963
[eval_20Minuten loss, ppl] step:44.25, 	loss: 1.3467369079589844, 	ppl: 3.9539339542388916
[eval_FOMC loss, ppl] step:44.25, 	loss: 0.5248709321022034, 	ppl: 1.5422518253326416
[eval_NumGLUEcm loss, ppl] step:44.25, 	loss: 3.4502182006835938, 	ppl: 50.38679504394531
[eval_ScienceQA loss, ppl] step:44.25, 	loss: 0.9241136908531189, 	ppl: 2.495464563369751
[eval_NumGLUEds loss, ppl] step:44.25, 	loss: 3.424726963043213, 	ppl: 68.08087158203125
[eval_Py150 loss, ppl] step:44.25, 	loss: 2.734745502471924, 	ppl: 17.28272247314453
[eval_MeetingBank loss, ppl] step:44.25, 	loss: 1.3310307264328003, 	ppl: 3.434699773788452
***** Evaluating perplexity, Epoch 3/5, step 14.0 *****
[eval loss, ppl] step:45.25, 	loss: 0.9158841967582703, 	ppl: 2.542189598083496
[eval_CSTANCE loss, ppl] step:45.25, 	loss: 0.5533057451248169, 	ppl: 2.206596851348877
[eval_20Minuten loss, ppl] step:45.25, 	loss: 1.347949743270874, 	ppl: 3.954392910003662
[eval_FOMC loss, ppl] step:45.25, 	loss: 0.5361827611923218, 	ppl: 1.5510591268539429
[eval_NumGLUEcm loss, ppl] step:45.25, 	loss: 3.405850887298584, 	ppl: 47.2794189453125
[eval_ScienceQA loss, ppl] step:45.25, 	loss: 0.9173446893692017, 	ppl: 2.4790163040161133
[eval_NumGLUEds loss, ppl] step:45.25, 	loss: 3.413849115371704, 	ppl: 66.62088775634766
[eval_Py150 loss, ppl] step:45.25, 	loss: 2.7325243949890137, 	ppl: 17.297067642211914
[eval_MeetingBank loss, ppl] step:45.25, 	loss: 1.3316738605499268, 	ppl: 3.4357922077178955
saving model to /data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/3...
Sucessful saving model after epoch 3
Beginning of Epoch 4/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 4/5, step 0.0 *****
[eval loss, ppl] step:46.875, 	loss: 0.9086783528327942, 	ppl: 2.526911735534668
[eval_CSTANCE loss, ppl] step:46.875, 	loss: 0.5485071539878845, 	ppl: 2.2215628623962402
[eval_20Minuten loss, ppl] step:46.875, 	loss: 1.3449907302856445, 	ppl: 3.9521849155426025
[eval_FOMC loss, ppl] step:46.875, 	loss: 0.5287647247314453, 	ppl: 1.546179175376892
[eval_NumGLUEcm loss, ppl] step:46.875, 	loss: 3.3961079120635986, 	ppl: 47.32474899291992
[eval_ScienceQA loss, ppl] step:46.875, 	loss: 0.9123321771621704, 	ppl: 2.461552143096924
[eval_NumGLUEds loss, ppl] step:46.875, 	loss: 3.4110186100006104, 	ppl: 67.70651245117188
[eval_Py150 loss, ppl] step:46.875, 	loss: 2.7394800186157227, 	ppl: 17.32748031616211
[eval_MeetingBank loss, ppl] step:46.875, 	loss: 1.3306862115859985, 	ppl: 3.436394453048706
***** Evaluating perplexity, Epoch 4/5, step 1.0 *****
[eval loss, ppl] step:47.875, 	loss: 0.9033879041671753, 	ppl: 2.5126240253448486
[eval_CSTANCE loss, ppl] step:47.875, 	loss: 0.5589613914489746, 	ppl: 2.231574535369873
[eval_20Minuten loss, ppl] step:47.875, 	loss: 1.3448683023452759, 	ppl: 3.952535629272461
[eval_FOMC loss, ppl] step:47.875, 	loss: 0.5320451855659485, 	ppl: 1.5463061332702637
[eval_NumGLUEcm loss, ppl] step:47.875, 	loss: 3.3782451152801514, 	ppl: 46.74709701538086
[eval_ScienceQA loss, ppl] step:47.875, 	loss: 0.9053629040718079, 	ppl: 2.4483017921447754
[eval_NumGLUEds loss, ppl] step:47.875, 	loss: 3.3886725902557373, 	ppl: 66.83097839355469
[eval_Py150 loss, ppl] step:47.875, 	loss: 2.7342474460601807, 	ppl: 17.324172973632812
[eval_MeetingBank loss, ppl] step:47.875, 	loss: 1.3318504095077515, 	ppl: 3.438687801361084
***** Evaluating perplexity, Epoch 4/5, step 2.0 *****
[eval loss, ppl] step:48.875, 	loss: 0.9006094932556152, 	ppl: 2.504903554916382
[eval_CSTANCE loss, ppl] step:48.875, 	loss: 0.562826931476593, 	ppl: 2.2288830280303955
[eval_20Minuten loss, ppl] step:48.875, 	loss: 1.3463525772094727, 	ppl: 3.952920913696289
[eval_FOMC loss, ppl] step:48.875, 	loss: 0.5445212721824646, 	ppl: 1.5502650737762451
[eval_NumGLUEcm loss, ppl] step:48.875, 	loss: 3.399782419204712, 	ppl: 47.89747619628906
[eval_ScienceQA loss, ppl] step:48.875, 	loss: 0.9003148078918457, 	ppl: 2.436391830444336
[eval_NumGLUEds loss, ppl] step:48.875, 	loss: 3.388753890991211, 	ppl: 67.30680084228516
[eval_Py150 loss, ppl] step:48.875, 	loss: 2.735671043395996, 	ppl: 17.283071517944336
[eval_MeetingBank loss, ppl] step:48.875, 	loss: 1.3322199583053589, 	ppl: 3.438441276550293
[2025-09-25 00:59:48,698] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-09-25 00:59:49,348] [INFO] [timer.py:264:stop] epoch=0/micro_step=400/global_step=50, RunningAvgSamplesPerSec=1.266436133977411, CurrSamplesPerSec=1.2398216799150306, MemAllocated=30.3GB, MaxMemAllocated=46.74GB
***** Evaluating perplexity, Epoch 4/5, step 3.0 *****
[eval loss, ppl] step:49.875, 	loss: 0.8956080079078674, 	ppl: 2.48907470703125
[eval_CSTANCE loss, ppl] step:49.875, 	loss: 0.5610446333885193, 	ppl: 2.2235162258148193
[eval_20Minuten loss, ppl] step:49.875, 	loss: 1.3460044860839844, 	ppl: 3.9512526988983154
[eval_FOMC loss, ppl] step:49.875, 	loss: 0.5517873764038086, 	ppl: 1.5587921142578125
[eval_NumGLUEcm loss, ppl] step:49.875, 	loss: 3.4119420051574707, 	ppl: 48.93799591064453
[eval_ScienceQA loss, ppl] step:49.875, 	loss: 0.8944555521011353, 	ppl: 2.4195356369018555
[eval_NumGLUEds loss, ppl] step:49.875, 	loss: 3.4307444095611572, 	ppl: 68.43357849121094
[eval_Py150 loss, ppl] step:49.875, 	loss: 2.734861373901367, 	ppl: 17.33088493347168
[eval_MeetingBank loss, ppl] step:49.875, 	loss: 1.3314703702926636, 	ppl: 3.436608076095581
***** Evaluating perplexity, Epoch 4/5, step 4.0 *****
[eval loss, ppl] step:50.875, 	loss: 0.8900187611579895, 	ppl: 2.467212200164795
[eval_CSTANCE loss, ppl] step:50.875, 	loss: 0.558607280254364, 	ppl: 2.223910093307495
[eval_20Minuten loss, ppl] step:50.875, 	loss: 1.343784213066101, 	ppl: 3.9481022357940674
[eval_FOMC loss, ppl] step:50.875, 	loss: 0.5515326857566833, 	ppl: 1.5636484622955322
[eval_NumGLUEcm loss, ppl] step:50.875, 	loss: 3.3982856273651123, 	ppl: 47.96455383300781
[eval_ScienceQA loss, ppl] step:50.875, 	loss: 0.888335645198822, 	ppl: 2.4055638313293457
[eval_NumGLUEds loss, ppl] step:50.875, 	loss: 3.4121525287628174, 	ppl: 68.66838073730469
[eval_Py150 loss, ppl] step:50.875, 	loss: 2.7306039333343506, 	ppl: 17.295488357543945
[eval_MeetingBank loss, ppl] step:50.875, 	loss: 1.3346222639083862, 	ppl: 3.441934108734131
***** Evaluating perplexity, Epoch 4/5, step 5.0 *****
[eval loss, ppl] step:51.875, 	loss: 0.8839324712753296, 	ppl: 2.44575834274292
[eval_CSTANCE loss, ppl] step:51.875, 	loss: 0.5673660039901733, 	ppl: 2.229534149169922
[eval_20Minuten loss, ppl] step:51.875, 	loss: 1.3461281061172485, 	ppl: 3.9513139724731445
[eval_FOMC loss, ppl] step:51.875, 	loss: 0.5531673431396484, 	ppl: 1.5665463209152222
[eval_NumGLUEcm loss, ppl] step:51.875, 	loss: 3.4242734909057617, 	ppl: 49.69865417480469
[eval_ScienceQA loss, ppl] step:51.875, 	loss: 0.8828394412994385, 	ppl: 2.3915207386016846
[eval_NumGLUEds loss, ppl] step:51.875, 	loss: 3.4367258548736572, 	ppl: 69.96442413330078
[eval_Py150 loss, ppl] step:51.875, 	loss: 2.7366294860839844, 	ppl: 17.320247650146484
[eval_MeetingBank loss, ppl] step:51.875, 	loss: 1.332078218460083, 	ppl: 3.442171573638916
***** Evaluating perplexity, Epoch 4/5, step 6.0 *****
[eval loss, ppl] step:52.875, 	loss: 0.8770937323570251, 	ppl: 2.425292491912842
[eval_CSTANCE loss, ppl] step:52.875, 	loss: 0.5516643524169922, 	ppl: 2.220174789428711
[eval_20Minuten loss, ppl] step:52.875, 	loss: 1.3457691669464111, 	ppl: 3.948340654373169
[eval_FOMC loss, ppl] step:52.875, 	loss: 0.5604962110519409, 	ppl: 1.5750750303268433
[eval_NumGLUEcm loss, ppl] step:52.875, 	loss: 3.4343459606170654, 	ppl: 50.35104751586914
[eval_ScienceQA loss, ppl] step:52.875, 	loss: 0.876925528049469, 	ppl: 2.3762834072113037
[eval_NumGLUEds loss, ppl] step:52.875, 	loss: 3.467175006866455, 	ppl: 71.06776428222656
[eval_Py150 loss, ppl] step:52.875, 	loss: 2.73366379737854, 	ppl: 17.23388671875
[eval_MeetingBank loss, ppl] step:52.875, 	loss: 1.3336224555969238, 	ppl: 3.442342758178711
***** Evaluating perplexity, Epoch 4/5, step 7.0 *****
[eval loss, ppl] step:53.875, 	loss: 0.8719294667243958, 	ppl: 2.4094972610473633
[eval_CSTANCE loss, ppl] step:53.875, 	loss: 0.5538626909255981, 	ppl: 2.222198486328125
[eval_20Minuten loss, ppl] step:53.875, 	loss: 1.3433181047439575, 	ppl: 3.9470760822296143
[eval_FOMC loss, ppl] step:53.875, 	loss: 0.5605182647705078, 	ppl: 1.5794434547424316
[eval_NumGLUEcm loss, ppl] step:53.875, 	loss: 3.4288299083709717, 	ppl: 50.10868835449219
[eval_ScienceQA loss, ppl] step:53.875, 	loss: 0.871680498123169, 	ppl: 2.3634703159332275
[eval_NumGLUEds loss, ppl] step:53.875, 	loss: 3.495241641998291, 	ppl: 72.08575439453125
[eval_Py150 loss, ppl] step:53.875, 	loss: 2.726572036743164, 	ppl: 17.307540893554688
[eval_MeetingBank loss, ppl] step:53.875, 	loss: 1.3325315713882446, 	ppl: 3.4424033164978027
***** Evaluating perplexity, Epoch 4/5, step 8.0 *****
[eval loss, ppl] step:54.875, 	loss: 0.8687079548835754, 	ppl: 2.4050486087799072
[eval_CSTANCE loss, ppl] step:54.875, 	loss: 0.553267776966095, 	ppl: 2.2270452976226807
[eval_20Minuten loss, ppl] step:54.875, 	loss: 1.3435163497924805, 	ppl: 3.9471664428710938
[eval_FOMC loss, ppl] step:54.875, 	loss: 0.560229480266571, 	ppl: 1.581270456314087
[eval_NumGLUEcm loss, ppl] step:54.875, 	loss: 3.4346847534179688, 	ppl: 51.047027587890625
[eval_ScienceQA loss, ppl] step:54.875, 	loss: 0.8670902848243713, 	ppl: 2.354508399963379
[eval_NumGLUEds loss, ppl] step:54.875, 	loss: 3.5131285190582275, 	ppl: 74.14974212646484
[eval_Py150 loss, ppl] step:54.875, 	loss: 2.730839729309082, 	ppl: 17.366317749023438
[eval_MeetingBank loss, ppl] step:54.875, 	loss: 1.3342205286026, 	ppl: 3.4446206092834473
***** Evaluating perplexity, Epoch 4/5, step 9.0 *****
[eval loss, ppl] step:55.875, 	loss: 0.864607572555542, 	ppl: 2.387284278869629
[eval_CSTANCE loss, ppl] step:55.875, 	loss: 0.5496543049812317, 	ppl: 2.235743284225464
[eval_20Minuten loss, ppl] step:55.875, 	loss: 1.3427060842514038, 	ppl: 3.9419937133789062
[eval_FOMC loss, ppl] step:55.875, 	loss: 0.5631389617919922, 	ppl: 1.586154818534851
[eval_NumGLUEcm loss, ppl] step:55.875, 	loss: 3.442030191421509, 	ppl: 49.93448257446289
[eval_ScienceQA loss, ppl] step:55.875, 	loss: 0.8634220361709595, 	ppl: 2.346224546432495
[eval_NumGLUEds loss, ppl] step:55.875, 	loss: 3.4831490516662598, 	ppl: 73.21923065185547
[eval_Py150 loss, ppl] step:55.875, 	loss: 2.7375457286834717, 	ppl: 17.36756134033203
[eval_MeetingBank loss, ppl] step:55.875, 	loss: 1.3350069522857666, 	ppl: 3.447648763656616
***** Evaluating perplexity, Epoch 4/5, step 10.0 *****
[eval loss, ppl] step:56.875, 	loss: 0.8583047389984131, 	ppl: 2.365849494934082
[eval_CSTANCE loss, ppl] step:56.875, 	loss: 0.5498859286308289, 	ppl: 2.2453324794769287
[eval_20Minuten loss, ppl] step:56.875, 	loss: 1.342433214187622, 	ppl: 3.938781976699829
[eval_FOMC loss, ppl] step:56.875, 	loss: 0.5722537636756897, 	ppl: 1.5909708738327026
[eval_NumGLUEcm loss, ppl] step:56.875, 	loss: 3.4272122383117676, 	ppl: 50.41075897216797
[eval_ScienceQA loss, ppl] step:56.875, 	loss: 0.8577937483787537, 	ppl: 2.334310293197632
[eval_NumGLUEds loss, ppl] step:56.875, 	loss: 3.474264621734619, 	ppl: 72.31988525390625
[eval_Py150 loss, ppl] step:56.875, 	loss: 2.7277348041534424, 	ppl: 17.376949310302734
[eval_MeetingBank loss, ppl] step:56.875, 	loss: 1.3347746133804321, 	ppl: 3.450133800506592
***** Evaluating perplexity, Epoch 4/5, step 11.0 *****
[eval loss, ppl] step:57.875, 	loss: 0.8518173098564148, 	ppl: 2.349271297454834
[eval_CSTANCE loss, ppl] step:57.875, 	loss: 0.549251914024353, 	ppl: 2.2419824600219727
[eval_20Minuten loss, ppl] step:57.875, 	loss: 1.3421614170074463, 	ppl: 3.9384193420410156
[eval_FOMC loss, ppl] step:57.875, 	loss: 0.5664095282554626, 	ppl: 1.5908477306365967
[eval_NumGLUEcm loss, ppl] step:57.875, 	loss: 3.4189634323120117, 	ppl: 49.631072998046875
[eval_ScienceQA loss, ppl] step:57.875, 	loss: 0.8519126772880554, 	ppl: 2.3184008598327637
[eval_NumGLUEds loss, ppl] step:57.875, 	loss: 3.48359751701355, 	ppl: 72.58317565917969
[eval_Py150 loss, ppl] step:57.875, 	loss: 2.727206230163574, 	ppl: 17.36036491394043
[eval_MeetingBank loss, ppl] step:57.875, 	loss: 1.3345006704330444, 	ppl: 3.4506161212921143
***** Evaluating perplexity, Epoch 4/5, step 12.0 *****
[eval loss, ppl] step:58.875, 	loss: 0.8452523946762085, 	ppl: 2.332810878753662
[eval_CSTANCE loss, ppl] step:58.875, 	loss: 0.5520652532577515, 	ppl: 2.2487900257110596
[eval_20Minuten loss, ppl] step:58.875, 	loss: 1.3429622650146484, 	ppl: 3.940488338470459
[eval_FOMC loss, ppl] step:58.875, 	loss: 0.5718098878860474, 	ppl: 1.6001205444335938
[eval_NumGLUEcm loss, ppl] step:58.875, 	loss: 3.404099225997925, 	ppl: 49.85707092285156
[eval_ScienceQA loss, ppl] step:58.875, 	loss: 0.8435975313186646, 	ppl: 2.300259590148926
[eval_NumGLUEds loss, ppl] step:58.875, 	loss: 3.470947027206421, 	ppl: 72.67119598388672
[eval_Py150 loss, ppl] step:58.875, 	loss: 2.7330851554870605, 	ppl: 17.44291114807129
[eval_MeetingBank loss, ppl] step:58.875, 	loss: 1.3349906206130981, 	ppl: 3.453890323638916
[2025-09-25 01:12:35,055] [INFO] [logging.py:107:log_dist] [Rank 0] step=60, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-09-25 01:12:35,936] [INFO] [timer.py:264:stop] epoch=0/micro_step=480/global_step=60, RunningAvgSamplesPerSec=1.2677828453498956, CurrSamplesPerSec=1.3366946318481279, MemAllocated=30.51GB, MaxMemAllocated=46.74GB
***** Evaluating perplexity, Epoch 4/5, step 13.0 *****
[eval loss, ppl] step:59.875, 	loss: 0.8396094441413879, 	ppl: 2.3198349475860596
[eval_CSTANCE loss, ppl] step:59.875, 	loss: 0.5523015260696411, 	ppl: 2.2468903064727783
[eval_20Minuten loss, ppl] step:59.875, 	loss: 1.34109365940094, 	ppl: 3.939821720123291
[eval_FOMC loss, ppl] step:59.875, 	loss: 0.5933898687362671, 	ppl: 1.6085546016693115
[eval_NumGLUEcm loss, ppl] step:59.875, 	loss: 3.3770079612731934, 	ppl: 49.04893493652344
[eval_ScienceQA loss, ppl] step:59.875, 	loss: 0.8369079828262329, 	ppl: 2.285975456237793
[eval_NumGLUEds loss, ppl] step:59.875, 	loss: 3.4327454566955566, 	ppl: 70.75330352783203
[eval_Py150 loss, ppl] step:59.875, 	loss: 2.7419135570526123, 	ppl: 17.495281219482422
[eval_MeetingBank loss, ppl] step:59.875, 	loss: 1.3367159366607666, 	ppl: 3.4569053649902344
***** Evaluating perplexity, Epoch 4/5, step 14.0 *****
[eval loss, ppl] step:60.875, 	loss: 0.8358455300331116, 	ppl: 2.3118081092834473
[eval_CSTANCE loss, ppl] step:60.875, 	loss: 0.5505093336105347, 	ppl: 2.2530977725982666
[eval_20Minuten loss, ppl] step:60.875, 	loss: 1.3420929908752441, 	ppl: 3.9421281814575195
[eval_FOMC loss, ppl] step:60.875, 	loss: 0.5838805437088013, 	ppl: 1.6098239421844482
[eval_NumGLUEcm loss, ppl] step:60.875, 	loss: 3.3613409996032715, 	ppl: 47.82822036743164
[eval_ScienceQA loss, ppl] step:60.875, 	loss: 0.8320467472076416, 	ppl: 2.272339105606079
[eval_NumGLUEds loss, ppl] step:60.875, 	loss: 3.4112565517425537, 	ppl: 70.32804870605469
[eval_Py150 loss, ppl] step:60.875, 	loss: 2.7310609817504883, 	ppl: 17.48712158203125
[eval_MeetingBank loss, ppl] step:60.875, 	loss: 1.3371810913085938, 	ppl: 3.4528205394744873
saving model to /data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/4...
Sucessful saving model after epoch 4
Beginning of Epoch 5/5, Total Micro Batches 125
***** Evaluating perplexity, Epoch 5/5, step 0.0 *****
[eval loss, ppl] step:62.5, 	loss: 0.8288107514381409, 	ppl: 2.297597646713257
[eval_CSTANCE loss, ppl] step:62.5, 	loss: 0.5382063984870911, 	ppl: 2.245234489440918
[eval_20Minuten loss, ppl] step:62.5, 	loss: 1.3438501358032227, 	ppl: 3.9473204612731934
[eval_FOMC loss, ppl] step:62.5, 	loss: 0.6014984846115112, 	ppl: 1.6218174695968628
[eval_NumGLUEcm loss, ppl] step:62.5, 	loss: 3.3110904693603516, 	ppl: 45.311195373535156
[eval_ScienceQA loss, ppl] step:62.5, 	loss: 0.8249434232711792, 	ppl: 2.2533538341522217
[eval_NumGLUEds loss, ppl] step:62.5, 	loss: 3.3794453144073486, 	ppl: 69.13481903076172
[eval_Py150 loss, ppl] step:62.5, 	loss: 2.7400472164154053, 	ppl: 17.478479385375977
[eval_MeetingBank loss, ppl] step:62.5, 	loss: 1.336865782737732, 	ppl: 3.4523167610168457
***** Evaluating perplexity, Epoch 5/5, step 1.0 *****
[eval loss, ppl] step:63.5, 	loss: 0.8228150606155396, 	ppl: 2.2858238220214844
[eval_CSTANCE loss, ppl] step:63.5, 	loss: 0.5429673790931702, 	ppl: 2.2524495124816895
[eval_20Minuten loss, ppl] step:63.5, 	loss: 1.3436949253082275, 	ppl: 3.946789264678955
[eval_FOMC loss, ppl] step:63.5, 	loss: 0.5966513752937317, 	ppl: 1.6235554218292236
[eval_NumGLUEcm loss, ppl] step:63.5, 	loss: 3.3032145500183105, 	ppl: 45.059017181396484
[eval_ScienceQA loss, ppl] step:63.5, 	loss: 0.8204058408737183, 	ppl: 2.2420947551727295
[eval_NumGLUEds loss, ppl] step:63.5, 	loss: 3.411595106124878, 	ppl: 69.5728530883789
[eval_Py150 loss, ppl] step:63.5, 	loss: 2.7388756275177, 	ppl: 17.536026000976562
[eval_MeetingBank loss, ppl] step:63.5, 	loss: 1.3349376916885376, 	ppl: 3.451763391494751
***** Evaluating perplexity, Epoch 5/5, step 2.0 *****
[eval loss, ppl] step:64.5, 	loss: 0.817474365234375, 	ppl: 2.2763073444366455
[eval_CSTANCE loss, ppl] step:64.5, 	loss: 0.5452108979225159, 	ppl: 2.2509188652038574
[eval_20Minuten loss, ppl] step:64.5, 	loss: 1.3438091278076172, 	ppl: 3.9493184089660645
[eval_FOMC loss, ppl] step:64.5, 	loss: 0.5960394740104675, 	ppl: 1.6290971040725708
[eval_NumGLUEcm loss, ppl] step:64.5, 	loss: 3.2846007347106934, 	ppl: 45.2436408996582
[eval_ScienceQA loss, ppl] step:64.5, 	loss: 0.815479040145874, 	ppl: 2.2302017211914062
[eval_NumGLUEds loss, ppl] step:64.5, 	loss: 3.43497896194458, 	ppl: 71.17945098876953
[eval_Py150 loss, ppl] step:64.5, 	loss: 2.740635395050049, 	ppl: 17.556385040283203
[eval_MeetingBank loss, ppl] step:64.5, 	loss: 1.337577223777771, 	ppl: 3.4547247886657715
***** Evaluating perplexity, Epoch 5/5, step 3.0 *****
[eval loss, ppl] step:65.5, 	loss: 0.814069926738739, 	ppl: 2.268808364868164
[eval_CSTANCE loss, ppl] step:65.5, 	loss: 0.5496350526809692, 	ppl: 2.2604448795318604
[eval_20Minuten loss, ppl] step:65.5, 	loss: 1.3437318801879883, 	ppl: 3.9486305713653564
[eval_FOMC loss, ppl] step:65.5, 	loss: 0.6029070019721985, 	ppl: 1.632065773010254
[eval_NumGLUEcm loss, ppl] step:65.5, 	loss: 3.3038783073425293, 	ppl: 45.49185562133789
[eval_ScienceQA loss, ppl] step:65.5, 	loss: 0.8112146258354187, 	ppl: 2.2188920974731445
[eval_NumGLUEds loss, ppl] step:65.5, 	loss: 3.4561774730682373, 	ppl: 73.19110107421875
[eval_Py150 loss, ppl] step:65.5, 	loss: 2.7459075450897217, 	ppl: 17.664043426513672
[eval_MeetingBank loss, ppl] step:65.5, 	loss: 1.3364362716674805, 	ppl: 3.4550838470458984
***** Evaluating perplexity, Epoch 5/5, step 4.0 *****
[eval loss, ppl] step:66.5, 	loss: 0.8108906745910645, 	ppl: 2.2632179260253906
[eval_CSTANCE loss, ppl] step:66.5, 	loss: 0.5470950603485107, 	ppl: 2.2455945014953613
[eval_20Minuten loss, ppl] step:66.5, 	loss: 1.343660593032837, 	ppl: 3.949761152267456
[eval_FOMC loss, ppl] step:66.5, 	loss: 0.6007531881332397, 	ppl: 1.636298418045044
[eval_NumGLUEcm loss, ppl] step:66.5, 	loss: 3.315420150756836, 	ppl: 46.34632873535156
[eval_ScienceQA loss, ppl] step:66.5, 	loss: 0.8075366020202637, 	ppl: 2.2097458839416504
[eval_NumGLUEds loss, ppl] step:66.5, 	loss: 3.4840567111968994, 	ppl: 75.75189208984375
[eval_Py150 loss, ppl] step:66.5, 	loss: 2.746100664138794, 	ppl: 17.676509857177734
[eval_MeetingBank loss, ppl] step:66.5, 	loss: 1.337306022644043, 	ppl: 3.457350254058838
***** Evaluating perplexity, Epoch 5/5, step 5.0 *****
[eval loss, ppl] step:67.5, 	loss: 0.8072731494903564, 	ppl: 2.2574105262756348
[eval_CSTANCE loss, ppl] step:67.5, 	loss: 0.5502894520759583, 	ppl: 2.2488791942596436
[eval_20Minuten loss, ppl] step:67.5, 	loss: 1.3427594900131226, 	ppl: 3.9523680210113525
[eval_FOMC loss, ppl] step:67.5, 	loss: 0.6058666110038757, 	ppl: 1.6427736282348633
[eval_NumGLUEcm loss, ppl] step:67.5, 	loss: 3.338634490966797, 	ppl: 46.358638763427734
[eval_ScienceQA loss, ppl] step:67.5, 	loss: 0.8024100065231323, 	ppl: 2.198152542114258
[eval_NumGLUEds loss, ppl] step:67.5, 	loss: 3.530604600906372, 	ppl: 79.54878234863281
[eval_Py150 loss, ppl] step:67.5, 	loss: 2.749382734298706, 	ppl: 17.74985122680664
[eval_MeetingBank loss, ppl] step:67.5, 	loss: 1.3377563953399658, 	ppl: 3.4593446254730225
***** Evaluating perplexity, Epoch 5/5, step 6.0 *****
[eval loss, ppl] step:68.5, 	loss: 0.8030423521995544, 	ppl: 2.248663902282715
[eval_CSTANCE loss, ppl] step:68.5, 	loss: 0.5478175282478333, 	ppl: 2.2464632987976074
[eval_20Minuten loss, ppl] step:68.5, 	loss: 1.3430747985839844, 	ppl: 3.95163893699646
[eval_FOMC loss, ppl] step:68.5, 	loss: 0.6032294631004333, 	ppl: 1.644020915031433
[eval_NumGLUEcm loss, ppl] step:68.5, 	loss: 3.349926710128784, 	ppl: 47.035438537597656
[eval_ScienceQA loss, ppl] step:68.5, 	loss: 0.7971450090408325, 	ppl: 2.1876914501190186
[eval_NumGLUEds loss, ppl] step:68.5, 	loss: 3.5162720680236816, 	ppl: 80.21891021728516
[eval_Py150 loss, ppl] step:68.5, 	loss: 2.7532505989074707, 	ppl: 17.782337188720703
[eval_MeetingBank loss, ppl] step:68.5, 	loss: 1.3385595083236694, 	ppl: 3.45867657661438
[2025-09-25 01:25:20,072] [INFO] [logging.py:107:log_dist] [Rank 0] step=70, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]
[2025-09-25 01:25:20,917] [INFO] [timer.py:264:stop] epoch=0/micro_step=560/global_step=70, RunningAvgSamplesPerSec=1.2693708971085451, CurrSamplesPerSec=1.3465484104781853, MemAllocated=30.3GB, MaxMemAllocated=46.74GB
***** Evaluating perplexity, Epoch 5/5, step 7.0 *****
[eval loss, ppl] step:69.5, 	loss: 0.7982693314552307, 	ppl: 2.238773822784424
[eval_CSTANCE loss, ppl] step:69.5, 	loss: 0.5526515245437622, 	ppl: 2.254894256591797
[eval_20Minuten loss, ppl] step:69.5, 	loss: 1.3439886569976807, 	ppl: 3.955321788787842
[eval_FOMC loss, ppl] step:69.5, 	loss: 0.6048531532287598, 	ppl: 1.6408145427703857
[eval_NumGLUEcm loss, ppl] step:69.5, 	loss: 3.354321241378784, 	ppl: 46.41476058959961
[eval_ScienceQA loss, ppl] step:69.5, 	loss: 0.7917423248291016, 	ppl: 2.1753029823303223
[eval_NumGLUEds loss, ppl] step:69.5, 	loss: 3.571894645690918, 	ppl: 82.30424499511719
[eval_Py150 loss, ppl] step:69.5, 	loss: 2.7493813037872314, 	ppl: 17.893369674682617
[eval_MeetingBank loss, ppl] step:69.5, 	loss: 1.3385671377182007, 	ppl: 3.460646629333496
***** Evaluating perplexity, Epoch 5/5, step 8.0 *****
[eval loss, ppl] step:70.5, 	loss: 0.7938949465751648, 	ppl: 2.228180170059204
[eval_CSTANCE loss, ppl] step:70.5, 	loss: 0.5406619310379028, 	ppl: 2.2329108715057373
[eval_20Minuten loss, ppl] step:70.5, 	loss: 1.3450483083724976, 	ppl: 3.9552011489868164
[eval_FOMC loss, ppl] step:70.5, 	loss: 0.5969505310058594, 	ppl: 1.636812448501587
[eval_NumGLUEcm loss, ppl] step:70.5, 	loss: 3.3543660640716553, 	ppl: 46.9256591796875
[eval_ScienceQA loss, ppl] step:70.5, 	loss: 0.787526547908783, 	ppl: 2.166057586669922
[eval_NumGLUEds loss, ppl] step:70.5, 	loss: 3.6046743392944336, 	ppl: 83.49720764160156
[eval_Py150 loss, ppl] step:70.5, 	loss: 2.7497172355651855, 	ppl: 17.890995025634766
[eval_MeetingBank loss, ppl] step:70.5, 	loss: 1.3390343189239502, 	ppl: 3.459606409072876
***** Evaluating perplexity, Epoch 5/5, step 9.0 *****
[eval loss, ppl] step:71.5, 	loss: 0.7902436852455139, 	ppl: 2.2192745208740234
[eval_CSTANCE loss, ppl] step:71.5, 	loss: 0.5413656830787659, 	ppl: 2.2461516857147217
[eval_20Minuten loss, ppl] step:71.5, 	loss: 1.344464898109436, 	ppl: 3.9528615474700928
[eval_FOMC loss, ppl] step:71.5, 	loss: 0.5921526551246643, 	ppl: 1.6332812309265137
[eval_NumGLUEcm loss, ppl] step:71.5, 	loss: 3.334510326385498, 	ppl: 45.51478576660156
[eval_ScienceQA loss, ppl] step:71.5, 	loss: 0.7845025658607483, 	ppl: 2.1593565940856934
[eval_NumGLUEds loss, ppl] step:71.5, 	loss: 3.6025218963623047, 	ppl: 83.77611541748047
[eval_Py150 loss, ppl] step:71.5, 	loss: 2.756276845932007, 	ppl: 17.857677459716797
[eval_MeetingBank loss, ppl] step:71.5, 	loss: 1.3399730920791626, 	ppl: 3.4629974365234375
***** Evaluating perplexity, Epoch 5/5, step 10.0 *****
[eval loss, ppl] step:72.5, 	loss: 0.7861840724945068, 	ppl: 2.2113265991210938
[eval_CSTANCE loss, ppl] step:72.5, 	loss: 0.538106381893158, 	ppl: 2.2509279251098633
[eval_20Minuten loss, ppl] step:72.5, 	loss: 1.345328450202942, 	ppl: 3.958904266357422
[eval_FOMC loss, ppl] step:72.5, 	loss: 0.5894594192504883, 	ppl: 1.6316436529159546
[eval_NumGLUEcm loss, ppl] step:72.5, 	loss: 3.3260884284973145, 	ppl: 45.834877014160156
[eval_ScienceQA loss, ppl] step:72.5, 	loss: 0.780543863773346, 	ppl: 2.1491763591766357
[eval_NumGLUEds loss, ppl] step:72.5, 	loss: 3.5921308994293213, 	ppl: 85.34912109375
[eval_Py150 loss, ppl] step:72.5, 	loss: 2.7513184547424316, 	ppl: 17.877437591552734
[eval_MeetingBank loss, ppl] step:72.5, 	loss: 1.3395694494247437, 	ppl: 3.4646735191345215
***** Evaluating perplexity, Epoch 5/5, step 11.0 *****
[eval loss, ppl] step:73.5, 	loss: 0.7817598581314087, 	ppl: 2.2035326957702637
[eval_CSTANCE loss, ppl] step:73.5, 	loss: 0.5419869422912598, 	ppl: 2.254697799682617
[eval_20Minuten loss, ppl] step:73.5, 	loss: 1.3457995653152466, 	ppl: 3.960272789001465
[eval_FOMC loss, ppl] step:73.5, 	loss: 0.5889628529548645, 	ppl: 1.63088858127594
[eval_NumGLUEcm loss, ppl] step:73.5, 	loss: 3.303344249725342, 	ppl: 45.493465423583984
[eval_ScienceQA loss, ppl] step:73.5, 	loss: 0.7765124440193176, 	ppl: 2.140294075012207
[eval_NumGLUEds loss, ppl] step:73.5, 	loss: 3.5961239337921143, 	ppl: 85.79026794433594
[eval_Py150 loss, ppl] step:73.5, 	loss: 2.7495903968811035, 	ppl: 17.88982391357422
[eval_MeetingBank loss, ppl] step:73.5, 	loss: 1.3405617475509644, 	ppl: 3.4631686210632324
***** Evaluating perplexity, Epoch 5/5, step 12.0 *****
[eval loss, ppl] step:74.5, 	loss: 0.7776370048522949, 	ppl: 2.192830801010132
[eval_CSTANCE loss, ppl] step:74.5, 	loss: 0.5390304327011108, 	ppl: 2.250509023666382
[eval_20Minuten loss, ppl] step:74.5, 	loss: 1.3458020687103271, 	ppl: 3.9627819061279297
[eval_FOMC loss, ppl] step:74.5, 	loss: 0.5841582417488098, 	ppl: 1.6278386116027832
[eval_NumGLUEcm loss, ppl] step:74.5, 	loss: 3.298790693283081, 	ppl: 43.872276306152344
[eval_ScienceQA loss, ppl] step:74.5, 	loss: 0.7739667296409607, 	ppl: 2.1319501399993896
[eval_NumGLUEds loss, ppl] step:74.5, 	loss: 3.575505256652832, 	ppl: 83.733154296875
[eval_Py150 loss, ppl] step:74.5, 	loss: 2.7496120929718018, 	ppl: 17.852672576904297
[eval_MeetingBank loss, ppl] step:74.5, 	loss: 1.3389850854873657, 	ppl: 3.4605720043182373
***** Evaluating perplexity, Epoch 5/5, step 13.0 *****
[eval loss, ppl] step:75.5, 	loss: 0.7734308242797852, 	ppl: 2.1826658248901367
[eval_CSTANCE loss, ppl] step:75.5, 	loss: 0.5410429835319519, 	ppl: 2.2495665550231934
[eval_20Minuten loss, ppl] step:75.5, 	loss: 1.347610354423523, 	ppl: 3.963250160217285
[eval_FOMC loss, ppl] step:75.5, 	loss: 0.5792275071144104, 	ppl: 1.6209478378295898
[eval_NumGLUEcm loss, ppl] step:75.5, 	loss: 3.2682807445526123, 	ppl: 44.57825469970703
[eval_ScienceQA loss, ppl] step:75.5, 	loss: 0.7709259986877441, 	ppl: 2.1244020462036133
[eval_NumGLUEds loss, ppl] step:75.5, 	loss: 3.582653284072876, 	ppl: 84.27668762207031
[eval_Py150 loss, ppl] step:75.5, 	loss: 2.7523834705352783, 	ppl: 17.94914436340332
[eval_MeetingBank loss, ppl] step:75.5, 	loss: 1.342136263847351, 	ppl: 3.462254285812378
***** Evaluating perplexity, Epoch 5/5, step 14.0 *****
[eval loss, ppl] step:76.5, 	loss: 0.7673043012619019, 	ppl: 2.1699540615081787
[eval_CSTANCE loss, ppl] step:76.5, 	loss: 0.5420332551002502, 	ppl: 2.2609307765960693
[eval_20Minuten loss, ppl] step:76.5, 	loss: 1.3491861820220947, 	ppl: 3.965202808380127
[eval_FOMC loss, ppl] step:76.5, 	loss: 0.5837948322296143, 	ppl: 1.62643563747406
[eval_NumGLUEcm loss, ppl] step:76.5, 	loss: 3.272916793823242, 	ppl: 44.362247467041016
[eval_ScienceQA loss, ppl] step:76.5, 	loss: 0.7635690569877625, 	ppl: 2.1112287044525146
[eval_NumGLUEds loss, ppl] step:76.5, 	loss: 3.5799849033355713, 	ppl: 83.7575454711914
[eval_Py150 loss, ppl] step:76.5, 	loss: 2.7428927421569824, 	ppl: 17.87137794494629
[eval_MeetingBank loss, ppl] step:76.5, 	loss: 1.3415333032608032, 	ppl: 3.4650237560272217
saving model to /data2/TAP/model_con/0924/Fisher_C-STANCE_FOMC_MeetingBank_ScienceQA_epoch5_Llama3Exp_0.001/5...
[2025-09-25 01:35:42,058] [INFO] [launch.py:351:main] Process 2680404 exits successfully.
[2025-09-25 01:35:42,059] [INFO] [launch.py:351:main] Process 2680405 exits successfully.
[2025-09-25 01:35:43,060] [INFO] [launch.py:351:main] Process 2680403 exits successfully.
Sucessful saving model after epoch 5
[2025-09-25 01:36:11,089] [INFO] [launch.py:351:main] Process 2680402 exits successfully.
